# AI大模型应用开发-扫盲篇

当下从工程技术角度来看，AI大模型可以分为十个领域：前沿大模型、基准评估、提示思维链、检索增强生成、智能体、代码生成、视觉、声音、图像/视频扩散、微调。
本文旨在扫盲大模型应用开发基础概念，并介绍大模型应用开发的常见模式。

## 一、怎样得到一个大模型？

> 有卡没卡两种玩法，如果只面向应用开发，结合上信息安全的背景，则推荐使用社区开源模型或集成企业内部模型(下节介绍)。

### 1. 囤卡自己训练（全流程定制）
- **核心概念**  
  - **AI(artificial intelligence)**: 由来已久的概念，本文特指的生成式人工智能（gen AI），将机器学习和深度学习的神经网络提升到了一个新的水平。它可以创建新的内容和想法（例如图像和视频）, 也可以使用已知知识来解决新问题。
  - **LLM (Large Language Model)**: 大型语言模型，是基于大量数据进行预训练得到的超大型深度学习模型，可自主学习（统计归纳），会理解（Token向量化）人类基本的语法、语言和知识。一般用参数的多少来衡量大模型能力。
  - **GPT (Generative Pre-trained Transformer)**: 生成式预训练 Transformer 模型，是一系列使用 Transformer 架构的神经网络模型，能够回答产出（关联预测）类似人类的文本和内容（图像、音乐等），并以对话方式回答问题。
  - **Transformer 模型**: GPT的核心，是一种神经网络架构，它使用了自注意力机制（self-attention）来处理序列数据，并使用了多头自注意力（multi-head self-attention）来提高模型的性能。
  - **Pre-trained (预训练)**: 在无标注通用数据（如Common Crawl）上训练出基础模型，从已知找规律（优化损失函数）学习语言规律和世界知识，预测产出未知。需从头构建模型，通常需要超万亿token的庞大数据集和数万张高性能GPU（如H100/A100）组成的算力集群。
  - **Fine-tuning (微调)**: 在预训练模型基础上，用于特定领域数据（如金融客服、辅助编码）调整参数，使模型更适应垂直场景的推理。通常仅需调整少量参数（如使用LoRA技术调整0.1%-1%的参数），显存需求可降低至预训练的10%-20%。
  - **模型压缩**: 模型压缩技术，如量化、剪枝、知识蒸馏等，可以减少模型的参数数量，从而降低显存需求。
    - **知识蒸馏**: 将大型复杂模型（教师模型）在有限损失的情况下知识转移到更小型、高效模型（学生模型）中, 这样做的好处包括降低计算成本、减少推理时间，同时保持高性能，适合资源受限环境的部署。
    - **模型量化**: 模型量化是指将模型参数从浮点数转换为整数或低精度数据类型，以减少存储和计算开销。
    <!-- - **模型稀疏**: 也叫剪枝，模型稀疏是指通过删除或减少模型中一些低权重或参数来减少模型的大小和计算量。 -->
  > 蒸馏提取精华，量化降低细节。

- **实施难点**  
  - **硬件门槛**: OpenAI的GPT-4预训练成本约7800万美元，下一代模型的训练成本可能突破10亿美元，甚至向100亿美元迈进。国内有厂商利用算法优化，达到GPT-4同等性能表现的情况下，使大模型训练成本大幅降低，但也需要近600万美元的成本。  
  - **数据要求**: 需清洗TB级高质量文本，避免噪声干扰。  
  - **技术复杂度**: 涉及分布式训练、梯度优化等工程难题。  

- **适用场景**  
  当下大模型训练成本呈现“头部攀升、尾部下降”的极化现象。技术创新（算法优化、工程协同、开源生态）和国产算力替代为降本提供可能，此模式推荐巨头企业或科研机构用于前沿模型研发（如训练行业专属大模型）。

---

### 2. 下载社区开放模型（主流选择）
上一种方法，需要自己训练、微调得到一个LLM模型，现在介绍直接下载使用开放的大模型。

- **推荐平台**
    
  推荐平台(类似于代码找github，容器镜像找dockerhub的关系),可下载热门模型(和数据集):
  - **国外平台**: [Hugging Face Hub](https://huggingface.co/models): 提供100万+开源模型，涵盖Qwen2、DeepSeek、LLaMA、Gemma等热门模型。  
  - **国内平台**: [魔搭ModelScope](https://www.modelscope.cn/)、智源悟道、讯飞星火。  

- **模型类型（可商用）**  

  | 模型名称    | 参数量 | 特点                     | 典型用途          |
  |------------|--------|--------------------------|-------------------|
  | LLaMA-3    | 8B~70B | Meta，支持微调      |  通用对话、推理   |
  | Mistral-7B | 7B     | 法国，性能接近LLaMA-13B        | 代码生成、摘要    |
  | Phi-4 | 14B | 微软，支持微调 | 擅长英语对话 |
  | Qwen-系列 | 0.5B~72B | 阿里，大语言模型和大型多模态模型系列      | 通用对话、推理    |
  | DeepSeek-v3      | 671B | 国内幻方，MoE模型，训练成本降60%   | 通用对话、推理 |
  | DeepSeek-R1-蒸馏 | 32B~70B | 国内幻方，模型蒸馏，支持微调 | 复杂推理、编码、数学 |
  | ... | ... | ... | ... |

- **对比维度**

    | 维度               | 说明                                                                 | 示例对比                                |
    |--------------------|----------------------------------------------------------------------|----------------------------------------|
    | **参数量**         | 7B/13B/70B等，参数量越大能力越强，但资源需求更高                     | LLaMA-7B（8GB显存） vs LLaMA-70B（需多卡） |
    | **上下文窗口**     | 单次输入支持的最大Token数                     | GPT-4（128K） vs Claude-3（200K）       |
    | **多语言支持**     | 是否支持中文、代码等非英语场景                                       | Qwen-72B（中文优化） vs CodeLlama       |
    | **推理速度**       | 生成100个Token所需时间，影响用户体验                                 | Mistral-7B（快） vs LLaMA-13B（较慢）   |
    | **CPU推理支持**       |  没有GPU的要考虑，是否提供量化版本及CPU优化方案                                           | LLaMA.cpp（CPU优化） vs DeepSeek-MoE（需GPU）|
    | **内存占用**      | 不同精度（FP16/INT8）消耗不同           | Phi-3（4GB） vs Qwen1.5-4B（8GB）           |
    | **许可证**         | 商用限制（如LLaMA-2可商用，LLaMA-1仅限研究）                         | 企业选型需重点审查                     |
    | **多模态支持**         | 输出格式：图像、文本、音频、视频等                                       | GPT-4o-mini(多模态) vs deepseek-v3(仅文本)                     |


> 参数B和Token的关系

> 参数B应该是指模型训练的参数量，比如7B就是70亿参数。Token则是问答处理中的基本单位，通常对应单词或子词(1 Token约为1个汉字或4个字母)。
> 参数数量影响模型的学习能力和复杂度，而Token处理涉及输入输出的长度和速度。两者在模型训练和推理阶段都有不同的影响。比如，参数多的模型通常需要更多的计算资源，处理Token的能力更强，生成更准确的文本。但这也可能带来更高的延迟和资源消耗。

---

## 二、怎么使用大模型

前面主要讲了怎么得到大模型（自己训练和下载开源模型），下面介绍怎么使用大模型，技术上简单讲就是调用大模型的API，让大模型（服务）输出结果（文本生成、代码生成、图像生成）。
可以类比为一种中间件，用户可只关注输入和输出，中间件（大模型）负责推理产出。

- **在线API调用**  
  - **国外厂商**: 典型OpenAI的`gpt-4-turbo`每千Token约$0.01，延迟低但数据需出境。  
  - **AI集成服务**: 在线集成的模型服务，本文发稿时众代理商均有活动，可领取百万Token免费额度
  - **国内厂商**: deepseek(性价比极高,每百万Tokens为2元)、智谱AI（符合数据合规要求）、千问、百川。  

- **本地私有化部署**  
  - **加速推理的引擎选择**:  

    | **工具**      | **优势**                          | **适用场景**              | **性能基准**                  |
    |---------------|-----------------------------------|---------------------------|-------------------------------|
    | **vLLM**      | PagedAttention技术提升吞吐量3倍   | 高并发API服务 （CPU/GPU/TPU..）            | 70B模型 120 tokens/s (A100)   |
    | **llama.cpp** | 极致CPU优化，支持苹果Metal加速    | 边缘设备部署              | 7B模型 45 tokens/s (M2 Ultra) |
    | **Ollama**    | 一键部署，内置模型库              | 快速原型验证              | 13B模型 28 tokens/s (i9-13900H) |
    | **TGI**       | 企业级特性（监控/负载均衡）       | 生产环境集群 （不支持CPU）             | 支持千级别并发请求            |

  - **硬件推荐指南**:

    | **硬件环境**        | **推荐模型**                              | **性能指标**                          | **适用场景**              |
    |----------------------|------------------------------------------|---------------------------------------|---------------------------|
    | **手机/边缘设备**    | Phi-3-mini (3.8B)                        | iPhone 15 Pro: 35 tokens/s            | 离线对话、实时翻译         |
    | **CPU笔记本**        | Qwen1.5-4B-Chat-GGUF (量化版)            | i7-12700H: 18 tokens/s                | 文档摘要、基础问答         |
    | **消费级GPU**        | Llama-3-8B-Instruct (4-bit量化)          | RTX 4090: 85 tokens/s                 | 代码生成、知识库问答       |
    | **企业级服务器**     | DeepSeek-MoE-16b-chat                    | 2×A100: 120 tokens/s                  | 金融分析、复杂推理         |
    | **混合部署集群**     | Qwen1.5-72B-Chat + vLLM                  | 8×H100: 240 tokens/s (动态批处理)      | 超长文本生成、多轮对话     |

- **硬件优化技巧**: 
    - CPU加速：
        - 内存优化，使用GGUF格式 + mmap技术加载模型
        - 计算加速，启用BLAS库加速（OpenBLAS/Intel MKL），ARM设备可使用NEON指令集优化
    - GPU显存压缩：
        - 量化，将模型参数的精度从浮点数降低到低位表示（FP8,INT4等），使模型在资源有限的设备上更高效地部署（会降低部分模型性能）

- **开发模式对比**:

    | 方式       | 优点                      | 缺点                  | 适用阶段         |
    |------------|---------------------------|-----------------------|------------------|
    | 在线API调用| 快速集成，无需运维        | 依赖网络，数据出内网风险| 原型验证         |
    | 本地私有化部署   | 数据隐私可控，定制化强    | 硬件成本高，需调优    | 企业生产环境     |
    | 公私混合部署   | 平衡成本与安全性          | 架构复杂度高          | 中大型项目       |

---

## 四、基于大模型的应用技术
### 1. 提示词工程（Prompt Engineering）
- **核心原则**  
  - **角色定义**: *“你是一位资深律师，需用简洁语言解释以下法条...”*  
  - **结构化指令**: 使用XML/JSON格式明确输入输出规则。  
  - **少样本学习（Few-Shot）**: 提供示例引导模型理解任务。  

- **工具推荐**  
  - [LangChain Hub](https://smith.langchain.com/hub): 共享高质量Prompt模板  
  - [PromptPerfect](https://promptperfect.jina.ai): 自动优化提示词  

### 2. RAG（检索增强生成）
- **技术流程**  
  ```mermaid
  graph LR
  A[文档切分] --> B[向量化存储]
  B --> C[用户提问检索]
  C --> D[注入Prompt生成答案]
  ```

- **关键组件**  
  - **Embedding模型**: 文本转向量（推荐text-embedding-3-small）  
  - **向量数据库**: Pinecone（云服务）、Chroma（本地轻量级）  
  - **重排序器**: Cohere Rerank、BAAI/bge-reranker-large  

- **难题中文分词语义**
  - 混合分词策略：Jieba（词典匹配） + LSTM-CRF模型（实体识别）
  - 上下文增强：在向量检索后添加语义重排层（使用bge-reranker-base-v1.5）

- **开源框架**  
  - [LlamaIndex](https://www.llamaindex.ai): 快速构建RAG流水线  
  - [Haystack](https://haystack.deepset.ai): 支持多模态检索  

### 3. Agent（智能体）
- **核心能力**  
  - **工具调用**: 联网搜索、执行代码、调用API  
  - **任务分解**: 将复杂问题拆解为子任务（如“规划旅行”→订机票→订酒店）  
  - **自我迭代**: 根据反馈调整策略  

- **开发框架**  
  - [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT): 自主任务完成  
  - [LangGraph](https://langchain-ai.github.io/langgraph): 构建多Agent协作系统  

---

## 五、实战案例：企业知识库问答系统
### 技术方案
```
用户提问 → 向量检索（Milvus） → Rerank排序 → Prompt拼接 → LLaMA-7B生成 → 返回答案
```
### 代码片段（Python）
```python
from llama_index import VectorStoreIndex, SimpleDirectoryReader

# 1. 加载文档
documents = SimpleDirectoryReader("data").load_data()

# 2. 构建索引
index = VectorStoreIndex.from_documents(documents)

# 3. 问答查询
query_engine = index.as_query_engine()
response = query_engine.query("公司年假政策如何规定？")
print(response)
```

---

## 六、避坑指南
1. **模型幻觉**  
   - 添加事实性校验层（如调用Wolfram Alpha验证数值结果）  
2. **长文本处理**  
   - 采用滑动窗口（Sliding Window）分割超长上下文  
3. **安全防护**  
   - 使用NeMo Guardrails过滤敏感词 & 防止Prompt注入  

---

希望通过以上内容，可以帮助大家系统掌握大模型应用开发的全链路要点，从模型获取到技术落地均有实用参考，推荐实践后，再根据兴趣/需求重点拓展跟进文章开头提到的十大热点领域。

<!-- > 本文有一些背景认知没有交代(于我个人算是门外汉的学习答疑总结)，欢迎大家提出改进意见，一起学习，共创新价值。 -->

作者：许小平