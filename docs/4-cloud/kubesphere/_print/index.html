<!doctype html><html lang=zh-cn class=no-js>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.91.2">
<link rel=canonical type=text/html href=https://xiaoping378.github.io/docs/4-cloud/kubesphere/>
<link rel=alternate type=application/rss+xml href=https://xiaoping378.github.io/docs/4-cloud/kubesphere/index.xml>
<meta name=robots content="noindex, nofollow">
<link rel="shortcut icon" href=/favicons/favicon.ico>
<title>Kubersphere | 现代技能栈</title>
<meta name=description content="青云家的混合容器云管理平台，持续更新... 
">
<meta property="og:title" content="Kubersphere">
<meta property="og:description" content="青云家的混合容器云管理平台，持续更新... 
">
<meta property="og:type" content="website">
<meta property="og:url" content="https://xiaoping378.github.io/docs/4-cloud/kubesphere/"><meta property="og:site_name" content="现代技能栈">
<meta itemprop=name content="Kubersphere">
<meta itemprop=description content="青云家的混合容器云管理平台，持续更新... 
"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Kubersphere">
<meta name=twitter:description content="青云家的混合容器云管理平台，持续更新... 
">
<link rel=preload href=/scss/main.min.176bec0c2f472a5f9b76b5dd1c633f882851f90397a404aed4770a9f47b45056.css as=style>
<link href=/scss/main.min.176bec0c2f472a5f9b76b5dd1c633f882851f90397a404aed4770a9f47b45056.css rel=stylesheet integrity>
<script src=/js/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=/js/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-217913492-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
<a class=navbar-brand href=/>
<span class=navbar-logo></span><span class="text-uppercase font-weight-bold">现代技能栈</span>
</a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/about/><span>关于</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class="nav-link active" href=/docs/><span class=active>文档</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/blog/><span>博客</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/community/><span>社区</span></a>
</li>
</ul>
</div>
<div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; 站内搜索…" aria-label=站内搜索… autocomplete=off data-offline-search-index-json-src=/offline-search-index.a8ca2ec743d7bd731fbc49bc68259cc3.json data-offline-search-base-href=/ data-offline-search-max-results=10>
</div>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.
</p><p>
<a href=/docs/4-cloud/kubesphere/>返回本页常规视图</a>.
</p>
</div>
<h1 class=title>Kubersphere</h1>
<div class=lead>青云家的混合容器云管理平台，持续更新...</div>
<ul>
<li>1: <a href=#pg-860749379d08e54cd43ce8d25912f35e>使用kind本地启动多集群</a></li>
<li>2: <a href=#pg-c79f030e7c908d1a1a3729cd2bf8e478>论Kubesphere的异地多活方案</a></li>
<li>3: <a href=#pg-3c27a4a4e50b0a6566a482a1d528c894>搭建kubesphere的开发调试环境</a></li>
</ul>
<div class=content>
<div class="pageinfo pageinfo-primary">
<p>主要记录介绍对kubersphere的架构认知和储备二次开发所必备的知识。</p>
<p>本系列文章中会把kubesphere简说成KCP（Qingcloud kubernetes controller-manage platform）</p>
</div>
</div>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-860749379d08e54cd43ce8d25912f35e>1 - 使用kind本地启动多集群</h1>
<div class=lead>在本地利用kind搭建多个集群，验证多集群纳管...</div>
<div class="pageinfo pageinfo-primary">
<p>我本地4c/8G的小本儿，跑了两个集群，组建了多集群环境，还行，能玩动...</p>
</div>
<h2 id=环境篇>环境篇</h2>
<ol>
<li>
<p>kind<a href=https://kind.sigs.k8s.io/docs/user/quick-start/>安装</a></p>
</li>
<li>
<p>镜像准备</p>
<p>视网络情况，可以把依赖镜像<code>kindest/node</code>提起pull到本地</p>
</li>
<li>
<p>docker的data-root目录</p>
<p>尽量不要放到/var目录下，kind起的集群容器会占用比较大的空间</p>
</li>
</ol>
<h2 id=实操>实操</h2>
<ol>
<li>创建集群</li>
</ol>
<p>执行完如下命令后，docker ps可以看到本地启动了两个容器，一个容器对应一个集群。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kind create cluster --image kindest/node:v1.19.16 --name host
kind create cluster --image kindest/node:v1.19.16 --name member
</code></pre></div><p><code>kubectl config use-context [kind-host | kind-member]</code>，可以切换kubecl执行的上下文</p>
<ol start=2>
<li>安装kubesphere</li>
</ol>
<p>分别在两个集群各自安装ks组件</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># 集群1安装</span>
kubectl config use-context kind-host
kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/kubesphere-installer.yaml   
kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/cluster-configuration.yaml

<span style=color:#080;font-style:italic># 集群2安装</span>
kubectl config use-context kind-member
kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/kubesphere-installer.yaml   
kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/cluster-configuration.yaml
</code></pre></div><ol start=3>
<li>纳管集群</li>
</ol>
<p>可以在上面的初始化阶段直接改好主和成员集群的关系，这里参考<a href=https://kubesphere.com.cn/docs/multicluster-management/enable-multicluster/direct-connection/>官文</a>即可</p>
<p>host集群的UI地址，可以通过<code>host容器IP:30880</code>来访问，主集群的容器ip，可以如下获取：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>docker inspect --format <span style=color:#b44>&#39;{{ .NetworkSettings.Networks.kind.IPAddress }}&#39;</span> host-control-plane
</code></pre></div><p>实操<code>添加</code>集群时，需要member集群的kubeconfig，可以用如下命令获取到</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kind get kubeconfig --name member
</code></pre></div><p>记得把kubeconfig中的<code>server</code>地址中改成<code>member容器ip:6443</code>，这样host集群才能访问到member集群</p>
<h2 id=总结>总结</h2>
<p>验证功能、测试开发，挺方便的，可以视本地资源紧张情况停掉监控的ns。</p>
<p>现在kind启动的集群默认使用了containerd的runtime，若想进一步调试查看集群内的情况，可以内部集成的<code>crictl</code>代替熟悉的docker工具。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c79f030e7c908d1a1a3729cd2bf8e478>2 - 论Kubesphere的异地多活方案</h1>
<div class=lead>混合容器云管理平台Kubesphere的异地多活方案...</div>
<div class="pageinfo pageinfo-primary">
<p>遇到这样一个场景，在同一套环境中需要存在多个host控制面集群...bulabula... 因此想探索下kubesphere的异地多活混合容器云管理方案</p>
</div>
<h2 id=集群角色介绍>集群角色介绍</h2>
<p>一个兼容原生的k8s集群，可通过<code>ks-installer</code>来初始化完成安装，成为一个QKE集群。QKE集群分为多种角色，默认是none角色（standalone模式），开启多集群功能时，可以设置为host或者member角色。</p>
<p><img src=/images/kcp-%E5%A4%9A%E9%9B%86%E7%BE%A4.png alt=多集群></p>
<ul>
<li>none角色，是最小化安装的默认模式，会安装必要的ks-apiserver, ks-controller-manager, ks-console和其他组件
<ul>
<li>ks-apiserver, kcp的API网关，包含审计、认证、权限校验等功能</li>
<li>ks-controller, 各类自定义crd的控制器和平台管理逻辑的实现</li>
<li>ks-console, 前端界面UI</li>
<li>ks-installer, 初始化安装和变更QKE集群的工具，由shell-operator触发ansible-playbooks来工作。</li>
</ul>
</li>
<li>member角色，承载工作负载的业务集群，和none模式的组件安装情况一致</li>
<li>host角色，整个混合云管理平台的控制面，会在none的基础上，再额外安装tower，kubefed-controller-manager， kubefed-admission-webhook等组件
<ul>
<li>tower，代理业务集群通信的server端，常用于不能直连member集群api-server的情况</li>
<li>kubefed-controller-manager，社区的<a href=https://github.com/kubernetes-sigs/kubefed>kubefed</a>联邦资源的控制器</li>
<li>kubefed-admission-webhook， 社区的kubefed联邦资源的动态准入校验器</li>
</ul>
</li>
</ul>
<h2 id=多集群管理原理>多集群管理原理</h2>
<p>上段提到QKE有3种角色，可通过修改<code>cc</code>配置文件的<code>clusterRole</code>来使能, ks-installer监听到配置变化的事件，会初始化对应集群角色的功能。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl edit cc ks-installer -n kubesphere-system
</code></pre></div><blockquote>
<p>角色不要改来改去，会出现莫名问题，主要是背后ansible维护的逻辑有疏漏，没闭环</p>
</blockquote>
<h3 id=host集群>host集群</h3>
<p>host角色的主集群会被创建25种联邦资源类型Kind，如下命令可查看，还会额外安装kubefed stack组件。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>➜  kubectl get FederatedTypeConfig  -A
</code></pre></div><p>此外api-server被重启后，会根据配置内容的变化，做两件事，注册多集群相关的路由和缓存同步部分联邦资源。</p>
<ul>
<li>添加url里包含<code>clusters/{cluster}</code>路径的agent路由和转发的功能，要访问业务集群的信息，这样可以直接转发过去。</li>
<li>cacheSync，缓存同步联邦资源，这里是个同步的操作。</li>
</ul>
<blockquote>
<p>当开启多集群后，如果某个member出现异常导致不可通信，那host的api-server此时遇到故障要重启，会卡在cacheSync这一步，导致无法启动，进而整个平台无法访问。</p>
</blockquote>
<p>controller-manager被重启后，同样会根据配置的变化，把部分资源类型自动转化成联邦资源的逻辑，也就是说，在host集群创建的这部分资源会自动同步到所有成员集群，实际的多集群同步靠kubefed-controller-manager来执行。以下资源会被自动创建联邦资源下发：</p>
<ul>
<li>users.iam.kubesphere.io -> federatedusers.types.kubefed.io</li>
<li>workspacetemplates.tenant.kubesphere.io -> federatedworkspaces.types.kubefed.io</li>
<li>workspaceroles.iam.kubesphere.io -> federatedworkspaceroles.types.kubefed.io</li>
<li>workspacerolebindings.iam.kubesphere.io -> federatedworkspacerolebindings.types.kubefed.io</li>
</ul>
<p>此外还会启动cluster、group和一些globalRole*相关资源的控制器逻辑，同上也会通过kubefed自动下发到所有集群，<code>clusters.cluster.kubesphere.io</code>资源除外。</p>
<blockquote>
<p>如果以上资源包含了<code>kubefed.io/managed: false</code>标签，kubefed就不会再做下发同步，而host集群下发完以上资源后，都会自动加上该标签，防止进入死循环</p>
</blockquote>
<h3 id=member集群>member集群</h3>
<p>修改为member集群时，需要cc中的<strong>jwtSecret</strong>与host集群的保持一致(若该值为空的话，ks-installer默认会随机生成)，提取host集群的该值时，需要去cm里找，如下：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>kubectl -n kubesphere-system get cm kubesphere-config -o yaml | grep -v <span style=color:#b44>&#34;apiVersion&#34;</span> | grep jwtSecret
</code></pre></div><blockquote>
<p>jwtSecret要保持一致，主要是为了在host集群<strong>签发</strong>的用户token，在用户访问业务集群时token<strong>校验</strong>能通过。</p>
</blockquote>
<h3 id=添加集群>添加集群</h3>
<p>本文只关注<code>直接连接</code>这种情况，当填好成员集群的kubeconfig信息，点击<code>添加</code>集群后,会做如下校验：</p>
<ul>
<li>通过kubeconfig信息先校验下是否会添加已存在的重复集群</li>
<li>校验成员集群的网络连通性</li>
<li>校验成员集群是否安装了ks-apiserver</li>
<li>校验成员集群的<code>jwtSecret</code>是否和主集群的一致</li>
</ul>
<blockquote>
<p>写稿时，此处有个问题，需要修复，如果kubeconfig使用了<code>insecure-skip-tls-verify: true</code>会导致该集群添加失败，经定位主要是kubefed 空指针panic了，后续有时间我会去fix一下，已提<a href=https://github.com/kubesphere/kubesphere/issues/4891>issue</a>。</p>
</blockquote>
<p>校验完必要信息后，就执行实质动作<code>joinFederation</code>加入联邦，kubesphere多集群纳管，实质上是先组成联邦集群:</p>
<ul>
<li>在成员集群创建ns kube-federation-system</li>
<li>在上面的命名空间中创建serviceAccount [clusterName]-kubesphere, 并绑定最高权限</li>
<li>在主集群的kube-federation-system的命名空间创建<code>kubefedclusters.core.kubefed.io</code>，由kubefed stack驱动联邦的建立</li>
<li>加入联邦后，主机群的联邦资源会通过kubefed stack同步过来</li>
</ul>
<blockquote>
<p>上述一顿操作，等效于 <code>kubefedctl join member-cluster --cluster-context member-cluster --host-cluster-context host-cluster</code></p>
</blockquote>
<h2 id=异地多活方案设计>异地多活方案设计</h2>
<p>异地多活的方案主要是多个主集群能同时存在，且保证数据双向同步，经过上面的原理分析，可知多个主集群是可以同时存在的，也就是一个成员集群可以和多个主集群组成联邦。整体方案示意图设计如下：</p>
<p><img src=/images/kcp-multi-hostclusters.png alt></p>
<blockquote>
<p>以下操作假设本地已具备三个QKE集群，如果不具备的可按照<a href=/docs/4-cloud/kubesphere/kind-multicluster-dev/>此处</a>快速搭建<code>host、host2、member</code>3个集群</p>
</blockquote>
<p>大致实现逻辑的前提介绍：</p>
<ol>
<li>三个集群的<code>jwtSecret</code>得保持一致</li>
<li>两个主集群都去<code>添加</code>纳管同一个member集群</li>
<li>利用<code>etcdctl make-mirror</code>实现双向同步</li>
</ol>
<h3 id=验证下可行性>验证下可行性</h3>
<p>实操双活前，先验证下可行性</p>
<p><strong>实验1：</strong></p>
<p>在两边创建一个同名用户，用户所有信息一致，可以添加成功，然后再修改一边的用户信息，使两边不一致</p>
<p>可以看到member集群的用户xxp，一直会被两边不断的更新...</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>root@member-control-plane:/# kubectl get user xxp -w
NAME   EMAIL         STATUS
xxp    xxp@163.com   Active
xxp    xxp-2@163.com   Active
xxp    xxp@163.com    Active
xxp    xxp-2@163.com   Active

... 周而复始 ...
</code></pre></div><p>这个实验，即使在创建用户时，页面表单上两边信息填的都一样，也会出现互相刷新覆盖的情况，因为yaml里的uid和time信息不一致</p>
<p><strong>实验2：</strong></p>
<p>在两边添加一个同名用户，但两边用户信息（用户角色）不一致，可以创建成功，但后创建者的kube-federa会同步失败, 到这里还能接受，毕竟有冲突直接就同步失败了</p>
<p>但member集群上该用户的关联角色会出现上文的情况，被两边的主集群持续反复地修改...</p>
<p><strong>实验3：</strong></p>
<p>在一侧的主集群上尝试修复冲突资源，即删除有冲突的用户资源，可以删除成功，但对应的联邦资源会出现删失败的情况</p>
<pre tabindex=0><code>➜  ~ kubectl get users.iam.kubesphere.io
NAME    EMAIL                 STATUS
admin   admin@kubesphere.io   Active
xxp3    xxp3@163.com          Active
➜  ~
➜  ~ kubectl get federatedusers.types.kubefed.io
NAME    AGE
admin   5h33m
xxp     65m #这里是个删不掉的资源，fed controller会重复做失败尝试
xxp3    61m
</code></pre><p>这样就会出现，两个主集群：一个要删，一个要同步，member集群上：持续上演“一会儿消失，一会儿又出现了”的奇观。</p>
<h3 id=总结>总结</h3>
<p>两个主集群可以同时工作，一旦出现同名冲突资源，处理起来会非常麻烦，尤其是背后的Dependent附属资源出现冲突时，往往问题点隐藏的更深，修复起来也棘手...</p>
<p>后来调研也发现：目前的社区方案make-mirror只支持单向同步，适合用来做灾备方案。</p>
<p>所以容器云平台的双活，除非具备跨AZ的etcd集群，否则需要二次开发改造类make-mirror方案来支持了。我最开始要考虑的问题答案也就显而易见了：如果要多个host集群共存，必须考虑通过行政管理手段，来尽量避免同名资源冲突。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-3c27a4a4e50b0a6566a482a1d528c894>3 - 搭建kubesphere的开发调试环境</h1>
<div class=lead>记录搭建kubesphere的开发调试环境 - 后端篇</div>
<h2 id=依赖工具介绍>依赖工具介绍</h2>
<ol>
<li>vscode，个人日常使用vscode开发，标配<code>Remote Development</code>插件</li>
<li>kt-connect，本地开发使用的流量代理工具，可以双向代理，本地可以直接访问pod和svc，也可以转发访问pod的流量到本地，相关<a href=https://github.com/alibaba/kt-connect/blob/master/docs/zh-cn/reference/mechanism.md>原理介绍</a>。</li>
</ol>
<h2 id=准备调试环境>准备调试环境</h2>
<h3 id=连接集群网络>连接集群网络</h3>
<p>使用kt-connect，打通网络，本地可直接访问Kubernetes集群内网</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo ktctl connect --context kind-host  --portForwardTimeout <span style=color:#666>300</span>
</code></pre></div><ul>
<li><code>ktctl</code>采用本地<code>kubectl</code>工具的集群配置，默认为~/.kube/config文件中配置的集群。</li>
<li>如果kubeconfig有多个集群，可以通过<code>--context</code>指定要连接的具体集群</li>
<li>如果本地环境是<code>kind</code>集群，需要修改kubeconfig中server的127地址为<code>容器IP:6443</code></li>
<li>如果网络比较差，会遇到错误<code>ERR Exit: pod kt-rectifier-tcxjk failed to start</code>，可适当增加等待时间<code>portForwardTimeout</code></li>
</ul>
<blockquote>
<p>另外注意的是，kt-connect需要root权限，上条命令会默认读取<code>/root/.kube/config</code>文件，自行copy或者另通过<code>-c</code>指定文件</p>
</blockquote>
<h3 id=clone代码>clone代码</h3>
<p>不表.</p>
<h3 id=编辑调试的配置文件>编辑调试的配置文件</h3>
<p>首先编辑vscode的调试配置文件, 我是如下配置的：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>➜  kubesphere git:<span style=color:#666>(</span>master<span style=color:#666>)</span> cat .vscode/launch.json
<span style=color:#666>{</span>
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid<span style=color:#666>=</span><span style=color:#666>830387</span>
    <span style=color:#b44>&#34;version&#34;</span>: <span style=color:#b44>&#34;0.2.0&#34;</span>,
    <span style=color:#b44>&#34;configurations&#34;</span>: <span style=color:#666>[</span>
        <span style=color:#666>{</span>
            <span style=color:#b44>&#34;name&#34;</span>: <span style=color:#b44>&#34;ks-apiserver&#34;</span>,
            <span style=color:#b44>&#34;type&#34;</span>: <span style=color:#b44>&#34;go&#34;</span>,
            <span style=color:#b44>&#34;request&#34;</span>: <span style=color:#b44>&#34;launch&#34;</span>,
            <span style=color:#b44>&#34;mode&#34;</span>: <span style=color:#b44>&#34;auto&#34;</span>,
            <span style=color:#b44>&#34;program&#34;</span>: <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>workspaceFolder</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/ks-apiserver/apiserver.go&#34;</span>
        <span style=color:#666>}</span>,
        <span style=color:#666>{</span>
            <span style=color:#b44>&#34;name&#34;</span>: <span style=color:#b44>&#34;controller-manager&#34;</span>,
            <span style=color:#b44>&#34;type&#34;</span>: <span style=color:#b44>&#34;go&#34;</span>,
            <span style=color:#b44>&#34;request&#34;</span>: <span style=color:#b44>&#34;launch&#34;</span>,
            <span style=color:#b44>&#34;mode&#34;</span>: <span style=color:#b44>&#34;auto&#34;</span>,
            <span style=color:#b44>&#34;program&#34;</span>: <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>workspaceFolder</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/controller-manager/controller-manager.go&#34;</span>
        <span style=color:#666>}</span>
    <span style=color:#666>]</span>
<span style=color:#666>}</span>
</code></pre></div><h3 id=准备调试环境-1>准备调试环境</h3>
<p>按F5，启动调试，左上角选择要调试的组件，我这里以controller-manager举例（需要hack的注意点比较多）。</p>
<p>过会儿会发现出现错误，错误提示很明显，因缺少配置文件，导致无法启动，通过查看<code>deployment yaml</code>, 发现后面还会缺失<code>Admission Webhooks</code>的证书，可如下统一提取到本地：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># 提取启动的配置文件(调试apiserver的时候也需要这一步，但要把文件放到对应cmd/ks-apiserver目录下)</span>
kubectl -n kubesphere-system get cm kubesphere-config -ojsonpath<span style=color:#666>=</span><span style=color:#b44>&#39;{.data.kubesphere\.yaml}&#39;</span> &gt; cmd/controller-manager/kubesphere.yaml
<span style=color:#080;font-style:italic># 提取webhook用到的证书</span>
mkdir -p /tmp/k8s-webhook-server/serving-certs/
<span style=color:#a2f>export</span> <span style=color:#b8860b>controller_pod</span><span style=color:#666>=</span><span style=color:#b44>`</span>kubectl -n kubesphere-system get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>ks-controller-manager  -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.items[0].metadata.name}&#39;</span><span style=color:#b44>`</span>
kubectl -n kubesphere-system <span style=color:#a2f>exec</span> -it <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>controller_pod</span><span style=color:#b68;font-weight:700>}</span> -- cat /tmp/k8s-webhook-server/serving-certs/ca.crt &gt; /tmp/k8s-webhook-server/serving-certs/ca.crt
kubectl -n kubesphere-system <span style=color:#a2f>exec</span> -it <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>controller_pod</span><span style=color:#b68;font-weight:700>}</span> -- cat /tmp/k8s-webhook-server/serving-certs/tls.crt &gt; /tmp/k8s-webhook-server/serving-certs/tls.crt
kubectl -n kubesphere-system <span style=color:#a2f>exec</span> -it <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>controller_pod</span><span style=color:#b68;font-weight:700>}</span> -- cat /tmp/k8s-webhook-server/serving-certs/tls.key &gt; /tmp/k8s-webhook-server/serving-certs/tls.key
</code></pre></div><p>继续启动，发现还会有缺文件的错误，应该是编译镜像时，内置了些文件，通过查看<code>build/ks-controller-manager/Dockerfile</code>，发现后面会缺的东西还是比较多的，推荐直接从运行中的pod直接copy到本地：</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo mkdir /var/helm-charts/
sudo chmod -R a+rw /var/helm-charts
kubectl -n kubesphere-system cp <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>controller_pod</span><span style=color:#b68;font-weight:700>}</span>:/var/helm-charts  /var/helm-charts/
</code></pre></div><p>继续启动，成功 ！</p>
<h2 id=开始调试>开始调试</h2>
<p>利用ktctl替换集群中的ks-controller-manager的服务为本地服务。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo ktctl exchange  ks-controller-manager --namespace kubesphere-system --mode scale  --recoverWaitTime <span style=color:#666>300</span> --expose 8443:8443
</code></pre></div><blockquote>
<p>如果本地集群只有一个节点，上述命令会一直pending，可以通过如下命令替代</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo ktctl exchange  ks-controller-manager --namespace kubesphere-system  --expose 8443:8443
kubectl -n kubesphere-system scale deployment ks-controller-manager --replicas<span style=color:#666>=</span><span style=color:#666>0</span>
</code></pre></div><p>结束调试，记得还原刚才缩容<code>replicas</code>的设置。</p>
</blockquote>
<p>后续就是vscode的正常断点调试或者本地开发验证了，有时间在整理贴图...</p>
</div>
</main>
</div>
</div>
<footer class="bg-dark py-5 row d-print-none">
<div class="container-fluid mx-sm-5">
<div class=row>
<div class="col-6 col-sm-4 text-xs-center order-sm-2">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="个人邮箱 xiaoping378@163.com" aria-label="个人邮箱 xiaoping378@163.com">
<a class=text-white target=_blank rel=noopener href=mailto:xiaoping378@163.com aria-label="个人邮箱 xiaoping378@163.com">
<i class="fa fa-envelope"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=微博 aria-label=微博>
<a class=text-white target=_blank rel=noopener href=https://weibo.com/xiaoping378 aria-label=微博>
<i class="fab fa-weibo"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=知乎 aria-label=知乎>
<a class=text-white target=_blank rel=noopener href=https://www.zhihu.com/people/xiaoping378 aria-label=知乎>
<i class="fab fa-zhihu"></i>
</a>
</li>
</ul>
</div>
<div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank rel=noopener href=https://github.com/xiaoping378/xiaoping378.github.io aria-label=GitHub>
<i class="fab fa-github"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack>
<a class=text-white target=_blank rel=noopener href=https://example.org/slack aria-label=Slack>
<i class="fab fa-slack"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Developer mailing list" aria-label="Developer mailing list">
<a class=text-white target=_blank rel=noopener href=https://example.org/mail aria-label="Developer mailing list">
<i class="fa fa-envelope"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-4 text-center py-2 order-sm-2">
<small class=text-white>&copy; 2022 xiaoping378 保留所有权利</small>
<small class=ml-1><a href=# target=_blank rel=noopener>隐私政策</a></small>
</div>
</div>
</div>
</footer>
</div>
<script src=/js/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/deflate.js></script>
<script src=/js/main.min.e016890ed6b0c42f5af3410eb57ac626a192a868609aee68cefe1e0f84a50b13.js integrity="sha256-4BaJDtawxC9a80EOtXrGJqGSqGhgmu5ozv4eD4SlCxM=" crossorigin=anonymous></script>
</body>
</html>