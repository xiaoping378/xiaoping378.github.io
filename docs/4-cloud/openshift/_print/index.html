<!doctype html><html lang=zh-cn class=no-js>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=generator content="Hugo 0.91.2">
<link rel=canonical type=text/html href=https://xiaoping378.github.io/docs/4-cloud/openshift/>
<link rel=alternate type=application/rss+xml href=https://xiaoping378.github.io/docs/4-cloud/openshift/index.xml>
<meta name=robots content="noindex, nofollow">
<link rel="shortcut icon" href=/favicons/favicon.ico>
<title>Openshift | 现代技能栈</title>
<meta name=description content="红帽家的k8s发行版，在持续演进中... 
">
<meta property="og:title" content="Openshift">
<meta property="og:description" content="红帽家的k8s发行版，在持续演进中... 
">
<meta property="og:type" content="website">
<meta property="og:url" content="https://xiaoping378.github.io/docs/4-cloud/openshift/"><meta property="og:site_name" content="现代技能栈">
<meta itemprop=name content="Openshift">
<meta itemprop=description content="红帽家的k8s发行版，在持续演进中... 
"><meta name=twitter:card content="summary">
<meta name=twitter:title content="Openshift">
<meta name=twitter:description content="红帽家的k8s发行版，在持续演进中... 
">
<link rel=preload href=/scss/main.min.9f63fcc1b5f091883eb4c0c66b7bddf337a299d08451443230a201303e53b01c.css as=style>
<link href=/scss/main.min.9f63fcc1b5f091883eb4c0c66b7bddf337a299d08451443230a201303e53b01c.css rel=stylesheet integrity>
<script src=/js/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=/js/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-217913492-1','auto'),ga('send','pageview'))</script>
<script async src=https://www.google-analytics.com/analytics.js></script>
</head>
<body class=td-section>
<header>
<nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
<a class=navbar-brand href=/>
<span class=navbar-logo></span><span class="text-uppercase font-weight-bold">现代技能栈</span>
</a>
<div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar>
<ul class="navbar-nav mt-2 mt-lg-0">
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/about/><span>关于</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class="nav-link active" href=/docs/><span class=active>文档</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/blog/><span>博客</span></a>
</li>
<li class="nav-item mr-4 mb-2 mb-lg-0">
<a class=nav-link href=/community/><span>社区</span></a>
</li>
</ul>
</div>
<div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; 站内搜索…" aria-label=站内搜索… autocomplete=off data-offline-search-index-json-src=/offline-search-index.2e4493fb40bc730b36b40ccf0e74dcbd.json data-offline-search-base-href=/ data-offline-search-max-results=10>
</div>
</nav>
</header>
<div class="container-fluid td-outer">
<div class=td-main>
<div class="row flex-xl-nowrap">
<main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main>
<div class=td-content>
<div class="pageinfo pageinfo-primary d-print-none">
<p>
这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.
</p><p>
<a href=/docs/4-cloud/openshift/>返回本页常规视图</a>.
</p>
</div>
<h1 class=title>Openshift</h1>
<div class=lead>红帽家的k8s发行版，在持续演进中...</div>
<ul>
<li>1: <a href=#pg-7bc2991b557d870871fd602d8d55369e>快速安装</a></li>
<li>2: <a href=#pg-1cfa457dd509531d8244077f99d5f24d>权限资源管理</a></li>
<li>3: <a href=#pg-ec5884d27cd5b8eb79c2b702bc96a1e0>项目开发实战</a></li>
<li>4: <a href=#pg-6e88b10af27d183cafde9b830a963484>DevOps实战-0</a></li>
<li>5: <a href=#pg-2d14ca450c11bea81bf6011dad4a599e>DevOps实战-1</a></li>
<li>6: <a href=#pg-be9d3e63866e8da759569cdab7948ae5> 编译和目录结构介绍</a></li>
<li>7: <a href=#pg-c4ea8f380e4d90a7452b796546dc9dea>多负载均衡方案</a></li>
<li>8: <a href=#pg-85297fa1083d97d27a86dc5bf59b1493>镜像管理</a></li>
<li>9: <a href=#pg-85fad76100d7a48bf023fda8e541db66>性能优化指南</a></li>
<li>10: <a href=#pg-6c736738cfe8af1d9bca20191186afaf>网络整理</a></li>
<li>11: <a href=#pg-cb9d98fa953e6ad69dc9105c1c11550b>监控梳理</a></li>
<li>12: <a href=#pg-65d2c9ba3dda05a2d82269e367f5bdee>日志分析</a></li>
</ul>
<div class=content>
<div class="pageinfo pageinfo-primary">
<p>主要记录介绍以前个人的Openshfit实践总结。</p>
</div>
</div>
</div>
<div class=td-content>
<h1 id=pg-7bc2991b557d870871fd602d8d55369e>1 - 快速安装</h1>
<div class=lead>介绍openshift的快速安装方法。</div>
<p>不知道为什么openshift在国内热度这么低，那些要做自己容器云的公司，不知道有openshift项目的存在么？完全满足我的需求。</p>
<p>docker负责应用的隔离打包，k8s提供集群管理和容器的编排服务，而openshfit则负责整个应用的生命周期：</p>
<ul>
<li>源码管理，CI&CD能力</li>
<li>多租户管理, 支持LDAP和Oauth</li>
<li>集成监控日志于web console</li>
</ul>
<p>先说下自接触到openshift项目就遇到的一个困惑，就是openshift origin/enterprise /online/dedicated/ocp之间的关系： <code>orgin相当于Fedora， 其他的相当于RHEL</code></p>
<p>接下来谈下我用自己的笔记本实践的过程与感受：</p>
<ol>
<li>快速安装</li>
</ol>
<p>本人日常基于ubuntu16.04办公，所以用oc直接上, oc相当于kubectl</p>
<p><a href=https://github.com/openshift/origin/releases>这里</a>直接下载oc客户端，或者自行编译, 编译结果在_output目录下</p>
<pre tabindex=0><code>git clone --depth=1 https://github.com/openshift/origin.git
cd origin &amp;&amp; make
mv _output/local/bin/linux/amd64/oc  /usr/local/bin

</code></pre><p>启动openshift, 默认开启监控并初始安装自最新版本，当前是v1.5.0-alpha.2</p>
<pre tabindex=0><code>oc cluster up --metrics=true  --version=latest --insecure-skip-tls-verify=true --public-hostname=air13
</code></pre><p>过程中会拉取所需镜像, 我这里显示比较多，之前已经做了些实验</p>
<pre tabindex=0><code>➜  ~ docker images | grep openshift | awk '{print $1}'
openshift/node
openshift/origin-sti-builder
openshift/origin-docker-builder
openshift/origin-deployer
openshift/origin-gitserver
openshift/origin-docker-registry
openshift/origin-haproxy-router
openshift/origin
openshift/hello-openshift
openshift/openvswitch
openshift/origin-pod
openshift/origin-metrics-cassandra
openshift/origin-metrics-hawkular-metrics
openshift/origin-metrics-heapster
openshift/origin-metrics-deployer
openshift/mysql-55-centos7
openshift/origin-logging-curator
openshift/origin-logging-fluentd
openshift/origin-logging-deployment
openshift/origin-logging-elasticsearch
openshift/origin-logging-kibana
openshift/origin-logging-auth-proxy
</code></pre><p>启动后，会打印如下信息</p>
<pre tabindex=0><code>OpenShift server started.
The server is accessible via web console at:
    https://air13:8443

The metrics service is available at:
    https://metrics-openshift-infra.192.168.31.49.xip.io

You are logged in as:
    User:     developer
    Password: developer

To login as administrator:
    oc login -u system:admin
</code></pre><p>打开浏览器，访问https://air13:8443，默认用developer登录，其实现在任意用户任意密码都可以的。</p>
<p>web console里是空空如野的，可以临时授权developer用户操作所有项目</p>
<pre tabindex=0><code>oc adm policy add-cluster-role-to-user cluster-admin developer
</code></pre><p>2.技巧总结</p>
<ul>
<li>命令行自动补全, 其实kubectl也可以如此</li>
</ul>
<p><code>source &lt;(oc completion bash)</code></p>
<ul>
<li>
<p>默认监控占用的资源太大了，可以如下降低资源占用，当然也可以web操作限制资源利用率</p>
<pre tabindex=0><code>oc env rc hawkular-cassandra-1 MAX_HEAP_SIZE=1024M -n openshift-infra
#重建下变量才会生效
oc scale rc hawkular-cassandra-1 --replicas 0 -n openshift-infra
oc scale rc hawkular-cassandra-1 --replicas 1 -n openshift-infra
</code></pre><p>因为是rc，所以直接杀掉没关系，要不env不生效</p>
</li>
<li>
<p>自己编译离线文档</p>
<pre tabindex=0><code># 下载源文件
git clone --depth=1 https://github.com/openshift/openshift-docs.git
# 编译
cd openshift-docs &amp;&amp; asciibinder build
# 结果会存放在 _preview下，
cd _preview &amp;&amp; python -m SimpleHTTPServer
#打开浏览器访问127.0.0.1:8000
</code></pre><p>推荐此人<a href=http://guifreelife.com/>blog</a>，有几篇干货</p>
</li>
</ul>
<p>3.后面会重点说下权限/资源管理和整个app开发的流程</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-1cfa457dd509531d8244077f99d5f24d>2 - 权限资源管理</h1>
<div class=lead>介绍openshift的账户、权限、资源配额管理。</div>
<p>重点介绍 project，limitRange，resourceQuta和 user, group, rule，role，policy，policybinding的关系,
我刚接触时，这几个概念老搞不太清楚，这里梳理下</p>
<h2 id=资源管理说明>资源管理说明</h2>
<p>可以对计算资源的大小和对象类型的数量来进行配额限制。</p>
<p><code>ResourceQuota</code>是面向project（namespace的基础上加了些注解）层面的，只有集群管理员可以基于namespace设置。</p>
<p><code>limtRange</code>是面向pod和container级别的，openshift额外还可以限制 image， imageStream和pvc，
也是只有集群管理员才可以基于project设置，而开发人员只能基于pod（container）设置cpu和内存的requests/limits。</p>
<h3 id=resourcequota>ResourceQuota</h3>
<p>看看具体可以管理哪些资源，期待网络相关的也加进来.简单来讲，可以基于project来限制可消耗的内存大小和可创建的pods数量</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#080;font-style:italic>// The following identify resource constants for Kubernetes object types
</span><span style=color:#080;font-style:italic></span><span style=color:#a2f;font-weight:700>const</span> (
	<span style=color:#080;font-style:italic>// Pods, number
</span><span style=color:#080;font-style:italic></span>	ResourcePods ResourceName = <span style=color:#b44>&#34;pods&#34;</span>
	<span style=color:#080;font-style:italic>// Services, number
</span><span style=color:#080;font-style:italic></span>	ResourceServices ResourceName = <span style=color:#b44>&#34;services&#34;</span>
	<span style=color:#080;font-style:italic>// ReplicationControllers, number
</span><span style=color:#080;font-style:italic></span>	ResourceReplicationControllers ResourceName = <span style=color:#b44>&#34;replicationcontrollers&#34;</span>
	<span style=color:#080;font-style:italic>// ResourceQuotas, number
</span><span style=color:#080;font-style:italic></span>	ResourceQuotas ResourceName = <span style=color:#b44>&#34;resourcequotas&#34;</span>
	<span style=color:#080;font-style:italic>// ResourceSecrets, number
</span><span style=color:#080;font-style:italic></span>	ResourceSecrets ResourceName = <span style=color:#b44>&#34;secrets&#34;</span>
	<span style=color:#080;font-style:italic>// ResourceConfigMaps, number
</span><span style=color:#080;font-style:italic></span>	ResourceConfigMaps ResourceName = <span style=color:#b44>&#34;configmaps&#34;</span>
	<span style=color:#080;font-style:italic>// ResourcePersistentVolumeClaims, number
</span><span style=color:#080;font-style:italic></span>	ResourcePersistentVolumeClaims ResourceName = <span style=color:#b44>&#34;persistentvolumeclaims&#34;</span>
	<span style=color:#080;font-style:italic>// ResourceServicesNodePorts, number
</span><span style=color:#080;font-style:italic></span>	ResourceServicesNodePorts ResourceName = <span style=color:#b44>&#34;services.nodeports&#34;</span>
	<span style=color:#080;font-style:italic>// ResourceServicesLoadBalancers, number
</span><span style=color:#080;font-style:italic></span>	ResourceServicesLoadBalancers ResourceName = <span style=color:#b44>&#34;services.loadbalancers&#34;</span>
	<span style=color:#080;font-style:italic>// CPU request, in cores. (500m = .5 cores)
</span><span style=color:#080;font-style:italic></span>	ResourceRequestsCPU ResourceName = <span style=color:#b44>&#34;requests.cpu&#34;</span>
	<span style=color:#080;font-style:italic>// Memory request, in bytes. (500Gi = 500GiB = 500 * 1024 * 1024 * 1024)
</span><span style=color:#080;font-style:italic></span>	ResourceRequestsMemory ResourceName = <span style=color:#b44>&#34;requests.memory&#34;</span>
	<span style=color:#080;font-style:italic>// Storage request, in bytes
</span><span style=color:#080;font-style:italic></span>	ResourceRequestsStorage ResourceName = <span style=color:#b44>&#34;requests.storage&#34;</span>
	<span style=color:#080;font-style:italic>// CPU limit, in cores. (500m = .5 cores)
</span><span style=color:#080;font-style:italic></span>	ResourceLimitsCPU ResourceName = <span style=color:#b44>&#34;limits.cpu&#34;</span>
	<span style=color:#080;font-style:italic>// Memory limit, in bytes. (500Gi = 500GiB = 500 * 1024 * 1024 * 1024)
</span><span style=color:#080;font-style:italic></span>	ResourceLimitsMemory ResourceName = <span style=color:#b44>&#34;limits.memory&#34;</span>
)
</code></pre></div><p>openshift额外支持的images相关的限制策略</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#080;font-style:italic>// ResourceImageStreams represents a number of image streams in a project.
</span><span style=color:#080;font-style:italic></span>ResourceImageStreams kapi.ResourceName = <span style=color:#b44>&#34;openshift.io/imagestreams&#34;</span>

<span style=color:#080;font-style:italic>// ResourceImageStreamImages represents a number of unique references to images in all image stream
</span><span style=color:#080;font-style:italic>// statuses of a project.
</span><span style=color:#080;font-style:italic></span>ResourceImageStreamImages kapi.ResourceName = <span style=color:#b44>&#34;openshift.io/images&#34;</span>

<span style=color:#080;font-style:italic>// ResourceImageStreamTags represents a number of unique references to images in all image stream specs
</span><span style=color:#080;font-style:italic>// of a project.
</span><span style=color:#080;font-style:italic></span>ResourceImageStreamTags kapi.ResourceName = <span style=color:#b44>&#34;openshift.io/image-tags&#34;</span>

</code></pre></div><p>此外，除了可以设置额度Quantity外，还可以指定配额的作用范围Scopes，其实就是作用于哪类pod上的:</p>
<ul>
<li>是否是长期运行的pod</li>
<li>是否有资源上限的pod</li>
</ul>
<p>目前只有pods数和计算资源（cpu,内存）才能指定作用域</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=color:#080;font-style:italic>// A ResourceQuotaScope defines a filter that must match each object tracked by a quota
</span><span style=color:#080;font-style:italic></span><span style=color:#a2f;font-weight:700>type</span> ResourceQuotaScope <span style=color:#0b0;font-weight:700>string</span>

<span style=color:#a2f;font-weight:700>const</span> (
	<span style=color:#080;font-style:italic>// Match all pod objects where spec.activeDeadlineSeconds，这个是标明pod的运行时长参数
</span><span style=color:#080;font-style:italic></span>	ResourceQuotaScopeTerminating ResourceQuotaScope = <span style=color:#b44>&#34;Terminating&#34;</span>
	<span style=color:#080;font-style:italic>// Match all pod objects where !spec.activeDeadlineSeconds ， 长期运行的pod
</span><span style=color:#080;font-style:italic></span>	ResourceQuotaScopeNotTerminating ResourceQuotaScope = <span style=color:#b44>&#34;NotTerminating&#34;</span>
	<span style=color:#080;font-style:italic>// Match all pod objects that have best effort quality of service， 只能用来描述资源无上限的pod数
</span><span style=color:#080;font-style:italic></span>	ResourceQuotaScopeBestEffort ResourceQuotaScope = <span style=color:#b44>&#34;BestEffort&#34;</span>
	<span style=color:#080;font-style:italic>// Match all pod objects that do not have best effort quality of service， 资源有上限的pod
</span><span style=color:#080;font-style:italic></span>	ResourceQuotaScopeNotBestEffort ResourceQuotaScope = <span style=color:#b44>&#34;NotBestEffort&#34;</span>
)
</code></pre></div><p>下面举个例子</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>compute-resources-long-running<span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>limits.cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>limits.memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scopes</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span>- NotTerminating<span style=color:#bbb>
</span></code></pre></div><p>上面的意思即是， 限制长期运行的pod最多只能创建4个，且共用4c和2G内存</p>
<p>如果不指定scopes的话，是描述的所有scopes的限制；</p>
<blockquote>
<p>本文参考<a href=https://docs.openshift.org/latest/admin_guide/quota.html>这里</a></p>
</blockquote>
<p>可以看到，通过资源配额管理，可以帮助我们解决以下问题：</p>
<ul>
<li>
<p>控制计算资源使用量</p>
<p>我们在实际生产环境中经常遇到的情况是，用户申请了过多的资源，用户应用的资源使用率太低，造成了资源的浪费。管理员通常会给集群设置超卖系数，来提高整个集群的资源使用率；另外管理员也会给用户设置资源配额上限，来限制用户使用资源的数量。通过上面的介绍我们可以看到，kubernetes的资源配额，我们可以从应用的层次上来进行配额管理，可以设置不同应用的资源配额上限。</p>
</li>
<li>
<p>控制besteffort类型POD资源使用量</p>
<p>如果POD中的所有容器都没有设置request和limit，那么这些POD的QoS类型是besteffort，这种类型的POD更方便kubernetes进行调度，但是存在的问题是，如果不对这些POD进行资源管理，那么就会导致这个kubernetes集群资源过载，会影响这个集群中的所有应用，所以通过将资源配额管理的作用范围设置成besteffort，kubernetes可以通过限制这些POD的资源，避免整个集群资源过载。</p>
</li>
<li>
<p>控制长期运行的应用和短暂运行的应用资源使用率</p>
<p>在实际使用中，在kubernetes集群中会同时存在两种类型的应用，一种是长期运行的应用，比如网站这种web应用，还有一种就是短暂运行的应用，比如编译网站的这种应用。通过资源配额管理，可以同时对这两种不同类型的应用设置资源使用上限，来控制不同应用的资源使用。</p>
</li>
</ul>
<h3 id=limitrange>LimitRange</h3>
<p><code>limtRange</code>是面向pod和container级别的，为什么只能集群管理员才可设置呢，因为这个的提出是为了防止有些应用忘记加资源边界的限定，而占用过多的资源，那么有了limitRange就给它来个默认限制。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;v1&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;LimitRange&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;core-resource-limits&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Pod&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>max</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>min</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;6Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Container&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>max</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>min</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>default</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;300m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>defaultRequest</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;200m&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100Mi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>maxLimitRequestRatio</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;openshift.io/Image&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>max</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1Gi&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;openshift.io/ImageStream&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>max</span>:<span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>openshift.io/image-tags</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>openshift.io/images</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;12&#34;</span><span style=color:#bbb>
</span></code></pre></div><p>如上使用oc create后，会看到我们对某namespace下的pod和container做了默认的资源设置，</p>
<p><img src=/limitRange.png alt=limitRange></p>
<h2 id=权限管理说明>权限管理说明</h2>
<p>这里不涉及到认证登录的介绍，openshfit支持很多认证方式，比如AllowAll，CA认证, HTPasswd, KeyStone, LDAP, Oauth等，这里为了简化，用默认的AllowAll来做权限控制的说明</p>
<p>权限管理，即访问API资源之前，必须要经过的访问策略校验，主要分为5种： AlwaysDeny、AlwaysAllow（默认）、ABAC、RBAC、Webhook</p>
<p>主要说明user, group, rule，role，policy，policybinding之间的关系，以及提出这些概念，各自是为了解决什么问题</p>
<ul>
<li>user和group</li>
</ul>
<p>说到user其实就是一个用户账号(userAccount)，用它来和k8s集群做交互（登录，kubectl等）， 但还有一个容易混淆的概念就是sercieAccount，有了userAccount为什么还又来个serviceAccount的设计， 这两者有什么区别 ？ 以下是kubernetes官方对两者的解释</p>
<blockquote>
<p>user account是为人类设计的，而service account则是为跑在pod里的进程用的，运行在pod里的进程需要调用Kubernetes API以及非Kubernetes API的其它服务（如image repository/被mount到pod上的NFS volumes中的file等）;</p>
</blockquote>
<blockquote>
<p>user account是global的，即跨namespace使用；而service account是namespaced内的，即仅在所属的namespace下使用;</p>
</blockquote>
<blockquote>
<p>user account可能会涉及到很多权限设定和商业逻辑在里面，而后者是更轻量级的，是集群用户针对某namespace内的服务使用的，一般遵循最小特权原则，如监控服务需要访问APIsever等;</p>
</blockquote>
<blockquote>
<p>useraccount需要借助第三方实现，后者系统都会默认在namesspace里创建default，亦可自定义</p>
</blockquote>
<p>两者大部分流程是一致的，都是要先认证通过再校验权限，然后才是action， 实际上一般是由userAccount来控制serviceAccount来完成特定的任务，
比如一个用户A自建了服务1和服务2， 但只想把服务2开发给用户B，这样的serviceAccount就可以排上用场了,
又或者我有几个服务，有了serviceAccount就可以来限制用户的访问权限（list, watch, update, delete）了.</p>
<p>说到group就是方便对user的权限批量操作而设计；</p>
<p>用户可以被分配到一个或多个组，每个组代表一组特定的用户。组在同时向多个用户管理权限时非常有用。</p>
<ul>
<li>rule和role</li>
</ul>
<p>rule是规则， 是对一组对象上被允许的动作（get, list, create, update, delete, deletecollection 和 watch）描述，可操作对象主要是 container，images，pod，servcie， project， user， build， imagestream， dc， route， templeate。</p>
<p>role 就是规则的集合，俗称角色， 不同对象上的不同动作，可以任意组成各种角色，系统默认的有 <code>admin basic-user cluster-admin cluster-admin edit self-provisioner view</code>；</p>
<p>policy，是策略, 保存特定namespace的所有角色roles的对象。 每个命名空间最多只有一个Policy策略。</p>
<p>rolebinding， 就是把user或者group与角色role进行关联，注意. user和group可以被关联到多个roles</p>
<p>pollicybing, 就是就是多个rolebindings的描述；</p>
<p>这样看，policy的概念提出有点儿扯淡了，感觉没什么用，其实不然，policy的提出主要是为了区分cluster-policy和local-policy的。</p>
<p>cluster policy是适用于所有namespace的角色和绑定；
local policy则是试用于具体的某个namespace的；</p>
<p>以上可以通过<code>oc describe clusterPolicy default</code>来看查看所有详细的信息；</p>
<p>小节：</p>
<p>可以通过<code>oc policy can-i --list</code>查看自己可以干些什么</p>
<p>还可以通过<code>oc policy who-can &lt;动作> &lt;资源对象></code>， 比如说查看谁能get pod之类的，就是<code>oc policy who-can get pod</code></p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>➜  openshift-docs git:<span style=color:#666>(</span>master<span style=color:#666>)</span> ✗ oc policy who-can get pod
Namespace: myproject
Verb:      get
Resource:  pods

Users:  developer
        system:admin
        system:serviceaccount:default:pvinstaller
        system:serviceaccount:myproject:deployer
        system:serviceaccount:openshift-infra:build-controller
        system:serviceaccount:openshift-infra:deployment-controller
        system:serviceaccount:openshift-infra:deploymentconfig-controller
        system:serviceaccount:openshift-infra:endpoint-controller
        system:serviceaccount:openshift-infra:namespace-controller
        system:serviceaccount:openshift-infra:pet-set-controller
        system:serviceaccount:openshift-infra:pv-binder-controller
        system:serviceaccount:openshift-infra:pv-recycler-controller

Groups: system:cluster-admins
        system:cluster-readers
        system:masters
        system:nodes
</code></pre></div><p>如果openshift自带的角色不能满足的话，还可以自定义角色role</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>$ oc get clusterrole view -o yaml &gt; clusterrole_view.yaml
$ cp clusterrole_view.yaml localrole_exampleview.yaml
$ vim localrole_exampleview.yaml
<span style=color:#080;font-style:italic># 1. Update kind: ClusterRole to kind: Role</span>
<span style=color:#080;font-style:italic># 2. Update name: view to name: exampleview</span>
<span style=color:#080;font-style:italic># 3. Remove resourceVersion, selfLink, uid, and creationTimestamp</span>
$ oc create -f path/to/localrole_exampleview.yaml -n &lt;project_you_want_to_add_the_local_role_exampleview_to&gt;
</code></pre></div><h2 id=下文介绍实战-结合实际场景-如何设置权限-即整个开发管理流程实践说明>下文介绍实战，结合实际场景，如何设置权限，即整个开发管理流程实践说明</h2>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-ec5884d27cd5b8eb79c2b702bc96a1e0>3 - 项目开发实战</h1>
<div class=lead>介绍openshift的项目开发实战。</div>
<p>下面的所有操作，都可以通过cli，web console，RestFul API实现，默认使用cli说明</p>
<h3 id=创建项目>创建项目</h3>
<p>这里是接着oc cluster up后，来说的， 默认<code>oc whoami</code>是 developer,拥有admin的Role角色，俗称项目经理（管理员）</p>
<ol>
<li>删除默认创建的项目，并创建一个实际中的项目</li>
</ol>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>oc delete project myproject
oc new-project eshop --display-name<span style=color:#666>=</span><span style=color:#b44>&#34;电商项目&#34;</span> --description<span style=color:#666>=</span><span style=color:#b44>&#34;一个神奇的网站&#34;</span>
</code></pre></div><p>现在项目管理员可以创建任意多个项目，从前面的源码可以看到目前是没法针对项目管理员去限制可创建项目上限的。</p>
<ol start=2>
<li>查看项目状态</li>
</ol>
<pre tabindex=0><code>#oc status
In project 电商项目 (eshop) on server https://192.168.31.49:8443

You have no services, deployment configs, or build configs.
Run 'oc new-app' to create an application.
</code></pre><p>空空如也，有提示语句提示可通过<code>oc new-app</code>去创建具体应用的</p>
<h3 id=创建应用>创建应用</h3>
<p>前面也说过，openshift的核心就是围绕应用的整个生命周期来的，所以从new-app说起</p>
<p>new-app的入口是<code>NewCmdNewApplication()</code>, 大部分实现是 <code>func (c *AppConfig) Run() (*AppResult, error)</code> 感兴趣的可以根据源码来理解openshift的devops理念。</p>
<ol>
<li>创建应用的方式
现在可以通过3种方式（源码， docker镜像， 模板）来创建一个应用。</li>
</ol>
<pre tabindex=0><code># oc new-app -h
#此处省略。。。
Usage:
  oc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [options]
#此处省略。。。

</code></pre><p>有很多灵活简便的方式来创建应用，甚至可以直接<code>oc new-app mysql</code>来创建一个mysql服务</p>
<p>比如下面的例子，是基于nodejs-ex项目的master分支，创建应用</p>
<pre tabindex=0><code>oc new-app https://github.com/xiaoping378/nodejs-ex.git#master
</code></pre><p>接着上面的nodejs-ex项目来说， 实际上，<code>oc new-app</code>就做了两件事，先build， 再deploy。</p>
<p>new-app一般会先创建一个bc, bc会产出一个iamge，new-app典型的还会创建一个dc，去部署新生成的image，也会创建相应的service来负载均衡方访问刚部署上的镜像里的业务。</p>
<p>这一切都是自动完成的，因为openshift origin里面有一些检测机制和默认规则，下面就针对上面那条命令看看内部都发生了什么</p>
<ul>
<li>
<p>首先openshift会执行 <code>git ls-remote</code>， 来查看此项目的所有remote分支，</p>
<p>如果存在master分支，下一步则直接clone和checkout了</p>
<p>checkout后，接着就是根据解析规则来定义如何build了。</p>
</li>
<li>
<p>build策略</p>
<p>首先会探测nodejs-ex项目根目录下，是否有dockerfile或者jenkinsfile，如果两者都没有则会根据“典型文件”判断这个项目的开发语言， 举例</p>
<p>如果存在app.json或者package.json文件，则认为是nodejs类型的项目， 更多的典型文件如下：</p>
<p><img src=/new-app-detector.png alt=detector></p>
<p>这部分的代码实现主要在 detector.go</p>
</li>
</ul>
<p>未完，待续。。。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-6e88b10af27d183cafde9b830a963484>4 - DevOps实战-0</h1>
<div class=lead>介绍openshift的DevOps实战-0。</div>
<p>主要涉及到<code>一键发布</code>，<code>快速回滚</code>，<code>弹性伸缩</code>，<code>蓝绿部署</code>方面。</p>
<ul>
<li>
<p>启动openshift</p>
<pre><code>oc cluster up --version=v1.5.0-rc.0 --metrics --use-existing-config=true
</code></pre>
<p>默认负责监控的pods占用资源太大了，可以这样限制下，或者cluster up时不加 <code>--metrics</code></p>
<pre><code>oc login -u system:admin
oc env rc hawkular-cassandra-1 MAX_HEAP_SIZE=1024M -n openshift-infra

#重建下,变量才会生效
oc scale rc hawkular-cassandra-1 --replicas 0 -n openshift-infra
oc scale rc hawkular-cassandra-1 --replicas 1 -n openshift-infra
</code></pre>
</li>
<li>
<p>建立本地Git仓</p>
<p>默认官方给出的例子基本都需要和Github结合，实在不好本地实战演示，所以本地要来一个<code>gogs</code>代码仓。</p>
<pre><code>oc login -u devloper
oc new-project ci

#先拉取所依赖镜像
docker pull openshiftdemos/gogs:0.9.97
docker pull centos/postgresql-94-centos7

#创建gogs服务，并禁用webhook时的TLS校验，不然无法触发build
oc new-app -f https://raw.githubusercontent.com/xiaoping378/gogs-openshift-docker/master/openshift/gogs-persistent-template.yaml -p SKIP_TLS_VERIFY=true -p HOSTNAME=gogs-ci.192.168.31.49.xip.io
</code></pre>
<p>上面的HOSTNAME，注意要换成自己宿主机的IPv4地址，默认创建的其他服务的路由都是这个形式的，</p>
<p>有个有意思的地方，为什么默认路由会是这种 <code>name+IP+xip.io</code> 形式呢，奥秘在 <a href=http://xip.io>http://xip.io</a> 的公共服务上。
这其实是个特殊的域DNS server，比如我们查询域名<code>gogs-ci.192.168.31.49.xip.io</code>时 ，会返回192.168.31.49的地址回来，
而这个地址恰好是我们Router的地址，这样子Router会根据route的配置负责负载到对应的POD上。自己试验下就知道怎么回事了。</p>
<pre><code>dig http://gogs-ci.192.168.31.49.xip.io +short
</code></pre>
<p>只做功能性演示，先不考虑https加密安全访问，创建完后，访问gogs服务 <code>http://gogs-ci.192.168.31.49.xip.io</code></p>
<p><img src=/openshift-gogs.png alt=gogs></p>
<p>这个项目，第一个注册用户即为管理员，比如我现在去页面注册一个叫<code>developer</code>的用户。</p>
</li>
<li>
<p>找个项目来实战吧</p>
<ul>
<li>
<p>克隆远程项目，并设置</p>
<pre><code>git clone https://github.com/xiaoping378/nodejs-ex.git &amp;&amp; cd nodejs-ex
git remote add gogs http://gogs-ci.192.168.31.49.xip.io/developer/nodejs-ex.git
</code></pre>
</li>
<li>
<p>通过web页面，在gogs上创建一个<code>nodejs-ex</code>仓库, 并如下push刚才克隆的项目</p>
<pre><code>$ git push gogs master

Username for 'http://gogs-ci.192.168.31.49.xip.io': developer
Password for 'http://developer@gogs-ci.192.168.31.49.xip.io':
Counting objects: 431, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (210/210), done.
Writing objects: 100% (431/431), 145.16 KiB | 0 bytes/s, done.
Total 431 (delta 159), reused 431 (delta 159)
To http://gogs-ci.192.168.31.49.xip.io/developer/nodejs-ex.git
* [new branch]      master -&gt; master
</code></pre>
<p>gogs的页面上会如实反馈信息</p>
<p><img src=/gogs-create-push.png alt=gogs></p>
<p>OK，现在本地项目就有了，接下来进入正题</p>
</li>
</ul>
</li>
<li>
<p>在openshift部署此nodejs应用</p>
<pre><code>   #创建web namespace
   oc new-project web

   #先拉取依赖镜像
   docker pull centos/mongodb-32-centos7
   docker pull centos/nodejs-4-centos7

   #部署此项目，并启用国内npm源和对应的git仓
   oc new-app nodejs-mongo-persistent --name=nodejs-ex -p NPM_MIRROR=https://registry.npm.taobao.org -p SOURCE_REPOSITORY_URL=http://gogs-ci.192.168.31.49.xip.io/developer/nodejs-ex.git
</code></pre>
<p>默认此模板会从指定的URL地址拉取代码，并根据预先的配置，采取<code>Source</code>编译策略，基于istag nodejs:4镜像编译出nodejs-mongo-persistent:latest镜像，编译出来的镜像又会自动触发部署。</p>
</li>
<li>
<p>最基本的DevOps能力</p>
<p>即push代码通过webhook触发自动编译，继而滚动部署</p>
<p>要实现这个目标前，需要先把webhook填写到gogs里。</p>
<p>在openshift界面上复制webhook地址</p>
<p><img src=/openshift-github-webhook.png alt=find_webhook></p>
<p>然后在gogs上填加一个webhook</p>
<p><img src=/openshift-gogs-webhook.png alt=add_webhook></p>
<p>这里我们随意修改些，然后推送代码，就会自动触发编译并滚动升级</p>
<pre><code>➜  nodejs-ex git:(master) vim views/index.html        
➜  nodejs-ex git:(master) ✗ git add .
➜  nodejs-ex git:(master) ✗ git commit -m &quot;这又是个测试&quot;
[master 082f05e] 这又是个测试
1 file changed, 1 insertion(+), 1 deletion(-)
➜  nodejs-ex git:(master) git push gogs master
Username for 'http://gogs-ci.192.168.31.49.xip.io': developer
Password for 'http://developer@gogs-ci.192.168.31.49.xip.io':
Counting objects: 4, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (3/3), done.
Writing objects: 100% (4/4), 365 bytes | 0 bytes/s, done.
Total 4 (delta 2), reused 0 (delta 0)
To http://gogs-ci.192.168.31.49.xip.io/developer/nodejs-ex.git
c3592e6..082f05e  master -&gt; master
</code></pre>
<p>编译成功后，会产生新镜像，继而触发滚动升级的截图</p>
<p><img src=/push-build-deploy.png alt=auto-deploy></p>
<p>现实中，如果项目没有很好的自动化测试的话，我们肯定不会这样操作的，除非想被开掉了。</p>
<p>其实可以简单的去掉webhook，采用手动触发build： 界面操作的话，去build界面点击<code>Start Build</code>，命令行的话如下</p>
<pre><code>oc start-build nodejs-mongo-persistent
</code></pre>
<p>另外，如果发现新版本的应用有重大缺陷，想回滚以前的部署版本，也有对应的界面和命令</p>
<pre><code>oc rollback nodejs-mongo-persistent --to-version=3
</code></pre>
<p><img src=/openshift-roll-back.png alt=rollBack></p>
</li>
<li>
<p>弹性伸缩</p>
<p>目前可以根据CPU使用率来进行弹性伸缩</p>
<p>有人问能不能基本mem进行弹性呢，其实这个是没什么意义的，一般应用都会自行缓存，内存基本只增不长， 所以cpu才能很好的实时反应业务的负载。</p>
<p>弹性伸缩前，要确保应用先行设置了cpu request，这点还没明白原因，为什么要这样，按理说，heapster一直会采集pod的资源使用情况的，HPA周期拿数据和设置的阈值对比就完了。</p>
<p>这里是部署界面的菜单栏，可以手动加上cpu request</p>
<p><img src=/openshift-dc-menu.png alt=dc-menu></p>
<p>添加 cpu request.</p>
<p><img src=/openshift-cpu-request.png alt=cpu-request></p>
<p>然后开启弹性伸缩特性，这里就不截图了，展示下命令行，我们设置成： 当cpu使用率达到80%时，就弹，最大可以弹出3个实例</p>
<pre><code>oc autoscale dc/nodejs-mongo-persistent --max=3 --cpu-percent=80
</code></pre>
<p>OK，之后我们通过ab工具简单做个压力模拟，因为环境在我的笔记本上，所以只模拟发送100万个连接，并发100的量</p>
<pre><code>ab -n 1000000 -c 100 http://nodejs-mongo-persistent-web.192.168.31.49.xip.io/
</code></pre>
<p>后台每1分钟采集一次cpu使用率，过不了一会儿，就会看到nodejs实例自动扩展了</p>
<p><img src=/openshift-autoscale-0.png alt=autoscale-0></p>
<p>当业务量降下来时，会自动减少实例，是根据平均CPU使用率来操作的。</p>
<p><img src=/openshift-autoscale-1.png alt=autoscale-1></p>
</li>
<li>
<p>蓝绿部署</p>
<p>这个也是API级别的支持，不描述具体操作细节了，原理还是以前的，从负载均衡层面入手。 实现新旧版本同时存在。
并不是所有业务都适合蓝绿部署的，要看后台数据是否允许，新旧版本同时发生读写数据</p>
<p>在openshift里实现蓝绿部署的，就太简单了。具体就是在Route层面添加同一应用的多个版本的service，并设置分流权重
截图如下</p>
<p>界面设置，只是为了展示功能，我随便添加了个service</p>
<p><img src=/openshift-blue-green-0.png alt=bluegreen-0></p>
<p>实际展示效果</p>
<p><img src=/openshift-blue-green-1.png alt=bluegreen-1></p>
</li>
</ul>
<h3 id=总结>总结</h3>
<p>Openshift平台本身在API层面实现了DevOps，所以基于它很容易做到DevOps as an service， 上面的演示可能与现实世界不太一样，</p>
<p>比如真实情况是有，测试，预发布，线上环境的，下次再分享: openshift基于jenkins pipeline如果实现更真实场景的需求。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-2d14ca450c11bea81bf6011dad4a599e>5 - DevOps实战-1</h1>
<div class=lead>介绍openshift的DevOps实战-1。</div>
<p>本文主要介绍基于openshift如何完成<code>开发->测试->线上</code>场景的变更，这是一个典型的应用生产流程，来看看openshift是如何利用容器优雅的完成整个过程的吧</p>
<p>下文基于上篇<a href=../openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-0>DevOps实战-0</a> 的<code>nodejs-ex</code>项目来说, 假设到这里，你本地已经有了nodejs-ex项目</p>
<h3 id=准备3个project>准备3个project</h3>
<p>用这3个project来模拟开发，测试，线上环境</p>
<p>现实中一般各个场景的服务器都是物理隔离的，这里可以利用<code>--node-selector</code>，来指定项目可以跑在哪些节点上。</p>
<pre tabindex=0><code>oc login -u sysetm:admin

#晚上在笔记本上写此blog，没合适的环境，单机模拟多台 -- start
oc label node 192.168.31.49 web-prod=true web-dev=true web-test=true
#晚上在笔记本上写此blog，没合适的环境，单机模拟多台 -- end

#1.创建web-dev项目
#2.授权developer为开发组项目管理员
#3.授权测试和运维人员可以从开发组拉取镜像
oc adm new-project web-dev --node-selector='web-dev=true'
oc policy add-role-to-user admin developer
oc policy add-role-to-group system:image-puller system:serviceaccounts:web-test -n web-dev
oc policy add-role-to-group system:image-puller system:serviceaccounts:web-prod -n web-dev

oc adm new-project web-test --node-selector='web-test=true'
oc policy add-role-to-user admin tester

oc adm new-project web-prod --node-selector='web-prod=true'
oc policy add-role-to-user admin ops
</code></pre><ul>
<li>
<p>你可能会注意到，这里用的<code>new-project</code> 前面还加了adm， 其实<code>oc adm</code>等效于<code>oadm</code>， 一般管理集群相关的用这个命令，这里是因为需要读取节点的标签（label）信息。</p>
</li>
<li>
<p>指定项目要运行那些节点，则是利用了注解-annotations， 即在原有的project结构上设置了注解，这样openshift在相应的项目里创建任何pod时，都对会自动注入node-selector</p>
</li>
<li>
<p>另外需要注意的，默认项目的管理员（developer）是没有权限读取node标签信息的，以前写过<a href=../openshift%E5%AE%9E%E8%B7%B5-%E6%9D%83%E9%99%90%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86>权限管理相关blog</a>，集群管理员可以授权node访问权限，即使如此developer还是不能改写项目级别的标签的，举个例子: developer在开发环境的pod上指定了<code>--node-selector='web-dev=false'</code>， 最终这个pod的node-selector会是<code>'web-dev=true, web-dev=flase'</code>, 导致最终不会被调度到任何节点上。</p>
</li>
<li>
<p>上面分别授权了3个用户，这里是不关心这些用户是否真实存在的，只是一个RABC的描述，因为是<code>oc cluster up</code>起来的环境，默认使用<code>anypassword</code>的身份认证，所以登录时，任意用户名和密码都是可以登录OK的。</p>
</li>
<li>
<p>通过<code>oc describe policybinding -n web-dev</code> 可以查看授权情况， 如果觉得默认的role不满足需求的话，也可以自定义role，另外通过<code>oc policy remove-role-from-group/user &lt;Role> &lt;name></code>可以移除相关授权，</p>
</li>
</ul>
<h3 id=初始化web-dev-web-test-web-prod环境>初始化web-dev, web-test, web-prod环境</h3>
<p>按照上篇<code>DevOps实战-0</code>里的方式</p>
<p>初始化我们的开发环境, 进入源码<code>nodejs-ex</code>目录</p>
<pre tabindex=0><code>oc new-app -f openshift/template/nodejs-mongo-persistent.json --name=nodejs-ex \
  -p NPM_MIRROR=https://registry.npm.taobao.org \
  -p SOURCE_REPOSITORY_URL=http://gogs-ci.192.168.31.49.xip.io/developer/nodejs-ex.git \
  -n web-dev
</code></pre><p>初始化测试环境，相较于上一步的模板json，只是注掉bc和更改了triggers的is,后面会详细介绍之间的差异</p>
<pre tabindex=0><code>oc new-app -f openshift/template/nodejs-mongo-persistent-test.json --name=nodejs-ex-test -n web-test
</code></pre><p>以tester登录web console，会发现只有mongodb部署上了，而前端nodejs还在等待依赖的镜像 <code>web-dev/nodejs-mongo-persistent:test</code>。</p>
<p><img src=/openshift-web-test-wait.png alt=webtest-wait></p>
<p>初始化生产环境, 这个生产的template.json有点儿简单，负载均衡和弹性伸缩都没有启用。</p>
<pre tabindex=0><code>oc new-app -f openshift/template/nodejs-mongo-persistent-prod.json --name=nodejs-ex-prod -n web-prod
</code></pre><h3 id=实战模拟-开发-测试-发布>实战模拟，开发->测试->发布</h3>
<ul>
<li>
<p>developer开发完特性或者修复完bug，push代码到镜像仓。</p>
<p>这里分享一个很方便的技巧，就是 <code>oc rsync</code>, 这个可以实时的同步本地目录到容器了，避免了频繁编译镜像和临时挂载目录到镜像里的hack了。</p>
<pre tabindex=0><code>vim ...
git add .
git commit -m &quot;fix bugs&quot;
git push gogs master
</code></pre><p>如上，由于上篇中设置了webhook, developer提交代码会触发了自动编译并部署，确认部署后的环境是否修复了bug，如果单元测试通过，那就要通知测试团队（如今大部分公司，应该没有测试人员了吧，也可以直接变更到线上）</p>
<p>测试那边的环境里一直在等待这个镜像<code>web-dev/nodejs-mongo-persistent:test</code>， 而默认developer配置成默认编译出来的是<code>web-dev/nodejs-mongo-persistent:latest</code></p>
<pre tabindex=0><code>oc login -u developer
oc tag web-dev/nodejs-mongo-persistent:latest web-dev/nodejs-mongo-persistent:v1.1
oc tag web-dev/nodejs-mongo-persistent:v1.1 web-dev/nodejs-mongo-persistent:test
</code></pre><p>如上操作后，开发人员更新版本号，然后在web-dev环境里会打上一个test的镜像tag出来，操作完如下所示</p>
<pre tabindex=0><code>➜  nodejs-ex git:(master) oc get is
NAME                      DOCKER REPO                                       TAGS               UPDATED
nodejs-mongo-persistent   172.30.1.1:5000/web-dev/nodejs-mongo-persistent   test,v1.1,latest   49 seconds ago
➜  nodejs-ex git:(master)
➜  nodejs-ex git:(master) oc get istag
NAME                             DOCKER REF                                                                                                                UPDATED          IMAGENAME
nodejs-mongo-persistent:latest   172.30.1.1:5000/web-dev/nodejs-mongo-persistent@sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7   3 minutes ago    sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7
nodejs-mongo-persistent:v1.1     172.30.1.1:5000/web-dev/nodejs-mongo-persistent@sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7   2 minutes ago    sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7
nodejs-mongo-persistent:test     172.30.1.1:5000/web-dev/nodejs-mongo-persistent@sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7   55 seconds ago   sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7
</code></pre><p>这样一来，测试环境里就会自动部署上刚才开发人员的环境了，再也不会有因为环境差异问题和测试吵吵了。</p>
<p>这一切都得益于openshift里新添加的imageStreams，它打通了编译和部署的环节，能自动通知对方，继而自动触发下一步动作。</p>
<p>测试通过后，通知Ops再重新tag成线上所需要的镜像tag，这样线上就会根据配置自动滚动升级了。</p>
<pre tabindex=0><code>#假设一个叫ops的人负责上线,那首先ops得有具备web-dev项目里编辑is的能力
oc login -u developer
#不该给ops这么高权限的，应该自定义一个只能tag is的role，这里为了简单演示
oc policy add-role-to-user edit ops -n web-dev
</code></pre><p>如上操纵，ops就具备了tag web-dev项目的镜像的能力，也可以通过UI来查看和授权</p>
<p><img src=/openshift-add-role.png alt></p>
<pre tabindex=0><code>oc login -u ops
oc tag web-dev/nodejs-mongo-persistent:v1.1 web-dev/nodejs-mongo-persistent:prod
</code></pre><p>然后打上线上依赖的镜像tag即可，发布上线，这样就完成了开发->测试->发布一条线，很快捷的<code>人工干预</code>上线了</p>
</li>
</ul>
<h2 id=总结>总结</h2>
<p>openshift 利用镜像tag的能力，来实现不同场景的同步，单纯基于docker也可以实现以上目标的，只是不够平台化，还是以前的脚本打天下，远不如openshift在API层面解决来的强大和灵活。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-be9d3e63866e8da759569cdab7948ae5>6 - 编译和目录结构介绍</h1>
<div class=lead>介绍openshift的编译和目录结构介绍。</div>
<p>介绍openshift的源码编译和目录结构组织，为了方便代码调试和了解大型Golang项目的构建方式</p>
<h3 id=编译>编译</h3>
<p>无论是openshift还是Kubernetes等大型Golang项目都用到了<code>Makefile</code>, 所以有必要从此开始说起，这里只说项目里用到的makefile特性，想了解更多的可以参考<a href=http://scc.qibebt.cas.cn/docs/linux/base/%B8%FA%CE%D2%D2%BB%C6%F0%D0%B4Makefile-%B3%C2%F0%A9.pdf>跟我一起写Makefile</a></p>
<h4 id=makefile介绍>Makefile介绍</h4>
<blockquote>
<p>makefile 关系到了整个工程的编译规则。一个工程中的源文件不计数，其按类型、功能、
模块分别放在若干个目录中，makefile 定义了一系列的规则来指定，哪些文件需要先编译，
哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为
makefile 就像一个 Shell 脚本一样，其中也可以执行操作系统的命令。 makefile 带来的好
处就是——“自动化编译”，一旦写好，只需要一个 make 命令，整个工程完全自动编译，
极大的提高了软件开发的效率。</p>
</blockquote>
<p>Makefile里的规则，就在做两件事，一个是指明依赖关系，另一个是生成目标的方法</p>
<p>Golang项目里用到的Makefile规则比较简单，基本就是定义一个目标的生成方法，下面的示例是Openshift项目里makefile中定义的第一个目标。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-makefile data-lang=makefile><span style=color:#00a000>all build</span><span style=color:#666>:</span>
	hack/build-go.sh <span style=color:#a2f;font-weight:700>$(</span>WHAT<span style=color:#a2f;font-weight:700>)</span> <span style=color:#a2f;font-weight:700>$(</span>GOFLAGS<span style=color:#a2f;font-weight:700>)</span>
<span style=color:#00a000>.PHONY</span><span style=color:#666>:</span> all build

</code></pre></div><ul>
<li>
<p><code>all build</code>，是定义的目标，看到这个就知道可以在源码的根目录上执行<code>make all build</code>来编译了</p>
</li>
<li>
<p>第二行说明生成目标的方法，就是去hack目录下执行build-go.sh脚本，这里还支持传入一些参数</p>
</li>
<li>
<p>第三行 <code>.PHONY</code>，起到一个标识的作用，没什么实际意义，是用来告诉make命令，这里是个伪目标，也可以说成是默认目标，所以在openshift的根目录上直接执行<code>make</code>, 等效于<code>make all build</code></p>
</li>
</ul>
<p>还可以自己决定是否编译出镜像或者rpm包（make release, make build-rpms）</p>
<h4 id=编译openshift>编译openshift</h4>
<p>上边介绍了，直接敲<code>make</code>就可以自动编译出所有平台（linux, mac, windows）的二进制，编译前介绍两个hack方法，</p>
<ul>
<li>
<p>在hack/build-go.sh的第二行加上<code>set -x</code>， 这样的话，shell脚本在运行时，里面的所有变量和执行路径会全部打印出来，一目了然，不用自己一行一行的加echo debug了</p>
</li>
<li>
<p>如下修改hack/build-cross.sh，不然会编译出多平台的二进制，花的时间略长啊。。。</p>
<pre tabindex=0><code># by default, build for these platforms
platforms=(
  linux/amd64
  # darwin/amd64
  # windows/amd64
  # linux/386
)
</code></pre></li>
</ul>
<p>下面简易说下执行make后，都发生了什么，只会捡关键点说。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>➜  origin git:<span style=color:#666>(</span>xxpDev<span style=color:#666>)</span> ✗ make

hack/build-go.sh  

<span style=color:#080;font-style:italic># 初始化一大堆变量，关键函数都在common.sh里实现的</span>
<span style=color:#a2f>source</span> hack/common.sh hack/util.sh hack/lib目录下的所有脚本

<span style=color:#080;font-style:italic># 还会改动GOPATH,然后会在$GOPATH/src/github.com/openshift下建个软连指向origin目录</span>
<span style=color:#a2f>export</span> <span style=color:#b8860b>GOPATH</span><span style=color:#666>=</span>_output/local/go

<span style=color:#080;font-style:italic># 最终组合成下面一条最原始的命令，来进行编译</span>
go install <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -pkgdir /home/xxp/Github/src/github.com/openshift/origin/_output/local/pkgdir/linux/amd64 <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -tags <span style=color:#b44>&#39; &#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  -ldflags <span style=color:#b44>&#39;-X github.com/openshift/origin/pkg/bootstrap/docker.defaultImageStreams=centos7 \
</span><span style=color:#b44>    -X github.com/openshift/origin/pkg/cmd/util/variable.DefaultImagePrefix=openshift/origin \
</span><span style=color:#b44>    -X github.com/openshift/origin/pkg/version.majorFromGit=3 \
</span><span style=color:#b44>    -X github.com/openshift/origin/pkg/version.minorFromGit=6+ \
</span><span style=color:#b44>    -X github.com/openshift/origin/pkg/version.versionFromGit=v3.6.0-alpha.0+83e3250-176-dirty \
</span><span style=color:#b44>    -X github.com/openshift/origin/pkg/version.commitFromGit=83e3250 \
</span><span style=color:#b44>    -X github.com/openshift/origin/pkg/version.buildDate=2017-04-06T05:34:29Z \
</span><span style=color:#b44>    -X github.com/openshift/origin/vendor/k8s.io/kubernetes/pkg/version.gitCommit=43a9be4 \
</span><span style=color:#b44>    -X github.com/openshift/origin/vendor/k8s.io/kubernetes/pkg/version.gitVersion=v1.5.2+43a9be4 \
</span><span style=color:#b44>    -X github.com/openshift/origin/vendor/k8s.io/kubernetes/pkg/version.buildDate=2017-04-06T05:34:29Z \
</span><span style=color:#b44>    -X github.com/openshift/origin/vendor/k8s.io/kubernetes/pkg/version.gitTreeState=clean&#39;</span> <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  github.com/openshift/origin/cmd/openshift <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  github.com/openshift/origin/cmd/oc <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  github.com/openshift/origin/pkg/sdn/plugin/sdn-cni-plugin <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  github.com/openshift/origin/vendor/github.com/containernetworking/cni/plugins/ipam/host-local <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  github.com/openshift/origin/vendor/github.com/containernetworking/cni/plugins/main/loopback
</code></pre></div><p>可以看到openshift会编译出5个二进制来，其中3个和网络CNI接口有关，最后会放置到_output/local/bin/linux/amd64, 并作相关的软链接（oadm, kubelet）</p>
<p>所以以后分析程序的切入点就从cmd/openshift和 cmd/oc入手就行了</p>
<p>来看下编译成果</p>
<pre tabindex=0><code>➜  origin git:(xxpDev) ✗ _output/local/bin/linux/amd64/oc version
oc v3.6.0-alpha.0+83e3250-176-dirty
kubernetes v1.5.2+43a9be4
features: Basic-Auth
</code></pre><p>看到输出<code>v3.6.0-alpha.0+83e3250-176-dirty</code>， 这就是上面编译时传进去的参数。</p>
<p><code>-X github.com/openshift/origin/pkg/version.majorFromGit=3</code>,意思是说编译文件<code>github.com/openshift/origin/pkg/version.go</code>时，对常量majorFromGit赋值为3</p>
<h3 id=项目目录结构>项目目录结构</h3>
<p>-- 未完待续</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-c4ea8f380e4d90a7452b796546dc9dea>7 - 多负载均衡方案</h1>
<div class=lead>介绍openshift的多负载均衡方案。</div>
<p>haproxy在openshift里默认有两种用处，一个种负责master的高可用，一种是负责外部对内服务的访问（ingress controller）</p>
<p>平台部署情况：</p>
<ul>
<li>3台master，etcd</li>
<li>1台node</li>
<li>1台lb（haproxy）</li>
</ul>
<h2 id=haproxy负载均衡master的高可用>haproxy负载均衡master的高可用</h2>
<p>lb负责master间的负载均衡，其实负载没那么大，更多得是用来避免单点故障</p>
<h3 id=debug介绍>Debug介绍</h3>
<ul>
<li>
<p>默认安装haproxy1.5.18版本，开启debug方法</p>
<pre tabindex=0><code># 默认systemd对haproxy做了封装，会以-Ds后台形式启动，debug信息是看不到的
systemctl stop harproxy

# vi /etc/haproxy/haproxy.cfg
 log         127.0.0.1 local3 debug

# 手动启动haproxy
haproxy -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -d
</code></pre><p>不知道是不是哪里还需要设置，打印出来的日志，信息并不是不太多</p>
<p>另外浏览<code>https://lbIP:9000</code>, 可以看到统计信息</p>
</li>
</ul>
<h3 id=配置介绍>配置介绍</h3>
<ul>
<li>
<p>使用<a href=https://github.com/xiaoping378/openshift-deploy>openshift-ansible</a>部署后，harpxy的配置如下</p>
<pre tabindex=0><code>[root@node4 ~]# cat /etc/haproxy/haproxy.cfg
# Global settings
#---------------------------------------------------------------------
global
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     20000
    user        haproxy
    group       haproxy
    daemon
    log         /dev/log local0 info    #定义debug级别

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults                                #默认配置，后面同KEY的设置会覆盖此处
    mode                    http        #工作在七层代理，客户端请求在转发至后端服务器之前将会被深度分板，所有不与RFC格式兼容的请求都会被拒绝，一些七层的过滤处理手段，可以使用。
    log                     global      #默认启用gloabl的日志设置
    option                  httplog     #默认日志类别为http日志格式
    option                  dontlognull #不记录健康检查日志信息（端口扫描，空信息）
#    option http-server-close
    option forwardfor       except 127.0.0.0/8 #如果上游服务器上的应用程序想记录客户端的真实IP地址，haproxy会把客户端的IP信息发送给上游服务器，在HTTP请求中添加”X-Forwarded-For”字段,但当是haproxy自身的健康检测机制去访问上游服务器时是不应该把这样的访问日志记录到日志中的，所以用except来排除127.0.0.0，即haproxy自身
    option                  redispatch         #代理的服务器挂掉后，强制定向到其他健康的服务器，避免cookie信息过时，仍可正常访问
    retries                 3     #3次连接失败就认为后端服务器不可用
    timeout http-request    10s   #默认客户端发送http请求的超时时间， 防DDOS攻击手段
    timeout queue           1m    #当后台服务器maxconn满了后，haproxy会把client发送来的请求放进一个队列中，一旦事件超过timeout queue，还没被处理，haproxy会自动返回503错误。
    timeout connect         10s   #haproxy与后端服务器连接超时时间，如果在同一个局域网可设置较小的时间
    timeout client          300s  #默认客户端与haproxy连接后，数据传输完毕，不再有数据传输，即非活动连接的超时时间
    timeout server          300s  #定义haproxy与后台服务器非活动连接的超时时间
    timeout http-keep-alive 10s   #默认新的http请求建立连接的超时时间，时间较短时可以尽快释放出资源，节约资源。和http-request配合使用
    timeout check           10s   #健康检测的时间的最大超时时间
    maxconn                 20000 #最大连接数

listen stats :9000
    mode http
    stats enable
    stats uri /

frontend  atomic-openshift-api
    bind *:8443
    default_backend atomic-openshift-api
    mode tcp        #在此模式下，客户端和服务器端之前将建立一个全双工的连接，不会对七层（http）报文做任何检查
    option tcplog

backend atomic-openshift-api
    balance source  #是基于请求源IP的算法，此算法对请求的源IP时行hash运算，然后将结果除以后端服务器的权重总和，来判断转发至哪台后端服务器，这种方法可保证同一客户端IP的请求始终转发到固定定的后端服务器。
    mode tcp
    server      master0 192.168.56.100:8443 check
    server      master1 192.168.56.101:8443 check
    server      master2 192.168.56.102:8443 check
</code></pre><p><a href=http://cbonte.github.io/haproxy-dconv/1.5/configuration.html>官方文档</a>介绍的非常详细，感兴趣的可以继续深入研究</p>
</li>
</ul>
<h2 id=router负责外部对内服务的访问>Router负责外部对内服务的访问</h2>
<h3 id=部署一个router并实现高可用>部署一个Router并实现高可用</h3>
<p>router是由harpoxy来承担的， 可以理解成kubernetes里的ingress controller部分，默认跑在容器里。</p>
<ul>
<li>
<p>使能 default项目下router，可以访问hostnetwork</p>
<pre><code>oc adm policy add-scc-to-user hostnetwork system:serviceaccount:default:router
</code></pre>
</li>
<li>
<p>使能其可以查看 label</p>
<pre><code>oc adm policy add-cluster-role-to-user \
    cluster-reader \
    system:serviceaccount:default:router
</code></pre>
</li>
<li>
<p>部署1个router， 选择具有标签<code>router=true</code>的节点</p>
<pre><code># 对节点设置标签
oc label 192.168.56.110 router=true
# 部署并指定serviceaccount
oc adm router router --replicas=1 --selector='router=true'  --service-account=router
</code></pre>
</li>
<li>
<p>设置router自身的高可用，参考<a href=https://docs.openshift.org/latest/admin_guide/high_availability.html#admin-guide-high-availability>这里</a></p>
<p>默认使用keepalived实现多个router的高可用，访问router变成访问VIP地址，keepalived再根据权重和健康监测，利用VRRP通告外界后台到底那个router在服务。</p>
<pre><code># 添加另一个node作为冗余
oc label no 192.168.56.111 router=true
oc scale dc router --replicas=2

#绑定serviceaccount特权，因为keepalived要操作iptables
oc adm policy add-scc-to-user privileged system:serviceaccount:default:ipfailover

#创建keepalived并指定VIP
oc adm ipfailover ha-router \
  --replicas=2 --watch-port=80 \
  --selector=&quot;router=true&quot; \
  --virtual-ips=&quot;192.168.56.170&quot; \
  --iptables-chain=&quot;INPUT&quot; \
  --service-account=ipfailover --create
</code></pre>
<p>这样，刚才创建的router就自高可用了，通过<code>192.168.56.170</code>来访问，有一点值得注意，</p>
<ul>
<li>按照现在的例子，如果以后还有router做高可用的话，要加上<code>--vrrp-id-offset=1</code>，保证一个vip用一个独有的vrrp-id。</li>
</ul>
</li>
</ul>
<h3 id=router分片>Router分片</h3>
<p>路由分片的概念，就是集群内有多个router，通过label来负责不同的routes。</p>
<p>这样可以实现一个project独享一个router，或者某几个route独享一个router，再或者大型集群，更多样化的需求，用这个router sharding的概念也可以满足。</p>
<p>我现在还没有具体的场景，先不实践，后续有机会会跟进更新下。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-85297fa1083d97d27a86dc5bf59b1493>8 - 镜像管理</h1>
<div class=lead>介绍openshift的镜像管理方案。</div>
<p>刚接触docker时，第一个接触到的应该就是镜像了，docker之所以如此火热，个人认为一大部分原因就是这个镜像的提出，极大的促进了DevOps推广和软件复用的能力。</p>
<p>而openshift对镜像的管理非常强大，直到写这篇blog，我才真正意识到这点，甚至犹豫是不是要放到开发实战篇后再来写<code>镜像管理</code>。</p>
<p>简要说下openshift里使用镜像的情况：</p>
<ul>
<li>首先openshift可以利用任何实现了<code>Docker registry API</code>的镜像仓，比如，Vmware的Harbor项目，Docker hub以及集成镜像仓（ integrated registry）</li>
<li>集成镜像仓，openshift内部的，可以动态生成，自动让用户编译的镜像有地方存， 其次它还负责通知openshift镜像的变动，然后openshift会根据策略去决定编译其他依赖镜像还是部署应用</li>
<li>第三方镜像， 可通过命令<code>oc import-image &lt;stream></code>来实时获取镜像tag信息并转换成镜像流，继而触发后续的编译或者部署。</li>
<li>当然<code>oc new-app</code>也支持直接从第三方镜像仓或者本地镜像里启动一个应用</li>
</ul>
<p>文末有安装集成镜像仓的说明，先介绍image Streams 和 istag的概念和应用场景。</p>
<h2 id=镜像管理>镜像管理</h2>
<ul>
<li>
<p>openshift基于docker的image概念又延伸出了Image Streams和Image Stream Tags概念</p>
<p>默认openshift项目下会有一些镜像流，是供自带模板里用的，所以想加速部署模板的话，可以在改这里，通过istag指向本地镜像仓。</p>
<pre tabindex=0><code>oc get is -n openshift
oc get istag -n openshift
</code></pre><p>image，通俗讲就是对应用运行依赖（库，配置，运行环境）的一个打包。<code>docker pull push</code>， 就是操作的镜像。
为什么openshift还要抽象出is和istag呢，主要是为了打通集成编译和部署环节（bc和dc），原生API就支持了DevOps理念。后面会细讲bc和dc</p>
<p>is,开发人员可以理解成git的分支，每个分支都会编译很多临时版本出来，这个就是对应到is～=分支和istag～=版本号。
其实is和istag只是记录了一些映射关系，并不会存放实际镜像数据，比如is里记录了build后要output的镜像仓地址和所有tags，而istag里又记录了具体某个tag与image（可能是存于外部镜像仓，也能是某个is）的关系， 利用此实现了bc/dc和镜像的解耦。</p>
<p>这里通过部署jenkins服务，来初步了解下具体的含义,</p>
<ul>
<li>创建ci项目</li>
</ul>
<pre tabindex=0><code>oc new-project ci
# 先拉取必要镜像
docker pull openshift/jenkins-1-centos7

#通过模板部署，下面一条命令就可以创建一个临时的jenkins服务的
#oc new-app jenkins-ephemeral
#跑之前我们先来注意几点
</code></pre><ul>
<li>更改默认的is</li>
</ul>
<p>先来查看默认的is</p>
<pre tabindex=0><code>oc get template jenkins-ephemeral -n openshift -o json
...
&quot;triggers&quot;: [
  {
      &quot;imageChangeParams&quot;: {
          &quot;automatic&quot;: true,
          &quot;containerNames&quot;: [
              &quot;jenkins&quot;
          ],
          &quot;from&quot;: {
              &quot;kind&quot;: &quot;ImageStreamTag&quot;,
              &quot;name&quot;: &quot;${JENKINS_IMAGE_STREAM_TAG}&quot;,
              &quot;namespace&quot;: &quot;${NAMESPACE}&quot;
          },
          &quot;lastTriggeredImage&quot;: &quot;&quot;
      },
      &quot;type&quot;: &quot;ImageChange&quot;
  },
  {
      &quot;type&quot;: &quot;ConfigChange&quot;
  }
  ]
...
{
    &quot;name&quot;: &quot;NAMESPACE&quot;,
    &quot;displayName&quot;: &quot;Jenkins ImageStream Namespace&quot;,
    &quot;description&quot;: &quot;The OpenShift Namespace where the Jenkins ImageStream resides.&quot;,
    &quot;value&quot;: &quot;openshift&quot;
},
{
    &quot;name&quot;: &quot;JENKINS_IMAGE_STREAM_TAG&quot;,
    &quot;displayName&quot;: &quot;Jenkins ImageStreamTag&quot;,
    &quot;description&quot;: &quot;Name of the ImageStreamTag to be used for the Jenkins image.&quot;,
    &quot;value&quot;: &quot;jenkins:latest&quot;
}
...
</code></pre><p>可以看到默认模板里部署jenkins时，会从openshfit的namespace里拉取jenkins:latest的镜像, 去openshift项目里找找看，确实存在对应的is</p>
<pre tabindex=0><code>➜  ~ oc get is -n openshift | grep jenkins
jenkins      170.16.131.234:5000/openshift/jenkins      latest,1                     2 days ago

➜  ~ oc get istag -n openshift | grep jenkins:latest
jenkins:latest      openshift/jenkins-1-centos7@sha256:ab590529e20470e53c1d4b6b970de5d4fd357d864320735a75c1df0e2fffde07       2 days ago   sha256:ab590529e20470e53c1d4b6b970de5d4fd357d864320735a75c1df0e2fffde07
</code></pre><p>上面的命令的输出，有两个点要阐述下，</p>
<ul>
<li>is里的<code>170.16.131.234:5000/openshift/jenkins</code>是个可有可无的地址，一般系统会填写集成镜像仓的地址</li>
<li>而istag里<code>openshift/jenkins-1-centos7@sha256:ab</code>则是指明了对应tag的镜像来源，</li>
</ul>
<p>这样的话，默认执行<code>oc new-app jenkins-ephemeral</code>的话，会从docker.io那里拉取镜像 openshift/jenkins-1-centos7@sha256:ab590529e20470e53c1d4b6b970de5d4fd357d864320735a75c1df0e2fffde07</p>
<p>为了加速部署，我们把刚才pull下来的镜像，push到集成镜像仓里</p>
<pre tabindex=0><code>#添加用户
#htpasswd /etc/origin/master/htpasswd xxp
docker tag openshift/jenkins-1-centos7 hub2.300.cn/openshift/jenkins
docker login -u developer -p `oc whoami -t` hub2.300.cn
docker push hub2.300.cn/openshift/jenkins
</code></pre><p>push完后再来看istag的变化，由以前的<code>openshift/jenkins-1-centos</code>变为了<code>170.16.131.234:5000/openshift/jenkins</code></p>
<pre tabindex=0><code>➜  ~ oc get istag -n openshift | grep jenkins:latest
jenkins:latest      170.16.131.234:5000/openshift/jenkins@sha256:dc0f434a492d11d6ae13711e77f87303a06a8fc0fb3a97ae327a4b88c33435b6   21 hours ago   sha256:dc0f434a492d11d6ae13711e77f87303a06a8fc0fb3a97ae327a4b88c33435b6

</code></pre><p>这里是push到集成镜像仓后，系统会自动更新对应的is和istag， 为了加速部署，更改istag还可以通过import-image从私有镜像仓（harbor）里来完成</p>
<p>这样再来部署jenkins应用就快速多了</p>
<p>还有很多更细致的东西，比如如何周期同步第三方镜像仓等等，有需要的查看<a href=https://docs.openshift.org/latest/dev_guide/managing_images.html#creating-an-image-stream-by-manually-pushing-an-image>官文</a></p>
</li>
</ul>
<h2 id=安装独立镜像仓>安装独立镜像仓</h2>
<p>可以通过openshift-ansible一次性安装OK，这里使用CLI安装，正好可以串一下openshift里的各种概念的使用场景。</p>
<h3 id=部署>部署</h3>
<pre tabindex=0><code>oc project default
oc adm policy add-scc-to-user privileged system:serviceaccount:default:registry
oc label node 192.168.56.102 registry=true
#使用hostPath存储,要在node上放行权限，默认registry用的1001 userID， 不然后续挂载进去的volume，没有写权限
sudo chown 1001:root /home/registry
#注意要指定使用的镜像版本，默认是拉取最新的， 一定要指明``--volume``参数，不然deploy，不会挂载主机目录，官文这里遗漏了。
oc adm registry --images=&quot;openshift/origin-docker-registry:v1.4.1&quot; --selector=&quot;registry=true&quot; --mount-host=&quot;/home/registry&quot; --service-account=registry --volume='/registry'```

中间莫名出现部署``error``的状态，重新部署了下``oc deploy docker-registry --retry``

### 加密镜像仓

- 先拿到serviceIP
```bash
[root@node0 master]# oc get svc docker-registry
NAME              CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
docker-registry   170.16.132.252   &lt;none&gt;        5000/TCP   1h
</code></pre><ul>
<li>创建自签名证书,如果已经有了，跳过此步</li>
</ul>
<pre tabindex=0><code>oc adm ca create-server-cert \
    --signer-cert=/etc/origin/master/ca.crt \
    --signer-key=/etc/origin/master/ca.key \
    --signer-serial=/etc/origin/master/ca.serial.txt \
    --hostnames='hub.example.com,docker-registry.default.svc.cluster.local,170.16.132.252:5000' \
    --cert=/etc/secrets/registry.crt \
    --key=/etc/secrets/registry.key
</code></pre><ul>
<li>
<p>创建secret， secret是专门来保存敏感信息的，比如密码，sshkey，token信息等等</p>
<p>更详细的介绍可以查看<a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/secrets.md>这里</a></p>
<pre tabindex=0><code>oc secrets new registry-secret \
    /etc/secrets/registry.crt \
    /etc/secrets/registry.key
</code></pre></li>
<li>
<p>绑定secret到serviceaccount</p>
</li>
</ul>
<pre tabindex=0><code>oc secrets link registry registry-secret
oc secrets link default  registry-secret
</code></pre><ul>
<li>更新dc，添加volume，把新创建的secret挂进去</li>
</ul>
<pre tabindex=0><code>oc volume dc/docker-registry --add --type=secret \
    --name=docker-registry --secret-name=registry-secret -m /etc/secrets
</code></pre><p>如果想移除的话，如下</p>
<pre tabindex=0><code>oc volume dc/docker-registry --remove --name=docker-registry
</code></pre><ul>
<li>更新环境变量</li>
</ul>
<pre tabindex=0><code>oc set env dc/docker-registry \
    REGISTRY_HTTP_TLS_CERTIFICATE=/etc/secrets/registry.crt \
    REGISTRY_HTTP_TLS_KEY=/etc/secrets/registry.key
</code></pre><ul>
<li>更新健康监测 HTTP->HTTPS</li>
</ul>
<pre tabindex=0><code>oc patch dc/docker-registry -p '{&quot;spec&quot;: {&quot;template&quot;: {&quot;spec&quot;: {&quot;containers&quot;:[{
    &quot;name&quot;:&quot;registry&quot;,
    &quot;livenessProbe&quot;:  {&quot;httpGet&quot;: {&quot;scheme&quot;:&quot;HTTPS&quot;}}
  }]}}}}'
</code></pre><pre tabindex=0><code>oc patch dc/docker-registry -p '{&quot;spec&quot;: {&quot;template&quot;: {&quot;spec&quot;: {&quot;containers&quot;:[{
      &quot;name&quot;:&quot;registry&quot;,
      &quot;readinessProbe&quot;:  {&quot;httpGet&quot;: {&quot;scheme&quot;:&quot;HTTPS&quot;}}
    }]}}}}'
</code></pre><ul>
<li>验证是否OK</li>
</ul>
<pre tabindex=0><code>[root@node0 master]# oc logs dc/docker-registry  | grep tls
time=&quot;2017-03-02T16:01:41.113619323Z&quot; level=info msg=&quot;listening on :5000, tls&quot; go.version=go1.7.4 instance.id=594aa09b-4540-4e38-a85a-851261cd1254
</code></pre><h3 id=添加路由>添加路由</h3>
<pre tabindex=0><code>oc create route passthrough registry --service=docker-registry --hostname=hub2.300.cn
</code></pre><h3 id=登录镜像仓>登录镜像仓</h3>
<pre tabindex=0><code>oc policy add-role-to-user admin developer -n default
oc login -u developer
docker login -u developer -p `oc whoami -t` hub2.300.cn
</code></pre><ul>
<li>push镜像</li>
</ul>
<pre tabindex=0><code>docker push hub2.300.cn/default/busybox
</code></pre><h3 id=安装镜像仓console>安装镜像仓console</h3>
<ul>
<li>oc利用官方模板安装</li>
</ul>
<pre tabindex=0><code>oc create -n default -f https://raw.githubusercontent.com/openshift/openshift-ansible/master/roles/openshift_hosted_templates/files/v1.4/origin/registry-console.yaml
oc create route passthrough --service registry-console \
    --hostname hub3.300.cn \
    -n default
oc new-app -n default --template=registry-console \
    -p OPENSHIFT_OAUTH_PROVIDER_URL=&quot;https://192.168.31.100:8443&quot; \
    -p REGISTRY_HOST=$(oc get route docker-registry -n default --template='{{ .spec.host }}') \
    -p COCKPIT_KUBE_URL=$(oc get route registry-console -n default --template='https://{{ .spec.host }}')
</code></pre><ul>
<li>登录浏览器打开<code>https://hub3.300.cn/registry</code>,使用已有的账户登录，比如这里是默认的developer和developer。
<img src=/openshift-registry-console.png alt=界面></li>
</ul>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-85fad76100d7a48bf023fda8e541db66>9 - 性能优化指南</h1>
<div class=lead>介绍openshift的性能优化方案。</div>
<p>主要参考的官方<a href=https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/scaling_and_performance_guide/>链接</a>， 本文是基于openshift 3.5说的。</p>
<h2 id=概览>概览</h2>
<p>本指南提供了如何提高OpenShift容器平台的集群性能和生产环境下的最佳实践。 主要包括建立，扩展和调优OpenShift集群的推荐做法。</p>
<p>个人看法，其实性能这个东西是个权衡的过程，根据自身硬件条件和实际需求，选择适合自己的调优手段。</p>
<h2 id=安装实践>安装实践</h2>
<h3 id=网络依赖>网络依赖</h3>
<p>首先安装自然要选择官方的<a href=https://github.com/openshift/openshift-ansible>openshift-ansible项目</a>， 默认是rpm安装方式，需要依赖网络，比如要去联网下载<code>atomic-openshift-*, iptables, 和 docker</code>包依赖，</p>
<p>如果有不能联网的节点，可以参考我之前写的<a href=https://github.com/xiaoping378/openshift-deploy>离线安装openshift</a>。</p>
<h3 id=ansible优化>ansible优化</h3>
<p>官方推荐使用ansible安装，这里说下针对ansible的优化，以提高安装效率，主要参考<a href=https://www.ansible.com/blog/ansible-performance-tuning>ansible官方blog</a>,</p>
<p>如果参考上文离线安装的话，不建议跨外网连接rpm仓或者镜像仓，下面是推荐的ansible配置</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#080;font-style:italic># cat /etc/ansible/ansible.cfg</span>
<span style=color:#080;font-style:italic># config file for ansible -- http://ansible.com/</span>
<span style=color:#080;font-style:italic># ==============================================</span>
<span style=color:#666>[</span>defaults<span style=color:#666>]</span>
<span style=color:#b8860b>forks</span> <span style=color:#666>=</span> <span style=color:#666>20</span> <span style=color:#080;font-style:italic># 20个并发是理想值，太高的话中间会有概率出错</span>
<span style=color:#b8860b>host_key_checking</span> <span style=color:#666>=</span> False
<span style=color:#b8860b>remote_user</span> <span style=color:#666>=</span> root
<span style=color:#b8860b>roles_path</span> <span style=color:#666>=</span> roles/
<span style=color:#b8860b>gathering</span> <span style=color:#666>=</span> smart
<span style=color:#b8860b>fact_caching</span> <span style=color:#666>=</span> jsonfile
<span style=color:#b8860b>fact_caching_connection</span> <span style=color:#666>=</span> <span style=color:#b8860b>$HOME</span>/ansible/facts
<span style=color:#b8860b>fact_caching_timeout</span> <span style=color:#666>=</span> <span style=color:#666>600</span>
<span style=color:#b8860b>log_path</span> <span style=color:#666>=</span> <span style=color:#b8860b>$HOME</span>/ansible.log
<span style=color:#b8860b>nocows</span> <span style=color:#666>=</span> <span style=color:#666>1</span>
<span style=color:#b8860b>callback_whitelist</span> <span style=color:#666>=</span> profile_tasks

<span style=color:#666>[</span>privilege_escalation<span style=color:#666>]</span>
<span style=color:#b8860b>become</span> <span style=color:#666>=</span> False

<span style=color:#666>[</span>ssh_connection<span style=color:#666>]</span>
<span style=color:#b8860b>ssh_args</span> <span style=color:#666>=</span> -o <span style=color:#b8860b>ControlMaster</span><span style=color:#666>=</span>auto -o <span style=color:#b8860b>ControlPersist</span><span style=color:#666>=</span>600s
<span style=color:#b8860b>control_path</span> <span style=color:#666>=</span> %<span style=color:#666>(</span>directory<span style=color:#666>)</span>s/%%h-%%r
<span style=color:#b8860b>pipelining</span> <span style=color:#666>=</span> True <span style=color:#080;font-style:italic># 多路复用，减少了控制机和目标间的连接次数，加速了性能。</span>
<span style=color:#b8860b>timeout</span> <span style=color:#666>=</span> <span style=color:#666>10</span>
</code></pre></div><h3 id=网络配置>网络配置</h3>
<p>这里必须要提下，一定要安装前做好网络规划，不然后面改起来很麻烦，</p>
<p>默认是每个node上最多可跑110个pods，这个要看自身硬件条件，比如说我的环境全是高配物理机，我就改成了，每个节点可以跑1024个pods，这个主要改下面的地方。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#b8860b>openshift_node_kubelet_args</span><span style=color:#666>={</span><span style=color:#b44>&#39;pods-per-core&#39;</span>: <span style=color:#666>[</span><span style=color:#b44>&#39;0&#39;</span><span style=color:#666>]</span>, <span style=color:#b44>&#39;max-pods&#39;</span>: <span style=color:#666>[</span><span style=color:#b44>&#39;1024&#39;</span><span style=color:#666>]</span>, <span style=color:#b44>&#39;image-gc-high-threshold&#39;</span>: <span style=color:#666>[</span><span style=color:#b44>&#39;90&#39;</span><span style=color:#666>]</span>, <span style=color:#b44>&#39;image-gc-low-threshold&#39;</span>: <span style=color:#666>[</span><span style=color:#b44>&#39;80&#39;</span><span style=color:#666>]}</span>
<span style=color:#b8860b>osm_host_subnet_length</span><span style=color:#666>=</span><span style=color:#666>10</span>
<span style=color:#b8860b>osm_cluster_network_cidr</span><span style=color:#666>=</span>12.1.0.0/12
</code></pre></div><p>关于网络的更多优化项，后面有单独介绍。</p>
<h2 id=主机节点优化>主机节点优化</h2>
<p>openhshift集群里，除了pod间的网络通信外，最大的开销就是master和etcd间的通信了，openshift的master集成了k8s里的api-server, master主要通过etcd来交互node状态，网络配置，secrets和卷挂载等等信息</p>
<h3 id=master侧>master侧</h3>
<p>主要优化点包括：</p>
<ul>
<li>master和etcd尽量部署在一起.</li>
<li>高可用集群里，master尽量部署在低延迟的网络里.</li>
<li>确保**/etc/origin/master/master-config.yaml**里的etcds，第一个是本地的etcd实例.</li>
</ul>
<h3 id=node侧>node侧</h3>
<p>node节点的配置主要在**/etc/origin/node/node-config.yaml**里， 优化点视具体情况定，主要可以优化的点有：</p>
<ul>
<li>iptables synchronization period,</li>
<li>MTU值</li>
<li>代理模式</li>
</ul>
<p>配合自文件里还可以配置kubelet的启动参数，主要关注两点<code>pods-per-core 和 max-pods</code>，这两个决定了node节点的pod数，两者不一致时，<code>取值小的</code>。如果数值过大（严重超卖）会导致：</p>
<ul>
<li>增加cpu消耗，主要是docker和openshift自身管理消耗的</li>
<li>降低pod调度效率</li>
<li>加大了OOM的风险</li>
<li>分配pod ip出异常（可能地址池不够了，默认254个ip）</li>
<li>影响应用的性能</li>
</ul>
<blockquote>
<p>有一点要注意，k8s体系的平台，跑一个pod，实际会启动两个容器，一个pause先于业务容器启动，主要负责网络事项，所以跑10个pods，实际上会运行20个容器</p>
</blockquote>
<h3 id=etcd节点>etcd节点</h3>
<p>etcd是一个分布式的key-value存储，所以有条件的话，存储读写性能的提升，上ssd最好了。</p>
<p>其次是网络的优化，比如和masters部署在一起，或者提供专网连接。</p>
<blockquote>
<p>etcd实际使用中，最好的提升手段，是关注内存，这个官网有个换算公式的，多少pods推荐多大内存的使用</p>
</blockquote>
<h3 id=内核优化>内核优化</h3>
<p>上面的所有节点，内核层面都需要做些优化，这里推荐使用tuned工具来做，这点属于常规运维优化了，具体可以参考<a href=https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html-single/Performance_Tuning_Guide/index.html#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuned>这里</a>来做， 不想明白原理的，可以如下 快速操作，redhat的人已经自动化了。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>yum install tuned
systemctl start tune
systemctl <span style=color:#a2f>enable</span> tuned
tuned-adm profile throughput-performance<span style=color:#666>)</span>来做
</code></pre></div><h2 id=资源优化>资源优化</h2>
<h3 id=超卖现象>超卖现象</h3>
<p>主要是资源管理这块儿的注意点， 我以前有<a href=../openshift%E5%AE%9E%E8%B7%B5-%E6%9D%83%E9%99%90%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86>blog</a>专门介绍过，主要值得一提的是，这里有个隐形的QoS级别</p>
<p><img src=/openshift-qos.png alt></p>
<p>Guaranteed类型的pod是优先级最高的，是有保证的，只会在程序本身“异常”超出limits（一般的应用在pod层设置了limits，就不会超过该限制的，除非是java系的，其需要用环境变量来控制），才会被杀掉，其他类型的配额在集群资源紧张时会被kill掉的。</p>
<p>这块儿更多的细节也可以参考<a href=https://docs.openshift.org/latest/admin_guide/overcommit.html>官文</a></p>
<h3 id=镜像>镜像</h3>
<p>这里需要注意的是，可以提前把需要的基础镜像先pull到node节点上，比如<code>origin-pod</code>镜像等，还有其他自定义的<code>Gold 镜像</code>，这样可以减少应用部署时间。</p>
<p>如果是采用镜像方式部署集群的话，也可以采取提前pull镜像的方式，当然有私有镜像仓的，可以忽略。</p>
<p>主要是现在默认的镜像拉取策略就是<code>IfNotPresent</code>，才能完成加速部署的效果</p>
<h3 id=线上debug容器>线上debug容器</h3>
<p>线上容器环境可能很<code>干净</code>， 如何调试一个线上正在运行的容器，估计困扰过很多开发人员，这个其实利用docker原生特性，可以很easy的做到</p>
<p>比如你自己build一个工具包镜像tools，里面装有<code>tcpdump，perf，strace</code>等等debug工具，如下可以很方便的动态的嵌入到运行的线上容器中。</p>
<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>docker run -t --pid<span style=color:#666>=</span>container:production <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --net<span style=color:#666>=</span>container:production <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --cap-add sys_admin <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  --cap-add sys_ptrace <span style=color:#b62;font-weight:700>\
</span><span style=color:#b62;font-weight:700></span>  tools
</code></pre></div><p>当然万能的日志调式，也是OK的。</p>
<h2 id=存储优化>存储优化</h2>
<p>这里的存储说的是docker的graph驱动（ Device Mapper, Overlay, 和 Btrfs），首先overlay在启停容器速度方面要优于devicemapper，其还能带来更优良的页面缓存共享，但存在POSIX兼容性问题，比如不支持SELinux。</p>
<p>官方是推荐使用thin devicemapper的，但需要额外的独立块盘才能搞定。如果系统是7.2的话，使用overlay亦可，关闭selinux的代价就是牺牲部分容器安全。</p>
<h2 id=路由和网络优化>路由和网络优化</h2>
<p>openshift里的Router是基于haproxy做的，等价于k8s里的nginx ingress服务，提供集群内的service供外访问能力。</p>
<p>一般一个4 vCPU/16GB RAM的虚机，可以提供7000-32000 HTTP keep-alive连接请求，这取决于连接是否加密和页面大小，如果是物理机的话，性能会翻倍。</p>
<p>可通过Router sharding的技术来扩展性能。下图各种配置是统计性能（默认100个routes）</p>
<p><img src=/haproxy-perf.png alt></p>
<h3 id=网络优化>网络优化</h3>
<p>默认openshift提供了一个基于ovs的sdn方案，其中涉及到了vxlan, OpenFlow和iptables，当然这些相关的优化项社区已经有很成熟的优化点和方法了，比如增大MTU，UDP-offload，多路复用等等，</p>
<p>这里重点说下vxlan， 基于二层网络，vxlan从4096提升到了16百万多个，vxlan就是把原报文封装进UDP报文，以提供所有pods间通信的能力，自然这样会增加cpu解封包的开销，具体网络吞吐取决于cpu的性能，另外还会额外增加延时响应。</p>
<p>直白的说，现在云主机或物理机的cpu都可以打满千兆网卡，如果是万兆网卡，那vxlan网络的吞吐带宽会卡在CPU上，这是所有overlay网络的现状。</p>
<p>如果你的主机用用万兆或者40Gbps, 那就要考虑网络的性能优化了：</p>
<ul>
<li>通过直接路由，负责pod间通信，不过需要手动维护node节点添加删除时的路由变化。</li>
<li>条件允许的话，可以考虑BGP的calico网络方案</li>
<li>另外就是购置支持udp-offload的网卡</li>
</ul>
<p>值得注意点是，及时使用了udp-offload的网卡，和非overlay网络比，延迟是不会减少的，只是减少了cpu开销，从而提高了带宽吞吐。</p>
<h3 id=子网大小>子网大小</h3>
<p>现在openshift-ansible项目默认的安装出来的配置是：</p>
<ul>
<li>集群里内最多1024个节点</li>
<li>每个节点最多可以跑510个pods</li>
<li>支持65,536个service</li>
</ul>
<p>比如我要搞一个8192个节点的集群，每个节点允许510个pods运行：</p>
<pre tabindex=0><code>[OSE3:vars]
osm_cluster_network_cidr=10.128.0.0/10
</code></pre><h2 id=监控>监控</h2>
<p>都知道k8s里的弹性伸缩，依赖于Heapster, 而openshift内置的监控系统又是用的自家的Haw系列，导致监控镜像相当的大</p>
<p>在opneshift里有两点要提的是<code>METRICS_RESOLUTION</code>和<code>METRICS_DURATION</code>变量，前者是默认是30s,指的是监控时间间隔，后者默认是7天，指的是监控数据保留时长（过期就会删掉）。</p>
<p><img src=/openshift-monitor%E7%A9%BA%E9%97%B4.png alt></p>
<p>默认的监控体系(Cassandra/Hawkular/Heapster)可以监控25000个pods。</p>
<h2 id=总结>总结</h2>
<p>其实openshift基于k8s提供了一站式解决方案, 如果公司不具备k8s二次开发能力，openshift足矣。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-6c736738cfe8af1d9bca20191186afaf>10 - 网络整理</h1>
<div class=lead>介绍openshift的网络方案。</div>
<p>介绍利用openshift-ansible项目安装后的生产环境里的网络情况。</p>
<p>待整理。。。</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-cb9d98fa953e6ad69dc9105c1c11550b>11 - 监控梳理</h1>
<div class=lead>介绍openshift的监控方案。</div>
<p>未完搞 ...</p>
</div>
<div class=td-content style=page-break-before:always>
<h1 id=pg-65d2c9ba3dda05a2d82269e367f5bdee>12 - 日志分析</h1>
<div class=lead>介绍openshift的日志方案。</div>
<p>未完搞 ...</p>
</div>
</main>
</div>
</div>
<footer class="bg-dark py-5 row d-print-none">
<div class="container-fluid mx-sm-5">
<div class=row>
<div class="col-6 col-sm-4 text-xs-center order-sm-2">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="个人邮箱 xiaoping378@163.com" aria-label="个人邮箱 xiaoping378@163.com">
<a class=text-white target=_blank rel=noopener href=mailto:xiaoping378@163.com aria-label="个人邮箱 xiaoping378@163.com">
<i class="fa fa-envelope"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=微博 aria-label=微博>
<a class=text-white target=_blank rel=noopener href=https://weibo.com/xiaoping378 aria-label=微博>
<i class="fab fa-weibo"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=知乎 aria-label=知乎>
<a class=text-white target=_blank rel=noopener href=https://www.zhihu.com/people/xiaoping378 aria-label=知乎>
<i class="fab fa-zhihu"></i>
</a>
</li>
</ul>
</div>
<div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
<ul class="list-inline mb-0">
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub>
<a class=text-white target=_blank rel=noopener href=https://github.com/xiaoping378/xiaoping378.github.io aria-label=GitHub>
<i class="fab fa-github"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack>
<a class=text-white target=_blank rel=noopener href=https://example.org/slack aria-label=Slack>
<i class="fab fa-slack"></i>
</a>
</li>
<li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Developer mailing list" aria-label="Developer mailing list">
<a class=text-white target=_blank rel=noopener href=https://example.org/mail aria-label="Developer mailing list">
<i class="fa fa-envelope"></i>
</a>
</li>
</ul>
</div>
<div class="col-12 col-sm-4 text-center py-2 order-sm-2">
<small class=text-white>&copy; 2022 xiaoping378 保留所有权利</small>
<small class=ml-1><a href=# target=_blank rel=noopener>隐私政策</a></small>
</div>
</div>
</div>
</footer>
</div>
<script src=/js/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/deflate.js></script>
<script src=/js/main.min.e016890ed6b0c42f5af3410eb57ac626a192a868609aee68cefe1e0f84a50b13.js integrity="sha256-4BaJDtawxC9a80EOtXrGJqGSqGhgmu5ozv4eD4SlCxM=" crossorigin=anonymous></script>
</body>
</html>