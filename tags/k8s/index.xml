<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>现代技能栈 – K8S</title><link>https://xiaoping378.github.io/tags/k8s/</link><description>Recent content in K8S on 现代技能栈</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://xiaoping378.github.io/tags/k8s/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: 快速安装</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/</guid><description>
&lt;p>不知道为什么openshift在国内热度这么低，那些要做自己容器云的公司，不知道有openshift项目的存在么？完全满足我的需求。&lt;/p>
&lt;p>docker负责应用的隔离打包，k8s提供集群管理和容器的编排服务，而openshfit则负责整个应用的生命周期：&lt;/p>
&lt;ul>
&lt;li>源码管理，CI&amp;amp;CD能力&lt;/li>
&lt;li>多租户管理, 支持LDAP和Oauth&lt;/li>
&lt;li>集成监控日志于web console&lt;/li>
&lt;/ul>
&lt;p>先说下自接触到openshift项目就遇到的一个困惑，就是openshift origin/enterprise /online/dedicated/ocp之间的关系： &lt;code>orgin相当于Fedora， 其他的相当于RHEL&lt;/code>&lt;/p>
&lt;p>接下来谈下我用自己的笔记本实践的过程与感受：&lt;/p>
&lt;ol>
&lt;li>快速安装&lt;/li>
&lt;/ol>
&lt;p>本人日常基于ubuntu16.04办公，所以用oc直接上, oc相当于kubectl&lt;/p>
&lt;p>&lt;a href="https://github.com/openshift/origin/releases">这里&lt;/a>直接下载oc客户端，或者自行编译, 编译结果在_output目录下&lt;/p>
&lt;pre tabindex="0">&lt;code>git clone --depth=1 https://github.com/openshift/origin.git
cd origin &amp;amp;&amp;amp; make
mv _output/local/bin/linux/amd64/oc /usr/local/bin
&lt;/code>&lt;/pre>&lt;p>启动openshift, 默认开启监控并初始安装自最新版本，当前是v1.5.0-alpha.2&lt;/p>
&lt;pre tabindex="0">&lt;code>oc cluster up --metrics=true --version=latest --insecure-skip-tls-verify=true --public-hostname=air13
&lt;/code>&lt;/pre>&lt;p>过程中会拉取所需镜像, 我这里显示比较多，之前已经做了些实验&lt;/p>
&lt;pre tabindex="0">&lt;code>➜ ~ docker images | grep openshift | awk '{print $1}'
openshift/node
openshift/origin-sti-builder
openshift/origin-docker-builder
openshift/origin-deployer
openshift/origin-gitserver
openshift/origin-docker-registry
openshift/origin-haproxy-router
openshift/origin
openshift/hello-openshift
openshift/openvswitch
openshift/origin-pod
openshift/origin-metrics-cassandra
openshift/origin-metrics-hawkular-metrics
openshift/origin-metrics-heapster
openshift/origin-metrics-deployer
openshift/mysql-55-centos7
openshift/origin-logging-curator
openshift/origin-logging-fluentd
openshift/origin-logging-deployment
openshift/origin-logging-elasticsearch
openshift/origin-logging-kibana
openshift/origin-logging-auth-proxy
&lt;/code>&lt;/pre>&lt;p>启动后，会打印如下信息&lt;/p>
&lt;pre tabindex="0">&lt;code>OpenShift server started.
The server is accessible via web console at:
https://air13:8443
The metrics service is available at:
https://metrics-openshift-infra.192.168.31.49.xip.io
You are logged in as:
User: developer
Password: developer
To login as administrator:
oc login -u system:admin
&lt;/code>&lt;/pre>&lt;p>打开浏览器，访问https://air13:8443，默认用developer登录，其实现在任意用户任意密码都可以的。&lt;/p>
&lt;p>web console里是空空如野的，可以临时授权developer用户操作所有项目&lt;/p>
&lt;pre tabindex="0">&lt;code>oc adm policy add-cluster-role-to-user cluster-admin developer
&lt;/code>&lt;/pre>&lt;p>2.技巧总结&lt;/p>
&lt;ul>
&lt;li>命令行自动补全, 其实kubectl也可以如此&lt;/li>
&lt;/ul>
&lt;p>&lt;code>source &amp;lt;(oc completion bash)&lt;/code>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>默认监控占用的资源太大了，可以如下降低资源占用，当然也可以web操作限制资源利用率&lt;/p>
&lt;pre tabindex="0">&lt;code>oc env rc hawkular-cassandra-1 MAX_HEAP_SIZE=1024M -n openshift-infra
#重建下变量才会生效
oc scale rc hawkular-cassandra-1 --replicas 0 -n openshift-infra
oc scale rc hawkular-cassandra-1 --replicas 1 -n openshift-infra
&lt;/code>&lt;/pre>&lt;p>因为是rc，所以直接杀掉没关系，要不env不生效&lt;/p>
&lt;/li>
&lt;li>
&lt;p>自己编译离线文档&lt;/p>
&lt;pre tabindex="0">&lt;code># 下载源文件
git clone --depth=1 https://github.com/openshift/openshift-docs.git
# 编译
cd openshift-docs &amp;amp;&amp;amp; asciibinder build
# 结果会存放在 _preview下，
cd _preview &amp;amp;&amp;amp; python -m SimpleHTTPServer
#打开浏览器访问127.0.0.1:8000
&lt;/code>&lt;/pre>&lt;p>推荐此人&lt;a href="http://guifreelife.com/">blog&lt;/a>，有几篇干货&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>3.后面会重点说下权限/资源管理和整个app开发的流程&lt;/p></description></item><item><title>Docs: 源码部署K8S</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2k8s/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>此文应该不能运行成功了，，，陈年老文，有待验证。&lt;/p>
&lt;/div>
&lt;h4 id="一-先介绍最省事的部署方法-直接从官网下release版本安装">一. 先介绍最省事的部署方法，直接从官网下release版本安装:&lt;/h4>
&lt;p>git clone 代码步骤省略 ...&lt;/p>
&lt;ol>
&lt;li>下载各依赖的release版本&lt;/li>
&lt;/ol>
&lt;p>通过修改配置文件 &lt;strong>cluster/centos/config-build.sh&lt;/strong>， 可自定义（k8s, docker, flannel, etcd）各自的下载地址和版本， 不同的版本的依赖可能会需要小改下脚本（版本变更有些打包路径发生了变化，兼容性问题）&lt;/p>
&lt;pre tabindex="0">&lt;code>cd cluster/centos &amp;amp;&amp;amp; ./build.sh all
&lt;/code>&lt;/pre>&lt;ol start="2">
&lt;li>安装并启动k8s集群环境&lt;/li>
&lt;/ol>
&lt;p>通过修改配置文件 &lt;strong>cluster/centos/config-default.sh&lt;/strong>，定义你环境里的设备的IP和其他参数，推荐运行脚本前先通过ssh-copy-id做好免密钥认证；&lt;/p>
&lt;pre tabindex="0">&lt;code>export KUBERNETES_PROVIDER=centos &amp;amp;&amp;amp; cluster/kube-up.sh
&lt;/code>&lt;/pre>&lt;h4 id="二-源码级编译安装">二. 源码级编译安装&lt;/h4>
&lt;p>本步骤基于上一大步来说,
先来看下载各依赖的release后，cluster/centos下目录发生了什么变化&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/k8s-binaries-tree.png" alt="">&lt;/p>
&lt;p>多了一个binaries的目录，里面是各master和minion上各依赖的二进制文件， 所以我们只要源码编译的结果，替换到这里来， 然后继续上一大步的第2小步即可。&lt;/p>
&lt;p>这里说下，本地编译k8s的话，需要设置安装godep，然后命令本地化。&lt;/p>
&lt;pre tabindex="0">&lt;code>export PATH=$PATH:$GOPATH/bin
&lt;/code>&lt;/pre>&lt;p>最后只需要去源码根目录下执行， 编译结果在_output目录下&lt;/p>
&lt;pre tabindex="0">&lt;code>make
&lt;/code>&lt;/pre>&lt;p>替换到相应的binaries目录下，重新运行kube-up.sh即可。&lt;/p></description></item><item><title>Docs: 权限资源管理</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%9D%83%E9%99%90%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%9D%83%E9%99%90%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</guid><description>
&lt;p>重点介绍 project，limitRange，resourceQuta和 user, group, rule，role，policy，policybinding的关系,
我刚接触时，这几个概念老搞不太清楚，这里梳理下&lt;/p>
&lt;h2 id="资源管理说明">资源管理说明&lt;/h2>
&lt;p>可以对计算资源的大小和对象类型的数量来进行配额限制。&lt;/p>
&lt;p>&lt;code>ResourceQuota&lt;/code>是面向project（namespace的基础上加了些注解）层面的，只有集群管理员可以基于namespace设置。&lt;/p>
&lt;p>&lt;code>limtRange&lt;/code>是面向pod和container级别的，openshift额外还可以限制 image， imageStream和pvc，
也是只有集群管理员才可以基于project设置，而开发人员只能基于pod（container）设置cpu和内存的requests/limits。&lt;/p>
&lt;h3 id="resourcequota">ResourceQuota&lt;/h3>
&lt;p>看看具体可以管理哪些资源，期待网络相关的也加进来.简单来讲，可以基于project来限制可消耗的内存大小和可创建的pods数量&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#080;font-style:italic">// The following identify resource constants for Kubernetes object types
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">const&lt;/span> (
&lt;span style="color:#080;font-style:italic">// Pods, number
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourcePods ResourceName = &lt;span style="color:#b44">&amp;#34;pods&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// Services, number
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceServices ResourceName = &lt;span style="color:#b44">&amp;#34;services&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// ReplicationControllers, number
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceReplicationControllers ResourceName = &lt;span style="color:#b44">&amp;#34;replicationcontrollers&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// ResourceQuotas, number
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceQuotas ResourceName = &lt;span style="color:#b44">&amp;#34;resourcequotas&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// ResourceSecrets, number
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceSecrets ResourceName = &lt;span style="color:#b44">&amp;#34;secrets&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// ResourceConfigMaps, number
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceConfigMaps ResourceName = &lt;span style="color:#b44">&amp;#34;configmaps&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// ResourcePersistentVolumeClaims, number
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourcePersistentVolumeClaims ResourceName = &lt;span style="color:#b44">&amp;#34;persistentvolumeclaims&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// ResourceServicesNodePorts, number
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceServicesNodePorts ResourceName = &lt;span style="color:#b44">&amp;#34;services.nodeports&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// ResourceServicesLoadBalancers, number
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceServicesLoadBalancers ResourceName = &lt;span style="color:#b44">&amp;#34;services.loadbalancers&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// CPU request, in cores. (500m = .5 cores)
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceRequestsCPU ResourceName = &lt;span style="color:#b44">&amp;#34;requests.cpu&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// Memory request, in bytes. (500Gi = 500GiB = 500 * 1024 * 1024 * 1024)
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceRequestsMemory ResourceName = &lt;span style="color:#b44">&amp;#34;requests.memory&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// Storage request, in bytes
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceRequestsStorage ResourceName = &lt;span style="color:#b44">&amp;#34;requests.storage&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// CPU limit, in cores. (500m = .5 cores)
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceLimitsCPU ResourceName = &lt;span style="color:#b44">&amp;#34;limits.cpu&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// Memory limit, in bytes. (500Gi = 500GiB = 500 * 1024 * 1024 * 1024)
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceLimitsMemory ResourceName = &lt;span style="color:#b44">&amp;#34;limits.memory&amp;#34;&lt;/span>
)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>openshift额外支持的images相关的限制策略&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#080;font-style:italic">// ResourceImageStreams represents a number of image streams in a project.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>ResourceImageStreams kapi.ResourceName = &lt;span style="color:#b44">&amp;#34;openshift.io/imagestreams&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// ResourceImageStreamImages represents a number of unique references to images in all image stream
&lt;/span>&lt;span style="color:#080;font-style:italic">// statuses of a project.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>ResourceImageStreamImages kapi.ResourceName = &lt;span style="color:#b44">&amp;#34;openshift.io/images&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// ResourceImageStreamTags represents a number of unique references to images in all image stream specs
&lt;/span>&lt;span style="color:#080;font-style:italic">// of a project.
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>ResourceImageStreamTags kapi.ResourceName = &lt;span style="color:#b44">&amp;#34;openshift.io/image-tags&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>此外，除了可以设置额度Quantity外，还可以指定配额的作用范围Scopes，其实就是作用于哪类pod上的:&lt;/p>
&lt;ul>
&lt;li>是否是长期运行的pod&lt;/li>
&lt;li>是否有资源上限的pod&lt;/li>
&lt;/ul>
&lt;p>目前只有pods数和计算资源（cpu,内存）才能指定作用域&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#080;font-style:italic">// A ResourceQuotaScope defines a filter that must match each object tracked by a quota
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> ResourceQuotaScope &lt;span style="color:#0b0;font-weight:bold">string&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">const&lt;/span> (
&lt;span style="color:#080;font-style:italic">// Match all pod objects where spec.activeDeadlineSeconds，这个是标明pod的运行时长参数
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceQuotaScopeTerminating ResourceQuotaScope = &lt;span style="color:#b44">&amp;#34;Terminating&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// Match all pod objects where !spec.activeDeadlineSeconds ， 长期运行的pod
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceQuotaScopeNotTerminating ResourceQuotaScope = &lt;span style="color:#b44">&amp;#34;NotTerminating&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// Match all pod objects that have best effort quality of service， 只能用来描述资源无上限的pod数
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceQuotaScopeBestEffort ResourceQuotaScope = &lt;span style="color:#b44">&amp;#34;BestEffort&amp;#34;&lt;/span>
&lt;span style="color:#080;font-style:italic">// Match all pod objects that do not have best effort quality of service， 资源有上限的pod
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> ResourceQuotaScopeNotBestEffort ResourceQuotaScope = &lt;span style="color:#b44">&amp;#34;NotBestEffort&amp;#34;&lt;/span>
)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>下面举个例子&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ResourceQuota&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>compute-resources-long-running&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hard&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">pods&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;4&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits.cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;4&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits.memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">scopes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- NotTerminating&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面的意思即是， 限制长期运行的pod最多只能创建4个，且共用4c和2G内存&lt;/p>
&lt;p>如果不指定scopes的话，是描述的所有scopes的限制；&lt;/p>
&lt;blockquote>
&lt;p>本文参考&lt;a href="https://docs.openshift.org/latest/admin_guide/quota.html">这里&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>可以看到，通过资源配额管理，可以帮助我们解决以下问题：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>控制计算资源使用量&lt;/p>
&lt;p>我们在实际生产环境中经常遇到的情况是，用户申请了过多的资源，用户应用的资源使用率太低，造成了资源的浪费。管理员通常会给集群设置超卖系数，来提高整个集群的资源使用率；另外管理员也会给用户设置资源配额上限，来限制用户使用资源的数量。通过上面的介绍我们可以看到，kubernetes的资源配额，我们可以从应用的层次上来进行配额管理，可以设置不同应用的资源配额上限。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>控制besteffort类型POD资源使用量&lt;/p>
&lt;p>如果POD中的所有容器都没有设置request和limit，那么这些POD的QoS类型是besteffort，这种类型的POD更方便kubernetes进行调度，但是存在的问题是，如果不对这些POD进行资源管理，那么就会导致这个kubernetes集群资源过载，会影响这个集群中的所有应用，所以通过将资源配额管理的作用范围设置成besteffort，kubernetes可以通过限制这些POD的资源，避免整个集群资源过载。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>控制长期运行的应用和短暂运行的应用资源使用率&lt;/p>
&lt;p>在实际使用中，在kubernetes集群中会同时存在两种类型的应用，一种是长期运行的应用，比如网站这种web应用，还有一种就是短暂运行的应用，比如编译网站的这种应用。通过资源配额管理，可以同时对这两种不同类型的应用设置资源使用上限，来控制不同应用的资源使用。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="limitrange">LimitRange&lt;/h3>
&lt;p>&lt;code>limtRange&lt;/code>是面向pod和container级别的，为什么只能集群管理员才可设置呢，因为这个的提出是为了防止有些应用忘记加资源边界的限定，而占用过多的资源，那么有了limitRange就给它来个默认限制。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;LimitRange&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;core-resource-limits&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Pod&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">max&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;1Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">min&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;200m&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;6Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Container&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">max&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;1Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">min&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;100m&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;4Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">default&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;300m&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;200Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">defaultRequest&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;200m&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;100Mi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxLimitRequestRatio&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;10&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;openshift.io/Image&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">max&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;1Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;openshift.io/ImageStream&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">max&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">openshift.io/image-tags&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;10&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">openshift.io/images&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;12&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如上使用oc create后，会看到我们对某namespace下的pod和container做了默认的资源设置，&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/limitRange.png" alt="limitRange">&lt;/p>
&lt;h2 id="权限管理说明">权限管理说明&lt;/h2>
&lt;p>这里不涉及到认证登录的介绍，openshfit支持很多认证方式，比如AllowAll，CA认证, HTPasswd, KeyStone, LDAP, Oauth等，这里为了简化，用默认的AllowAll来做权限控制的说明&lt;/p>
&lt;p>权限管理，即访问API资源之前，必须要经过的访问策略校验，主要分为5种： AlwaysDeny、AlwaysAllow（默认）、ABAC、RBAC、Webhook&lt;/p>
&lt;p>主要说明user, group, rule，role，policy，policybinding之间的关系，以及提出这些概念，各自是为了解决什么问题&lt;/p>
&lt;ul>
&lt;li>user和group&lt;/li>
&lt;/ul>
&lt;p>说到user其实就是一个用户账号(userAccount)，用它来和k8s集群做交互（登录，kubectl等）， 但还有一个容易混淆的概念就是sercieAccount，有了userAccount为什么还又来个serviceAccount的设计， 这两者有什么区别 ？ 以下是kubernetes官方对两者的解释&lt;/p>
&lt;blockquote>
&lt;p>user account是为人类设计的，而service account则是为跑在pod里的进程用的，运行在pod里的进程需要调用Kubernetes API以及非Kubernetes API的其它服务（如image repository/被mount到pod上的NFS volumes中的file等）;&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>user account是global的，即跨namespace使用；而service account是namespaced内的，即仅在所属的namespace下使用;&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>user account可能会涉及到很多权限设定和商业逻辑在里面，而后者是更轻量级的，是集群用户针对某namespace内的服务使用的，一般遵循最小特权原则，如监控服务需要访问APIsever等;&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>useraccount需要借助第三方实现，后者系统都会默认在namesspace里创建default，亦可自定义&lt;/p>
&lt;/blockquote>
&lt;p>两者大部分流程是一致的，都是要先认证通过再校验权限，然后才是action， 实际上一般是由userAccount来控制serviceAccount来完成特定的任务，
比如一个用户A自建了服务1和服务2， 但只想把服务2开发给用户B，这样的serviceAccount就可以排上用场了,
又或者我有几个服务，有了serviceAccount就可以来限制用户的访问权限（list, watch, update, delete）了.&lt;/p>
&lt;p>说到group就是方便对user的权限批量操作而设计；&lt;/p>
&lt;p>用户可以被分配到一个或多个组，每个组代表一组特定的用户。组在同时向多个用户管理权限时非常有用。&lt;/p>
&lt;ul>
&lt;li>rule和role&lt;/li>
&lt;/ul>
&lt;p>rule是规则， 是对一组对象上被允许的动作（get, list, create, update, delete, deletecollection 和 watch）描述，可操作对象主要是 container，images，pod，servcie， project， user， build， imagestream， dc， route， templeate。&lt;/p>
&lt;p>role 就是规则的集合，俗称角色， 不同对象上的不同动作，可以任意组成各种角色，系统默认的有 &lt;code>admin basic-user cluster-admin cluster-admin edit self-provisioner view&lt;/code>；&lt;/p>
&lt;p>policy，是策略, 保存特定namespace的所有角色roles的对象。 每个命名空间最多只有一个Policy策略。&lt;/p>
&lt;p>rolebinding， 就是把user或者group与角色role进行关联，注意. user和group可以被关联到多个roles&lt;/p>
&lt;p>pollicybing, 就是就是多个rolebindings的描述；&lt;/p>
&lt;p>这样看，policy的概念提出有点儿扯淡了，感觉没什么用，其实不然，policy的提出主要是为了区分cluster-policy和local-policy的。&lt;/p>
&lt;p>cluster policy是适用于所有namespace的角色和绑定；
local policy则是试用于具体的某个namespace的；&lt;/p>
&lt;p>以上可以通过&lt;code>oc describe clusterPolicy default&lt;/code>来看查看所有详细的信息；&lt;/p>
&lt;p>小节：&lt;/p>
&lt;p>可以通过&lt;code>oc policy can-i --list&lt;/code>查看自己可以干些什么&lt;/p>
&lt;p>还可以通过&lt;code>oc policy who-can &amp;lt;动作&amp;gt; &amp;lt;资源对象&amp;gt;&lt;/code>， 比如说查看谁能get pod之类的，就是&lt;code>oc policy who-can get pod&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">➜ openshift-docs git:&lt;span style="color:#666">(&lt;/span>master&lt;span style="color:#666">)&lt;/span> ✗ oc policy who-can get pod
Namespace: myproject
Verb: get
Resource: pods
Users: developer
system:admin
system:serviceaccount:default:pvinstaller
system:serviceaccount:myproject:deployer
system:serviceaccount:openshift-infra:build-controller
system:serviceaccount:openshift-infra:deployment-controller
system:serviceaccount:openshift-infra:deploymentconfig-controller
system:serviceaccount:openshift-infra:endpoint-controller
system:serviceaccount:openshift-infra:namespace-controller
system:serviceaccount:openshift-infra:pet-set-controller
system:serviceaccount:openshift-infra:pv-binder-controller
system:serviceaccount:openshift-infra:pv-recycler-controller
Groups: system:cluster-admins
system:cluster-readers
system:masters
system:nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果openshift自带的角色不能满足的话，还可以自定义角色role&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">$ oc get clusterrole view -o yaml &amp;gt; clusterrole_view.yaml
$ cp clusterrole_view.yaml localrole_exampleview.yaml
$ vim localrole_exampleview.yaml
&lt;span style="color:#080;font-style:italic"># 1. Update kind: ClusterRole to kind: Role&lt;/span>
&lt;span style="color:#080;font-style:italic"># 2. Update name: view to name: exampleview&lt;/span>
&lt;span style="color:#080;font-style:italic"># 3. Remove resourceVersion, selfLink, uid, and creationTimestamp&lt;/span>
$ oc create -f path/to/localrole_exampleview.yaml -n &amp;lt;project_you_want_to_add_the_local_role_exampleview_to&amp;gt;
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="下文介绍实战-结合实际场景-如何设置权限-即整个开发管理流程实践说明">下文介绍实战，结合实际场景，如何设置权限，即整个开发管理流程实践说明&lt;/h2></description></item><item><title>Docs: 离线安装kubernetes</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/kuberbetes-1.5-%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/kuberbetes-1.5-%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>虽然距离当前主流版本已经差之千里，但其中的思想仍记得借鉴。&lt;/p>
&lt;/div>
&lt;p>经常遇到全新初始安装k8s集群的问题，所以想着搞成离线模式，本着最小依赖原则，提高安装速度&lt;/p>
&lt;p>基于Centos7-1511-minimal, 非此版本脚本应该会运行出错，自行修改吧&lt;/p>
&lt;p>本离线安装所有的依赖都打包放到了&lt;a href="https://pan.baidu.com/s/1i5jusip">百度网盘&lt;/a>&lt;/p>
&lt;p>为了便于维护，已建立独立项目&lt;a href="https://github.com/xiaoping378/k8s-deploy">k8s-deploy&lt;/a>&lt;/p>
&lt;h2 id="第一步">第一步&lt;/h2>
&lt;p>基本思路是，在k8s-deploy目录下，临时启个http server， node节点上会从此拉取所依赖镜像和rpms&lt;/p>
&lt;pre tabindex="0">&lt;code># python -m SimpleHTTPServer
Serving HTTP on 0.0.0.0 port 8000 ...
&lt;/code>&lt;/pre>&lt;p>windows上可以用hfs临时启个http server， 自行百度如何使用&lt;/p>
&lt;h2 id="master侧">master侧&lt;/h2>
&lt;p>运行以下命令，初始化master&lt;/p>
&lt;p>192.168.56.1:8000 是我的http-server, 注意要将k8s-deploy.sh 里的HTTP-SERVER变量也改下&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -L http://192.168.56.1:8000/k8s-deploy.sh | bash -s master
&lt;/code>&lt;/pre>&lt;h2 id="minion侧">minion侧&lt;/h2>
&lt;p>视自己的情况而定&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -L http://192.168.56.1:8000/k8s-deploy.sh | bash -s join --token=6669b1.81f129bc847154f9 192.168.56.100
&lt;/code>&lt;/pre>&lt;h2 id="总结">总结&lt;/h2>
&lt;p>整个脚本实现比较简单， 坑都在脚本里解决了。脚本文件在&lt;a href="https://gist.github.com/xiaoping378/3a129aa6c81eaecae199a50236ad8bf7">这里&lt;/a>&lt;/p>
&lt;p>就一个master-up和node-up， 基本一个函数只做一件事，很清晰，可以自己查看具体过程。&lt;/p>
&lt;p>1.5 与 1.3给我感觉最大的变化是网络部分， 1.5启用了cni网络插件
不需要像以前一样非要把flannel和docker绑在一起了（先启flannel才能启docker）。&lt;/p>
&lt;p>具体可以看这里
&lt;a href="https://github.com/containernetworking/cni/blob/master/Documentation/flannel.md">https://github.com/containernetworking/cni/blob/master/Documentation/flannel.md&lt;/a>&lt;/p>
&lt;p>master侧如果是单核的话，会因资源不足， dns安装失败。&lt;/p></description></item><item><title>Docs: k8s的各组件和特性扫盲</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/k8s%E5%90%84%E7%BB%84%E4%BB%B6%E7%9A%84%E8%AE%A4%E7%9F%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/k8s%E5%90%84%E7%BB%84%E4%BB%B6%E7%9A%84%E8%AE%A4%E7%9F%A5/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>了解一个工具的特性可以从它的参数入手&lt;/p>
&lt;/div>
&lt;h2 id="api-server">api-server&lt;/h2>
&lt;p>在k8s内发挥的网关和api&lt;/p>
&lt;p>CSR特性&lt;/p>
&lt;h2 id="网络">网络&lt;/h2>
&lt;h3 id="flannel">flannel&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>flannel的设计就是为集群中所有节点能重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，
并让属于不同节点上的容器能够直接通过内网IP通信。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>实际上就是给每个节点的docker重新设置容器上可分配的IP段， &lt;code>--bip&lt;/code>的妙用。
这恰好迎合了k8s的设计，即一个pod（container）在集群中拥有唯一、可路由到的IP，带来的好处就是减少跨主机容器间通信要port mapping的复杂性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>原理&lt;/p>
&lt;ul>
&lt;li>flannle需要运行一个叫flanned的agent，其用etcd来存储网络配置、已经分配的子网、和辅助信息（主机IP),如下&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#666">[&lt;/span>root@master1 ~&lt;span style="color:#666">]&lt;/span>&lt;span style="color:#080;font-style:italic"># etcdctl ls /coreos.com/network&lt;/span>
/coreos.com/network/config
/coreos.com/network/subnets
&lt;span style="color:#666">[&lt;/span>root@master1 ~&lt;span style="color:#666">]&lt;/span>&lt;span style="color:#080;font-style:italic">#&lt;/span>
&lt;span style="color:#666">[&lt;/span>root@master1 ~&lt;span style="color:#666">]&lt;/span>&lt;span style="color:#080;font-style:italic"># etcdctl get /coreos.com/network/config&lt;/span>
&lt;span style="color:#666">{&lt;/span>&lt;span style="color:#b44">&amp;#34;Network&amp;#34;&lt;/span>:&lt;span style="color:#b44">&amp;#34;172.16.0.0/16&amp;#34;&lt;/span>&lt;span style="color:#666">}&lt;/span>
&lt;span style="color:#666">[&lt;/span>root@master1 ~&lt;span style="color:#666">]&lt;/span>&lt;span style="color:#080;font-style:italic">#&lt;/span>
&lt;span style="color:#666">[&lt;/span>root@master1 ~&lt;span style="color:#666">]&lt;/span>&lt;span style="color:#080;font-style:italic"># etcdctl ls /coreos.com/network/subnets&lt;/span>
/coreos.com/network/subnets/172.16.29.0-24
/coreos.com/network/subnets/172.16.40.0-24
/coreos.com/network/subnets/172.16.60.0-24
&lt;span style="color:#666">[&lt;/span>root@master1 ~&lt;span style="color:#666">]&lt;/span>&lt;span style="color:#080;font-style:italic">#&lt;/span>
&lt;span style="color:#666">[&lt;/span>root@master1 ~&lt;span style="color:#666">]&lt;/span>&lt;span style="color:#080;font-style:italic"># etcdctl get /coreos.com/network/subnets/172.16.29.0-24&lt;/span>
&lt;span style="color:#666">{&lt;/span>&lt;span style="color:#b44">&amp;#34;PublicIP&amp;#34;&lt;/span>:&lt;span style="color:#b44">&amp;#34;192.168.1.129&amp;#34;&lt;/span>&lt;span style="color:#666">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;p>flannel0 还负责解封装报文,或者创建路由。
flannel有多种方式可以完成报文的转发。&lt;/p>
&lt;ul>
&lt;li>UDP&lt;/li>
&lt;li>vxlan&lt;/li>
&lt;li>host-gw&lt;/li>
&lt;li>aws-vpc&lt;/li>
&lt;li>gce&lt;/li>
&lt;li>alloc&lt;/li>
&lt;/ul>
&lt;p>下图是经典的UDP封装方式数据流图
&lt;img src="https://xiaoping378.github.io/flannel-packet-01.png" alt="UDP">&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: 项目开发实战</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/</guid><description>
&lt;p>下面的所有操作，都可以通过cli，web console，RestFul API实现，默认使用cli说明&lt;/p>
&lt;h3 id="创建项目">创建项目&lt;/h3>
&lt;p>这里是接着oc cluster up后，来说的， 默认&lt;code>oc whoami&lt;/code>是 developer,拥有admin的Role角色，俗称项目经理（管理员）&lt;/p>
&lt;ol>
&lt;li>删除默认创建的项目，并创建一个实际中的项目&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">oc delete project myproject
oc new-project eshop --display-name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;电商项目&amp;#34;&lt;/span> --description&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;一个神奇的网站&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>现在项目管理员可以创建任意多个项目，从前面的源码可以看到目前是没法针对项目管理员去限制可创建项目上限的。&lt;/p>
&lt;ol start="2">
&lt;li>查看项目状态&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code>#oc status
In project 电商项目 (eshop) on server https://192.168.31.49:8443
You have no services, deployment configs, or build configs.
Run 'oc new-app' to create an application.
&lt;/code>&lt;/pre>&lt;p>空空如也，有提示语句提示可通过&lt;code>oc new-app&lt;/code>去创建具体应用的&lt;/p>
&lt;h3 id="创建应用">创建应用&lt;/h3>
&lt;p>前面也说过，openshift的核心就是围绕应用的整个生命周期来的，所以从new-app说起&lt;/p>
&lt;p>new-app的入口是&lt;code>NewCmdNewApplication()&lt;/code>, 大部分实现是 &lt;code>func (c *AppConfig) Run() (*AppResult, error)&lt;/code> 感兴趣的可以根据源码来理解openshift的devops理念。&lt;/p>
&lt;ol>
&lt;li>创建应用的方式
现在可以通过3种方式（源码， docker镜像， 模板）来创建一个应用。&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code># oc new-app -h
#此处省略。。。
Usage:
oc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [options]
#此处省略。。。
&lt;/code>&lt;/pre>&lt;p>有很多灵活简便的方式来创建应用，甚至可以直接&lt;code>oc new-app mysql&lt;/code>来创建一个mysql服务&lt;/p>
&lt;p>比如下面的例子，是基于nodejs-ex项目的master分支，创建应用&lt;/p>
&lt;pre tabindex="0">&lt;code>oc new-app https://github.com/xiaoping378/nodejs-ex.git#master
&lt;/code>&lt;/pre>&lt;p>接着上面的nodejs-ex项目来说， 实际上，&lt;code>oc new-app&lt;/code>就做了两件事，先build， 再deploy。&lt;/p>
&lt;p>new-app一般会先创建一个bc, bc会产出一个iamge，new-app典型的还会创建一个dc，去部署新生成的image，也会创建相应的service来负载均衡方访问刚部署上的镜像里的业务。&lt;/p>
&lt;p>这一切都是自动完成的，因为openshift origin里面有一些检测机制和默认规则，下面就针对上面那条命令看看内部都发生了什么&lt;/p>
&lt;ul>
&lt;li>
&lt;p>首先openshift会执行 &lt;code>git ls-remote&lt;/code>， 来查看此项目的所有remote分支，&lt;/p>
&lt;p>如果存在master分支，下一步则直接clone和checkout了&lt;/p>
&lt;p>checkout后，接着就是根据解析规则来定义如何build了。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>build策略&lt;/p>
&lt;p>首先会探测nodejs-ex项目根目录下，是否有dockerfile或者jenkinsfile，如果两者都没有则会根据“典型文件”判断这个项目的开发语言， 举例&lt;/p>
&lt;p>如果存在app.json或者package.json文件，则认为是nodejs类型的项目， 更多的典型文件如下：&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/new-app-detector.png" alt="detector">&lt;/p>
&lt;p>这部分的代码实现主要在 detector.go&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>未完，待续。。。&lt;/p></description></item><item><title>Docs: DevOps实战-0</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-0/</guid><description>
&lt;p>主要涉及到&lt;code>一键发布&lt;/code>，&lt;code>快速回滚&lt;/code>，&lt;code>弹性伸缩&lt;/code>，&lt;code>蓝绿部署&lt;/code>方面。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>启动openshift&lt;/p>
&lt;pre>&lt;code>oc cluster up --version=v1.5.0-rc.0 --metrics --use-existing-config=true
&lt;/code>&lt;/pre>
&lt;p>默认负责监控的pods占用资源太大了，可以这样限制下，或者cluster up时不加 &lt;code>--metrics&lt;/code>&lt;/p>
&lt;pre>&lt;code>oc login -u system:admin
oc env rc hawkular-cassandra-1 MAX_HEAP_SIZE=1024M -n openshift-infra
#重建下,变量才会生效
oc scale rc hawkular-cassandra-1 --replicas 0 -n openshift-infra
oc scale rc hawkular-cassandra-1 --replicas 1 -n openshift-infra
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>建立本地Git仓&lt;/p>
&lt;p>默认官方给出的例子基本都需要和Github结合，实在不好本地实战演示，所以本地要来一个&lt;code>gogs&lt;/code>代码仓。&lt;/p>
&lt;pre>&lt;code>oc login -u devloper
oc new-project ci
#先拉取所依赖镜像
docker pull openshiftdemos/gogs:0.9.97
docker pull centos/postgresql-94-centos7
#创建gogs服务，并禁用webhook时的TLS校验，不然无法触发build
oc new-app -f https://raw.githubusercontent.com/xiaoping378/gogs-openshift-docker/master/openshift/gogs-persistent-template.yaml -p SKIP_TLS_VERIFY=true -p HOSTNAME=gogs-ci.192.168.31.49.xip.io
&lt;/code>&lt;/pre>
&lt;p>上面的HOSTNAME，注意要换成自己宿主机的IPv4地址，默认创建的其他服务的路由都是这个形式的，&lt;/p>
&lt;p>有个有意思的地方，为什么默认路由会是这种 &lt;code>name+IP+xip.io&lt;/code> 形式呢，奥秘在 &lt;a href="http://xip.io">http://xip.io&lt;/a> 的公共服务上。
这其实是个特殊的域DNS server，比如我们查询域名&lt;code>gogs-ci.192.168.31.49.xip.io&lt;/code>时 ，会返回192.168.31.49的地址回来，
而这个地址恰好是我们Router的地址，这样子Router会根据route的配置负责负载到对应的POD上。自己试验下就知道怎么回事了。&lt;/p>
&lt;pre>&lt;code>dig http://gogs-ci.192.168.31.49.xip.io +short
&lt;/code>&lt;/pre>
&lt;p>只做功能性演示，先不考虑https加密安全访问，创建完后，访问gogs服务 &lt;code>http://gogs-ci.192.168.31.49.xip.io&lt;/code>&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-gogs.png" alt="gogs">&lt;/p>
&lt;p>这个项目，第一个注册用户即为管理员，比如我现在去页面注册一个叫&lt;code>developer&lt;/code>的用户。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>找个项目来实战吧&lt;/p>
&lt;ul>
&lt;li>
&lt;p>克隆远程项目，并设置&lt;/p>
&lt;pre>&lt;code>git clone https://github.com/xiaoping378/nodejs-ex.git &amp;amp;&amp;amp; cd nodejs-ex
git remote add gogs http://gogs-ci.192.168.31.49.xip.io/developer/nodejs-ex.git
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>通过web页面，在gogs上创建一个&lt;code>nodejs-ex&lt;/code>仓库, 并如下push刚才克隆的项目&lt;/p>
&lt;pre>&lt;code>$ git push gogs master
Username for 'http://gogs-ci.192.168.31.49.xip.io': developer
Password for 'http://developer@gogs-ci.192.168.31.49.xip.io':
Counting objects: 431, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (210/210), done.
Writing objects: 100% (431/431), 145.16 KiB | 0 bytes/s, done.
Total 431 (delta 159), reused 431 (delta 159)
To http://gogs-ci.192.168.31.49.xip.io/developer/nodejs-ex.git
* [new branch] master -&amp;gt; master
&lt;/code>&lt;/pre>
&lt;p>gogs的页面上会如实反馈信息&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/gogs-create-push.png" alt="gogs">&lt;/p>
&lt;p>OK，现在本地项目就有了，接下来进入正题&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>在openshift部署此nodejs应用&lt;/p>
&lt;pre>&lt;code> #创建web namespace
oc new-project web
#先拉取依赖镜像
docker pull centos/mongodb-32-centos7
docker pull centos/nodejs-4-centos7
#部署此项目，并启用国内npm源和对应的git仓
oc new-app nodejs-mongo-persistent --name=nodejs-ex -p NPM_MIRROR=https://registry.npm.taobao.org -p SOURCE_REPOSITORY_URL=http://gogs-ci.192.168.31.49.xip.io/developer/nodejs-ex.git
&lt;/code>&lt;/pre>
&lt;p>默认此模板会从指定的URL地址拉取代码，并根据预先的配置，采取&lt;code>Source&lt;/code>编译策略，基于istag nodejs:4镜像编译出nodejs-mongo-persistent:latest镜像，编译出来的镜像又会自动触发部署。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最基本的DevOps能力&lt;/p>
&lt;p>即push代码通过webhook触发自动编译，继而滚动部署&lt;/p>
&lt;p>要实现这个目标前，需要先把webhook填写到gogs里。&lt;/p>
&lt;p>在openshift界面上复制webhook地址&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-github-webhook.png" alt="find_webhook">&lt;/p>
&lt;p>然后在gogs上填加一个webhook&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-gogs-webhook.png" alt="add_webhook">&lt;/p>
&lt;p>这里我们随意修改些，然后推送代码，就会自动触发编译并滚动升级&lt;/p>
&lt;pre>&lt;code>➜ nodejs-ex git:(master) vim views/index.html
➜ nodejs-ex git:(master) ✗ git add .
➜ nodejs-ex git:(master) ✗ git commit -m &amp;quot;这又是个测试&amp;quot;
[master 082f05e] 这又是个测试
1 file changed, 1 insertion(+), 1 deletion(-)
➜ nodejs-ex git:(master) git push gogs master
Username for 'http://gogs-ci.192.168.31.49.xip.io': developer
Password for 'http://developer@gogs-ci.192.168.31.49.xip.io':
Counting objects: 4, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (3/3), done.
Writing objects: 100% (4/4), 365 bytes | 0 bytes/s, done.
Total 4 (delta 2), reused 0 (delta 0)
To http://gogs-ci.192.168.31.49.xip.io/developer/nodejs-ex.git
c3592e6..082f05e master -&amp;gt; master
&lt;/code>&lt;/pre>
&lt;p>编译成功后，会产生新镜像，继而触发滚动升级的截图&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/push-build-deploy.png" alt="auto-deploy">&lt;/p>
&lt;p>现实中，如果项目没有很好的自动化测试的话，我们肯定不会这样操作的，除非想被开掉了。&lt;/p>
&lt;p>其实可以简单的去掉webhook，采用手动触发build： 界面操作的话，去build界面点击&lt;code>Start Build&lt;/code>，命令行的话如下&lt;/p>
&lt;pre>&lt;code>oc start-build nodejs-mongo-persistent
&lt;/code>&lt;/pre>
&lt;p>另外，如果发现新版本的应用有重大缺陷，想回滚以前的部署版本，也有对应的界面和命令&lt;/p>
&lt;pre>&lt;code>oc rollback nodejs-mongo-persistent --to-version=3
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-roll-back.png" alt="rollBack">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>弹性伸缩&lt;/p>
&lt;p>目前可以根据CPU使用率来进行弹性伸缩&lt;/p>
&lt;p>有人问能不能基本mem进行弹性呢，其实这个是没什么意义的，一般应用都会自行缓存，内存基本只增不长， 所以cpu才能很好的实时反应业务的负载。&lt;/p>
&lt;p>弹性伸缩前，要确保应用先行设置了cpu request，这点还没明白原因，为什么要这样，按理说，heapster一直会采集pod的资源使用情况的，HPA周期拿数据和设置的阈值对比就完了。&lt;/p>
&lt;p>这里是部署界面的菜单栏，可以手动加上cpu request&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-dc-menu.png" alt="dc-menu">&lt;/p>
&lt;p>添加 cpu request.&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-cpu-request.png" alt="cpu-request">&lt;/p>
&lt;p>然后开启弹性伸缩特性，这里就不截图了，展示下命令行，我们设置成： 当cpu使用率达到80%时，就弹，最大可以弹出3个实例&lt;/p>
&lt;pre>&lt;code>oc autoscale dc/nodejs-mongo-persistent --max=3 --cpu-percent=80
&lt;/code>&lt;/pre>
&lt;p>OK，之后我们通过ab工具简单做个压力模拟，因为环境在我的笔记本上，所以只模拟发送100万个连接，并发100的量&lt;/p>
&lt;pre>&lt;code>ab -n 1000000 -c 100 http://nodejs-mongo-persistent-web.192.168.31.49.xip.io/
&lt;/code>&lt;/pre>
&lt;p>后台每1分钟采集一次cpu使用率，过不了一会儿，就会看到nodejs实例自动扩展了&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-autoscale-0.png" alt="autoscale-0">&lt;/p>
&lt;p>当业务量降下来时，会自动减少实例，是根据平均CPU使用率来操作的。&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-autoscale-1.png" alt="autoscale-1">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>蓝绿部署&lt;/p>
&lt;p>这个也是API级别的支持，不描述具体操作细节了，原理还是以前的，从负载均衡层面入手。 实现新旧版本同时存在。
并不是所有业务都适合蓝绿部署的，要看后台数据是否允许，新旧版本同时发生读写数据&lt;/p>
&lt;p>在openshift里实现蓝绿部署的，就太简单了。具体就是在Route层面添加同一应用的多个版本的service，并设置分流权重
截图如下&lt;/p>
&lt;p>界面设置，只是为了展示功能，我随便添加了个service&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-blue-green-0.png" alt="bluegreen-0">&lt;/p>
&lt;p>实际展示效果&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-blue-green-1.png" alt="bluegreen-1">&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="总结">总结&lt;/h3>
&lt;p>Openshift平台本身在API层面实现了DevOps，所以基于它很容易做到DevOps as an service， 上面的演示可能与现实世界不太一样，&lt;/p>
&lt;p>比如真实情况是有，测试，预发布，线上环境的，下次再分享: openshift基于jenkins pipeline如果实现更真实场景的需求。&lt;/p></description></item><item><title>Docs: Helm模板介绍</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/kubernetes-helm%E6%A8%A1%E6%9D%BF%E6%80%BB%E7%BB%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/kubernetes-helm%E6%A8%A1%E6%9D%BF%E6%80%BB%E7%BB%93/</guid><description>
&lt;h3 id="概要">概要&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Helm是一个管理kubernetes集群内应用的工具，提供了一系列管理应用的快捷方式，例如 inspect， install， upgrade， delete等，经验可以沿用以前apt，yum，homebrew的,区别就是helm管理的是kubernetes集群内的应用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>还有一个概念必须得提，就是&lt;code>chart&lt;/code>， 它代表的就是被helm管理的应用包，里面具体就是放一些预先配置的Kubernetes资源(pod, rc, deployment, service, ingress)，一个包描述文件(&lt;code>Chart.yaml&lt;/code>), 还可以通过指定依赖来组织成更复杂的应用，支持go template语法，可参数化模板，让使用者定制化安装
charts可以存放在本地，也可以放在远端，这点理解成yum仓很合适。。。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这里有个&lt;a href="https://kubeapps.com">应用市场&lt;/a> ，里面罗列了各种应用charts。由开源项目&lt;a href="https://github.com/helm/monocular">monocular&lt;/a>支撑&lt;/p>
&lt;p>下面主要介绍helm的基本使用流程和具体场景的实践。&lt;/p>
&lt;h3 id="初始化k8s集群v1-6-2">初始化k8s集群v1.6.2&lt;/h3>
&lt;p>先来准备k8s环境，可以通过&lt;a href="https://github.com/xiaoping378/k8s-deploy">k8s-deploy&lt;/a>项目来离线安装高可用kubernetes集群，我这里是单机演示环境。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubeadm init --kubernetes-version v1.6.2 --pod-network-cidr 12.240.0.0/12
&lt;span style="color:#080;font-style:italic">#方便命令自动补全&lt;/span>
&lt;span style="color:#a2f">source&lt;/span> &amp;lt;&lt;span style="color:#666">(&lt;/span>kubectl completion zsh&lt;span style="color:#666">)&lt;/span>
&lt;span style="color:#080;font-style:italic">#安装cni网络&lt;/span>
cp /etc/kubernetes/admin.conf &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube/config
kubectl apply -f kube-flannel-rbac.yml
kubectl apply -f kube-flannel.yml
&lt;span style="color:#080;font-style:italic">#使能master可以被调度&lt;/span>
kubectl taint node --all node-role.kubernetes.io/master-
&lt;span style="color:#080;font-style:italic">#安装ingress-controller, 边界路由作用&lt;/span>
kubectl create -f ingress-traefik-rbac.yml
kubectl create -f ingress-traefik-deploy.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>这样一个比较完整的k8s环境就具备了，另外监控和日志不在此文的讨论范围内。&lt;/p>
&lt;h3 id="初始化helm环境">初始化Helm环境&lt;/h3>
&lt;p>由于刚才创建的k8s集群默认启用RBAC机制，个人认为这个特性是k8s真正走向成熟的一大标志，废话不表，为了helm可以安装任何应用，我们先给他最高权限。&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl create serviceaccount helm --namespace kube-system
kubectl create clusterrolebinding cluster-admin-helm --clusterrole=cluster-admin --serviceaccount=kube-system:helm
&lt;/code>&lt;/pre>&lt;p>初始化helm，如下执行，会在kube-system namepsace里安装一个tiller服务端，这个服务端就是用来解析helm语义的，后台再转成api-server的API执行：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ helm init --service-account helm
&lt;span style="color:#b8860b">$HELM_HOME&lt;/span> has been configured at /home/xxp/.helm.
Tiller &lt;span style="color:#666">(&lt;/span>the helm server side component&lt;span style="color:#666">)&lt;/span> has been installed into your Kubernetes Cluster.
Happy Helming!
➜ helm version
Client: &amp;amp;version.Version&lt;span style="color:#666">{&lt;/span>SemVer:&lt;span style="color:#b44">&amp;#34;v2.4.1&amp;#34;&lt;/span>, GitCommit:&lt;span style="color:#b44">&amp;#34;46d9ea82e2c925186e1fc620a8320ce1314cbb02&amp;#34;&lt;/span>, GitTreeState:&lt;span style="color:#b44">&amp;#34;clean&amp;#34;&lt;/span>&lt;span style="color:#666">}&lt;/span>
Server: &amp;amp;version.Version&lt;span style="color:#666">{&lt;/span>SemVer:&lt;span style="color:#b44">&amp;#34;v2.4.1&amp;#34;&lt;/span>, GitCommit:&lt;span style="color:#b44">&amp;#34;46d9ea82e2c925186e1fc620a8320ce1314cbb02&amp;#34;&lt;/span>, GitTreeState:&lt;span style="color:#b44">&amp;#34;clean&amp;#34;&lt;/span>&lt;span style="color:#666">}&lt;/span>
&lt;span style="color:#080;font-style:italic">#命令行补全&lt;/span>
➜ &lt;span style="color:#a2f">source&lt;/span> &amp;lt;&lt;span style="color:#666">(&lt;/span>helm completion zsh&lt;span style="color:#666">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="安装第一个应用">安装第一个应用&lt;/h3>
&lt;p>初始化Helm后，默认就导入了2个repos，后面安装和搜索应用时，都是从这2个仓里出的，当然也可以自己通过&lt;code>helm repo add&lt;/code>添加本地私有仓&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ helm repo list
NAME URL
stable https://kubernetes-charts.storage.googleapis.com
&lt;span style="color:#a2f">local&lt;/span> http://127.0.0.1:8879/charts
&lt;/code>&lt;/pre>&lt;/div>&lt;p>其实上面的repo仓的索引信息是存放在&lt;code>~/.helm/repository&lt;/code>的, 类似/etc/yum.repos.d/的作用&lt;/p>
&lt;p>helm的使用基本流程如下:&lt;/p>
&lt;ul>
&lt;li>helm search: 搜索自己想要安装的应用（chart）&lt;/li>
&lt;li>helm fetch: 下载应用（chart）到本地，可以忽略此步&lt;/li>
&lt;li>helm install: 安装应用&lt;/li>
&lt;li>helm ls: 查看已安装的应用情况&lt;/li>
&lt;/ul>
&lt;p>这里举例安装redis&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ helm install stable/redis --set persistence.enabled&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">false&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>如上，如果网络给力的话，很快就会装上最新的redis版本，Helm安装应用，目前有四种方式：&lt;/p>
&lt;ul>
&lt;li>&lt;code>helm install stable/mariadb&lt;/code> 通过chart仓来安装&lt;/li>
&lt;li>&lt;code>helm install ./nginx-1.2.3.tgz&lt;/code> 通过本地打包后的压缩chart包来安装&lt;/li>
&lt;li>&lt;code>helm install ./nginx&lt;/code> 通过本地的chart目录来安装&lt;/li>
&lt;li>&lt;code>helm install https://example.com/charts/nginx-1.2.3.tgz&lt;/code> 通过绝对网络地址来安装chart压缩包&lt;/li>
&lt;/ul>
&lt;h3 id="实战演示">实战演示&lt;/h3>
&lt;p>主要从&lt;code>制作自己的chart&lt;/code>， &lt;code>构建自己的repo&lt;/code>， &lt;code>组装复杂应用的实战&lt;/code>三方面来演示&lt;/p>
&lt;h4 id="制作自己的chart">制作自己的chart&lt;/h4>
&lt;p>helm有一个很好的引导教程模板, 如下会自动创建一个通用的应用模板&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ helm create myapp
Creating myapp
➜ tree myapp
myapp
├── charts //此应用包的依赖包定义（如果有的话，也会是类似此包的目录结构）
├── Chart.yaml // 包的描述文件
├── templates // 包的主体目录
│ ├── deployment.yaml // kubernetes里的deployment yaml文件
│ ├── _helpers.tpl // 模板里如果复杂的话，可能需要函数或者其他数据结构，这里就是定义的地方
│ ├── ingress.yaml // kubernetes里的ingress yaml文件
│ ├── NOTES.txt // 想提供给使用者的一些注意事项，一般提供install后，如何访问之类的信息
│ └── service.yaml // kubernetes里的service yaml文件
└── values.yaml // 参数的默认值
&lt;span style="color:#666">2&lt;/span> directories, &lt;span style="color:#666">7&lt;/span> files
&lt;/code>&lt;/pre>&lt;/div>&lt;p>如上操作，我们就有了一个&lt;code>myapp&lt;/code>的应用，目录结构如上，来看看看values.yaml的内容, 这个里面就是模板里可定制参数的默认值&lt;/p>
&lt;p>很容易看到，kubernetes里的rc实例数，镜像名，servie配置，路由ingress配置都可以轻松定制。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-YAML" data-lang="YAML">&lt;span style="color:#080;font-style:italic"># Default values for myapp.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># This is a YAML-formatted file.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># Declare variables to be passed into your templates.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">replicaCount&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">repository&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">tag&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stable&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">pullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>IfNotPresent&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">service&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterIP&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">externalPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">internalPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">ingress&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">enabled&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Used to create Ingress record (should used with service.type: ClusterIP).&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hosts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- chart-example.local&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># kubernetes.io/ingress.class: nginx&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># kubernetes.io/tls-acme: &amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">tls&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Secrets must be manually created in the namespace.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># - secretName: chart-example-tls&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># hosts:&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># - chart-example.local&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>100m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>128Mi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>100m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">memory&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>128Mi&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>note.&lt;/p>
&lt;blockquote>
&lt;p>一般拿到一个现有的app chart后，这个文件是必看的，通过&lt;code>helm fetch myapp&lt;/code>会得到一个类似上面目录的压缩包&lt;/p>
&lt;/blockquote>
&lt;p>我们可以通过 &lt;code>--set&lt;/code>或传入values.yaml文件来定制化安装，&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># 安装myapp模板： 启动2个实例，并可通过ingress对外提供myapp.192.168.31.49.xip.io的域名访问&lt;/span>
➜ helm install --name myapp --set &lt;span style="color:#b8860b">replicaCount&lt;/span>&lt;span style="color:#666">=&lt;/span>2,ingress.enabled&lt;span style="color:#666">=&lt;/span>true,ingress.hosts&lt;span style="color:#666">={&lt;/span>myapp.192.168.31.49.xip.io&lt;span style="color:#666">}&lt;/span> ./myapp
➜ helm ls
NAME REVISION UPDATED STATUS CHART NAMESPACE
exasperated-rottweiler &lt;span style="color:#666">1&lt;/span> Wed May &lt;span style="color:#666">10&lt;/span> 13:58:56 2017 DEPLOYED redis-0.5.2 default
myapp &lt;span style="color:#666">1&lt;/span> Wed May &lt;span style="color:#666">10&lt;/span> 21:46:51 2017 DEPLOYED myapp-0.1.0 default
&lt;span style="color:#080;font-style:italic">#通过传入yml文件来安装&lt;/span>
&lt;span style="color:#080;font-style:italic">#helm install --name myapp -f myvalues.yaml ./myapp&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="构建私有charts-repo">构建私有charts repo&lt;/h4>
&lt;p>通过 &lt;code>helm repo list&lt;/code>, 得知默认的local repo地址是&lt;code>http://127.0.0.1:8879/charts&lt;/code>， 可以简单的通过&lt;code>helm serve&lt;/code>来操作，再或者自己起个web server也是一样的。&lt;/p>
&lt;p>这里举例，把刚才创建的myapp放到本地仓里&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ helm search myapp
No results found
➜
➜ &lt;span style="color:#a2f">source&lt;/span> &amp;lt;&lt;span style="color:#666">(&lt;/span>helm completion zsh&lt;span style="color:#666">)&lt;/span>
➜
➜ helm package myapp
➜
➜ helm serve &amp;amp;
&lt;span style="color:#666">[&lt;/span>1&lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">10619&lt;/span>
➜ Regenerating index. This may take a moment.
Now serving you on 127.0.0.1:8879
➜ deis helm search myapp
NAME VERSION DESCRIPTION
local/myapp 0.1.0 A Helm chart &lt;span style="color:#a2f;font-weight:bold">for&lt;/span> Kubernetes
&lt;/code>&lt;/pre>&lt;/div>&lt;p>目前个人感觉体验不太好的是，私有仓里的app必须以tar包的形式存在。&lt;/p>
&lt;h4 id="构建复杂应用">构建复杂应用&lt;/h4>
&lt;p>透过例子学习，会加速理解，我们从deis里的workflow应用来介绍&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ ~ helm repo add deis https://charts.deis.com/workflow
&lt;span style="color:#b44">&amp;#34;deis&amp;#34;&lt;/span> has been added to your repositories
➜ ~
➜ ~ helm search workflow
NAME VERSION DESCRIPTION
deis/workflow v2.14.0 Deis Workflow
➜ ~
➜ ~ helm fetch deis/workflow --untar
➜ ~ helm dep list workflow
NAME VERSION REPOSITORY STATUS
builder v2.10.1 https://charts.deis.com/builder unpacked
slugbuilder v2.4.12 https://charts.deis.com/slugbuilder unpacked
dockerbuilder v2.7.2 https://charts.deis.com/dockerbuilder unpacked
controller v2.14.0 https://charts.deis.com/controller unpacked
slugrunner v2.3.0 https://charts.deis.com/slugrunner unpacked
database v2.5.3 https://charts.deis.com/database unpacked
fluentd v2.9.0 https://charts.deis.com/fluentd unpacked
redis v2.2.6 https://charts.deis.com/redis unpacked
logger v2.4.3 https://charts.deis.com/logger unpacked
minio v2.3.5 https://charts.deis.com/minio unpacked
monitor v2.9.0 https://charts.deis.com/monitor unpacked
nsqd v2.2.7 https://charts.deis.com/nsqd unpacked
registry v2.4.0 https://charts.deis.com/registry unpacked
registry-proxy v1.3.0 https://charts.deis.com/registry-proxy unpacked
registry-token-refresher v1.1.2 https://charts.deis.com/registry-token-refresher unpacked
router v2.12.1 https://charts.deis.com/router unpacked
workflow-manager v2.5.0 https://charts.deis.com/workflow-manager unpacked
➜ ~ ls workflow
charts Chart.yaml requirements.lock requirements.yaml templates values.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>如上操作，我们会得到一个巨型应用，实际上便是deis出品的workflow开源paas平台，具体这个平台的介绍下次有机会再分享&lt;/p>
&lt;p>整个大型应用是通过 wofkflow/requirements.yaml组织起来的，所有依赖的chart放到charts目录，然后charts目录里就是些类似myapp的小应用&lt;/p>
&lt;p>更复杂的应用，甚至有人把openstack用helm安装到Kubernetes上，感兴趣的可以参考&lt;a href="https://github.com/openstack/openstack-helm">这里&lt;/a>&lt;/p></description></item><item><title>Docs: DevOps实战-1</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-1/</guid><description>
&lt;p>本文主要介绍基于openshift如何完成&lt;code>开发-&amp;gt;测试-&amp;gt;线上&lt;/code>场景的变更，这是一个典型的应用生产流程，来看看openshift是如何利用容器优雅的完成整个过程的吧&lt;/p>
&lt;p>下文基于上篇&lt;a href="../openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-0">DevOps实战-0&lt;/a> 的&lt;code>nodejs-ex&lt;/code>项目来说, 假设到这里，你本地已经有了nodejs-ex项目&lt;/p>
&lt;h3 id="准备3个project">准备3个project&lt;/h3>
&lt;p>用这3个project来模拟开发，测试，线上环境&lt;/p>
&lt;p>现实中一般各个场景的服务器都是物理隔离的，这里可以利用&lt;code>--node-selector&lt;/code>，来指定项目可以跑在哪些节点上。&lt;/p>
&lt;pre tabindex="0">&lt;code>oc login -u sysetm:admin
#晚上在笔记本上写此blog，没合适的环境，单机模拟多台 -- start
oc label node 192.168.31.49 web-prod=true web-dev=true web-test=true
#晚上在笔记本上写此blog，没合适的环境，单机模拟多台 -- end
#1.创建web-dev项目
#2.授权developer为开发组项目管理员
#3.授权测试和运维人员可以从开发组拉取镜像
oc adm new-project web-dev --node-selector='web-dev=true'
oc policy add-role-to-user admin developer
oc policy add-role-to-group system:image-puller system:serviceaccounts:web-test -n web-dev
oc policy add-role-to-group system:image-puller system:serviceaccounts:web-prod -n web-dev
oc adm new-project web-test --node-selector='web-test=true'
oc policy add-role-to-user admin tester
oc adm new-project web-prod --node-selector='web-prod=true'
oc policy add-role-to-user admin ops
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>你可能会注意到，这里用的&lt;code>new-project&lt;/code> 前面还加了adm， 其实&lt;code>oc adm&lt;/code>等效于&lt;code>oadm&lt;/code>， 一般管理集群相关的用这个命令，这里是因为需要读取节点的标签（label）信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>指定项目要运行那些节点，则是利用了注解-annotations， 即在原有的project结构上设置了注解，这样openshift在相应的项目里创建任何pod时，都对会自动注入node-selector&lt;/p>
&lt;/li>
&lt;li>
&lt;p>另外需要注意的，默认项目的管理员（developer）是没有权限读取node标签信息的，以前写过&lt;a href="../openshift%E5%AE%9E%E8%B7%B5-%E6%9D%83%E9%99%90%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86">权限管理相关blog&lt;/a>，集群管理员可以授权node访问权限，即使如此developer还是不能改写项目级别的标签的，举个例子: developer在开发环境的pod上指定了&lt;code>--node-selector='web-dev=false'&lt;/code>， 最终这个pod的node-selector会是&lt;code>'web-dev=true, web-dev=flase'&lt;/code>, 导致最终不会被调度到任何节点上。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>上面分别授权了3个用户，这里是不关心这些用户是否真实存在的，只是一个RABC的描述，因为是&lt;code>oc cluster up&lt;/code>起来的环境，默认使用&lt;code>anypassword&lt;/code>的身份认证，所以登录时，任意用户名和密码都是可以登录OK的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过&lt;code>oc describe policybinding -n web-dev&lt;/code> 可以查看授权情况， 如果觉得默认的role不满足需求的话，也可以自定义role，另外通过&lt;code>oc policy remove-role-from-group/user &amp;lt;Role&amp;gt; &amp;lt;name&amp;gt;&lt;/code>可以移除相关授权，&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="初始化web-dev-web-test-web-prod环境">初始化web-dev, web-test, web-prod环境&lt;/h3>
&lt;p>按照上篇&lt;code>DevOps实战-0&lt;/code>里的方式&lt;/p>
&lt;p>初始化我们的开发环境, 进入源码&lt;code>nodejs-ex&lt;/code>目录&lt;/p>
&lt;pre tabindex="0">&lt;code>oc new-app -f openshift/template/nodejs-mongo-persistent.json --name=nodejs-ex \
-p NPM_MIRROR=https://registry.npm.taobao.org \
-p SOURCE_REPOSITORY_URL=http://gogs-ci.192.168.31.49.xip.io/developer/nodejs-ex.git \
-n web-dev
&lt;/code>&lt;/pre>&lt;p>初始化测试环境，相较于上一步的模板json，只是注掉bc和更改了triggers的is,后面会详细介绍之间的差异&lt;/p>
&lt;pre tabindex="0">&lt;code>oc new-app -f openshift/template/nodejs-mongo-persistent-test.json --name=nodejs-ex-test -n web-test
&lt;/code>&lt;/pre>&lt;p>以tester登录web console，会发现只有mongodb部署上了，而前端nodejs还在等待依赖的镜像 &lt;code>web-dev/nodejs-mongo-persistent:test&lt;/code>。&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-web-test-wait.png" alt="webtest-wait">&lt;/p>
&lt;p>初始化生产环境, 这个生产的template.json有点儿简单，负载均衡和弹性伸缩都没有启用。&lt;/p>
&lt;pre tabindex="0">&lt;code>oc new-app -f openshift/template/nodejs-mongo-persistent-prod.json --name=nodejs-ex-prod -n web-prod
&lt;/code>&lt;/pre>&lt;h3 id="实战模拟-开发-测试-发布">实战模拟，开发-&amp;gt;测试-&amp;gt;发布&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>developer开发完特性或者修复完bug，push代码到镜像仓。&lt;/p>
&lt;p>这里分享一个很方便的技巧，就是 &lt;code>oc rsync&lt;/code>, 这个可以实时的同步本地目录到容器了，避免了频繁编译镜像和临时挂载目录到镜像里的hack了。&lt;/p>
&lt;pre tabindex="0">&lt;code>vim ...
git add .
git commit -m &amp;quot;fix bugs&amp;quot;
git push gogs master
&lt;/code>&lt;/pre>&lt;p>如上，由于上篇中设置了webhook, developer提交代码会触发了自动编译并部署，确认部署后的环境是否修复了bug，如果单元测试通过，那就要通知测试团队（如今大部分公司，应该没有测试人员了吧，也可以直接变更到线上）&lt;/p>
&lt;p>测试那边的环境里一直在等待这个镜像&lt;code>web-dev/nodejs-mongo-persistent:test&lt;/code>， 而默认developer配置成默认编译出来的是&lt;code>web-dev/nodejs-mongo-persistent:latest&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>oc login -u developer
oc tag web-dev/nodejs-mongo-persistent:latest web-dev/nodejs-mongo-persistent:v1.1
oc tag web-dev/nodejs-mongo-persistent:v1.1 web-dev/nodejs-mongo-persistent:test
&lt;/code>&lt;/pre>&lt;p>如上操作后，开发人员更新版本号，然后在web-dev环境里会打上一个test的镜像tag出来，操作完如下所示&lt;/p>
&lt;pre tabindex="0">&lt;code>➜ nodejs-ex git:(master) oc get is
NAME DOCKER REPO TAGS UPDATED
nodejs-mongo-persistent 172.30.1.1:5000/web-dev/nodejs-mongo-persistent test,v1.1,latest 49 seconds ago
➜ nodejs-ex git:(master)
➜ nodejs-ex git:(master) oc get istag
NAME DOCKER REF UPDATED IMAGENAME
nodejs-mongo-persistent:latest 172.30.1.1:5000/web-dev/nodejs-mongo-persistent@sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7 3 minutes ago sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7
nodejs-mongo-persistent:v1.1 172.30.1.1:5000/web-dev/nodejs-mongo-persistent@sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7 2 minutes ago sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7
nodejs-mongo-persistent:test 172.30.1.1:5000/web-dev/nodejs-mongo-persistent@sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7 55 seconds ago sha256:55615da49dd299064e7bba75923ac7996bf0d109e0322d4f84f9b41665b2e4c7
&lt;/code>&lt;/pre>&lt;p>这样一来，测试环境里就会自动部署上刚才开发人员的环境了，再也不会有因为环境差异问题和测试吵吵了。&lt;/p>
&lt;p>这一切都得益于openshift里新添加的imageStreams，它打通了编译和部署的环节，能自动通知对方，继而自动触发下一步动作。&lt;/p>
&lt;p>测试通过后，通知Ops再重新tag成线上所需要的镜像tag，这样线上就会根据配置自动滚动升级了。&lt;/p>
&lt;pre tabindex="0">&lt;code>#假设一个叫ops的人负责上线,那首先ops得有具备web-dev项目里编辑is的能力
oc login -u developer
#不该给ops这么高权限的，应该自定义一个只能tag is的role，这里为了简单演示
oc policy add-role-to-user edit ops -n web-dev
&lt;/code>&lt;/pre>&lt;p>如上操纵，ops就具备了tag web-dev项目的镜像的能力，也可以通过UI来查看和授权&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-add-role.png" alt="">&lt;/p>
&lt;pre tabindex="0">&lt;code>oc login -u ops
oc tag web-dev/nodejs-mongo-persistent:v1.1 web-dev/nodejs-mongo-persistent:prod
&lt;/code>&lt;/pre>&lt;p>然后打上线上依赖的镜像tag即可，发布上线，这样就完成了开发-&amp;gt;测试-&amp;gt;发布一条线，很快捷的&lt;code>人工干预&lt;/code>上线了&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>openshift 利用镜像tag的能力，来实现不同场景的同步，单纯基于docker也可以实现以上目标的，只是不够平台化，还是以前的脚本打天下，远不如openshift在API层面解决来的强大和灵活。&lt;/p></description></item><item><title>Docs: k8s的监控方案</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/kuerbernetes%E7%9A%84%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/kuerbernetes%E7%9A%84%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88/</guid><description>
&lt;h2 id="方案选型">方案选型&lt;/h2>
&lt;p>如果已存在完善的监控系统的话，推荐使用k8s原生的&lt;strong>heapster&lt;/strong>，比较轻量，容易集成。&lt;/p>
&lt;p>我选择的是&lt;strong>prometheus&lt;/strong>, 它是比较完善的云平台级监控方案，继k8s之后同样已被列入&lt;a href="https://cncf.io/projects">云计算基金会&lt;/a>项目, 除了具备heapster的能力之外，还支持监控广泛的应用(mysql, JMX, HAProxy等)和灵活的告警的能力，并具备多IDC federation的能力，兼容多种开源监控系统（StatsD, Ganglia, collectd, nagios等）。&lt;/p>
&lt;p>本文主要参考&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/heapster/issues/645">prometheus和heapster开发者之间的对话&lt;/a>&lt;/li>
&lt;li>CoreOS的blog&lt;a href="https://coreos.com/blog/monitoring-kubernetes-with-prometheus.html">Monitoring Kubernetes with Prometheus&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>下面分别介绍下两种方案&lt;/p>
&lt;h3 id="heapster">heapster&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>heapster的介绍:&lt;/p>
&lt;p>通过向kubelet拉取stats的方式， 可提供15分钟内的缓存供k8s的dashboard用，也支持第三方存储，如influxdb等，还具备REST API(经我实验，这个API还不完善 &lt;a href="https://github.com/kubernetes/heapster/issues/1155">缺少diskIO API&lt;/a>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>heapster的监控范围&lt;/p>
&lt;p>可监控的内容包括集群内的Container, Pod, Node 和 Namespace的性能或配置信息，
目前container级别还不支持网络和硬盘信息，具体性能项如下&lt;/p>
&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Metric Name&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>cpu/limit&lt;/td>
&lt;td>CPU hard limit in millicores.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu/node_capacity&lt;/td>
&lt;td>Cpu capacity of a node.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu/node_allocatable&lt;/td>
&lt;td>Cpu allocatable of a node.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu/node_reservation&lt;/td>
&lt;td>Share of cpu that is reserved on the node allocatable.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu/node_utilization&lt;/td>
&lt;td>CPU utilization as a share of node allocatable.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu/request&lt;/td>
&lt;td>CPU request (the guaranteed amount of resources) in millicores.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu/usage&lt;/td>
&lt;td>Cumulative CPU usage on all cores.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>cpu/usage_rate&lt;/td>
&lt;td>CPU usage on all cores in millicores.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>filesystem/usage&lt;/td>
&lt;td>Total number of bytes consumed on a filesystem.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>filesystem/limit&lt;/td>
&lt;td>The total size of filesystem in bytes.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>filesystem/available&lt;/td>
&lt;td>The number of available bytes remaining in a the filesystem&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/limit&lt;/td>
&lt;td>Memory hard limit in bytes.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/major_page_faults&lt;/td>
&lt;td>Number of major page faults.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/major_page_faults_rate&lt;/td>
&lt;td>Number of major page faults per second.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/node_capacity&lt;/td>
&lt;td>Memory capacity of a node.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/node_allocatable&lt;/td>
&lt;td>Memory allocatable of a node.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/node_reservation&lt;/td>
&lt;td>Share of memory that is reserved on the node allocatable.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/node_utilization&lt;/td>
&lt;td>Memory utilization as a share of memory allocatable.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/page_faults&lt;/td>
&lt;td>Number of page faults.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/page_faults_rate&lt;/td>
&lt;td>Number of page faults per second.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/request&lt;/td>
&lt;td>Memory request (the guaranteed amount of resources) in bytes.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/usage&lt;/td>
&lt;td>Total memory usage.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>memory/working_set&lt;/td>
&lt;td>Total working set usage. Working set is the memory being used and not easily dropped by the kernel.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>network/rx&lt;/td>
&lt;td>Cumulative number of bytes received over the network.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>network/rx_errors&lt;/td>
&lt;td>Cumulative number of errors while receiving over the network.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>network/rx_errors_rate&lt;/td>
&lt;td>Number of errors while receiving over the network per second.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>network/rx_rate&lt;/td>
&lt;td>Number of bytes received over the network per second.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>network/tx&lt;/td>
&lt;td>Cumulative number of bytes sent over the network&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>network/tx_errors&lt;/td>
&lt;td>Cumulative number of errors while sending over the network&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>network/tx_errors_rate&lt;/td>
&lt;td>Number of errors while sending over the network&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>network/tx_rate&lt;/td>
&lt;td>Number of bytes sent over the network per second.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>uptime&lt;/td>
&lt;td>Number of milliseconds since the container was started.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="prometheus">Prometheus&lt;/h3>
&lt;p>Prometheus集成了数据采集，存储，异常告警多项功能，是一款一体化的完整方案。 它针对大规模的集群环境设计了拉取式的数据采集方式、多维度数据存储格式以及服务发现等创新功能。&lt;/p>
&lt;h4 id="功能特点">功能特点：&lt;/h4>
&lt;pre>&lt;code>* 多维数据模型（有metric名称和键值对确定的时间序列）
* 灵活的查询语言
* 不依赖分布式存储
* 通过pull方式采集时间序列，通过http协议传输
* 支持通过中介网关的push时间序列的方式
* 监控数据通过服务或者静态配置来发现
* 支持多维度可视化分析和dashboard等
&lt;/code>&lt;/pre>
&lt;h4 id="组件介绍">组件介绍：&lt;/h4>
&lt;p>这个生态里包含的组件，大多是可选的：
* 核心prometheus server提供收集和存储时间序列数据
* 大量的&lt;a href="https://prometheus.io/docs/instrumenting/clientlibs/">client libraries&lt;/a>来支持应用业务代码的探针
* 适用于短时任务的push gateway
* 基于Rails/SQL语句的可视化分析
* 特殊用途的exporter（包括HAProxy、StatsD、Ganglia等）
* 用于报警的alertmanager
* 支持命令行查询的工具
* 其他工具
大多数的组件都是用Go语言来完成的，使得它们方便构建和部署。&lt;/p>
&lt;h4 id="架构图">架构图：&lt;/h4>
&lt;p>&lt;img src="https://xiaoping378.github.io/prometheus-architecture.png" alt="架构图">&lt;/p>
&lt;p>Promethues直接或通过短期Jobs的中介网关拉取收集指标。 它在本地存储所有抓取的数据样本，并对数据进行规则匹配检测，这样可以基于现有数据创建新的时间系列指标或生成警报。
PromDash或其他API使用者对收集的数据进行可视化。&lt;/p>
&lt;h4 id="引入prometheus对k8s的影响">引入Prometheus对k8s的影响&lt;/h4>
&lt;p>下图是Redhat研发人员的回答&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/prometheus-affect-k8s.png" alt="promethue-affect-k8s">&lt;/p>
&lt;p>并不会对k8s产生太大的影响，其主要是通过api-server来发现需要监控的目标，然后会周期性的通过各个Node上kubelet来拉取数据。
更详细的讨论见&lt;a href="https://github.com/prometheus/prometheus/pull/905">这里&lt;/a>&lt;/p>
&lt;h3 id="部署prometheus">部署Prometheus&lt;/h3>
&lt;p>下文是基于&lt;a href="https://github.com/xiaoping378/k8s-monitor">k8s-monitor项目&lt;/a>来说的&lt;/p>
&lt;p>&lt;a href="https://prometheus.io/">Prometheus&lt;/a> is an open-source monitoring solution that includes the gathering of metrics, their storage in an internal time series database as well as querying and alerting based on that data.&lt;/p>
&lt;p>It offers a lot of integrations incl. Docker, Kubernetes, etc.&lt;/p>
&lt;p>Prometheus can also visualize your data. However, in this recipe we include another open-source tool, &lt;a href="http://grafana.org/">Grafana&lt;/a>, for the visualization part, as it offers a more powerful and flexible way to generate visuals and dashboards.&lt;/p>
&lt;p>If you just want to get Prometheus and Grafana up and running you can deploy the whole recipe with a single command instead of going through all steps detailed out below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl create --filename manifests/
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="deploying-prometheus">Deploying Prometheus&lt;/h2>
&lt;p>First, we need to create the configuration for our Prometheus. For this we use a Config Map, which we later mount into our Prometheus pod to configure it. This way we can change the configuration without having to redeploy Prometheus itself.&lt;/p>
&lt;p>&lt;code>kubectl create --filename manifests/prometheus-core-configmap.yaml&lt;/code>&lt;/p>
&lt;p>Then, we create a service to be able to access Prometheus.&lt;/p>
&lt;p>&lt;code>kubectl create --filename manifests/prometheus-core-service.yaml&lt;/code>&lt;/p>
&lt;p>Finally, we can deploy Prometheus itself.&lt;/p>
&lt;p>&lt;code>kubectl create --filename manifests/prometheus-core-deployment.yaml&lt;/code>&lt;/p>
&lt;p>Further, we need the Prometheus Node Exporter deployed to each node. For this we use a Daemon Set and a fronting service for Prometheus to be able to access the node exporters.&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl create --filename manifests/prometheus-node-exporter-service.yaml
kubectl create --filename manifests/prometheus-node-exporter-daemonset.yaml
&lt;/code>&lt;/pre>&lt;p>Wait a bit for all the pods to come up. Then Prometheus should be ready and running. We can check the Prometheus targets at &lt;a href="https://mycluster.k8s.gigantic.io/api/v1/proxy/namespaces/default/services/prometheus/targets">https://mycluster.k8s.gigantic.io/api/v1/proxy/namespaces/default/services/prometheus/targets&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/prometheus_targets.png" alt="Prometheus Targets">&lt;/p>
&lt;h2 id="deploying-alertmanager">Deploying Alertmanager&lt;/h2>
&lt;p>we need to create the configuration for our Alertmanager. For this we use a Config Map, which we later mount into our Alertmanager pod to configure it. This way we can change the configuration without having to redeploy Alertmanager itself.&lt;/p>
&lt;p>&lt;code>kubectl create --filename manifests/prometheus-alert-configmap.yaml&lt;/code>&lt;/p>
&lt;p>Then, we create a service to be able to access Alertmanager.&lt;/p>
&lt;p>&lt;code>kubectl create --filename manifests/prometheus-alert-service.yaml&lt;/code>&lt;/p>
&lt;p>Finally, we can deploy Alertmanager itself.&lt;/p>
&lt;p>&lt;code>kubectl create --filename manifests/prometheus-alert-deployment.yaml&lt;/code>&lt;/p>
&lt;p>Wait a bit for all the pods to come up. Then Alertmanager should be ready and running. We can check the Alertmanager targets at
&lt;a href="https://mycluster.k8s.gigantic.io/api/v1/proxy/namespaces/default/services/alertmanager/">https://mycluster.k8s.gigantic.io/api/v1/proxy/namespaces/default/services/alertmanager/&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/alertmanager.png" alt="Alertmanager">&lt;/p>
&lt;h2 id="deploying-grafana">Deploying Grafana&lt;/h2>
&lt;p>Now that we have Prometheus up and running we can deploy Grafana to have a nicer frontend for our metrics.&lt;/p>
&lt;p>Again, we create a service to be able to access Grafana and a deployment to manage the pods.&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl create --filename manifests/grafana-services.yaml
kubectl create --filename manifests/grafana-deployment.yaml
&lt;/code>&lt;/pre>&lt;p>Wait a bit for Grafana to come up. Then you can access Grafana at &lt;a href="https://mycluster.k8s.gigantic.io/api/v1/proxy/namespaces/default/services/grafana/">https://mycluster.k8s.gigantic.io/api/v1/proxy/namespaces/default/services/grafana/&lt;/a>&lt;/p>
&lt;h2 id="setting-up-grafana">Setting Up Grafana&lt;/h2>
&lt;p>TLDR: If you don't want to go through all the manual steps below you can let the following job use the API to configure Grafana to a similar state.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl create --filename manifests/grafana-import-dashboards-job.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once we're in Grafana we need to first configure &lt;a href="https://grafana.net/plugins/prometheus">Prometheus&lt;/a> as a data source.&lt;/p>
&lt;ul>
&lt;li>&lt;code>Grafana UI / Data Sources / Add data source&lt;/code>
&lt;ul>
&lt;li>&lt;code>Name&lt;/code>: &lt;code>prometheus&lt;/code>&lt;/li>
&lt;li>&lt;code>Type&lt;/code>: &lt;code>Prometheus&lt;/code>&lt;/li>
&lt;li>&lt;code>Url&lt;/code>: &lt;code>http://prometheus:9090&lt;/code>&lt;/li>
&lt;li>&lt;code>Add&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://xiaoping378.github.io/grafana_datasource.png" alt="Grafana Datasource">&lt;/p>
&lt;p>Then go to the Dashboards tab and import the &lt;a href="https://grafana.net/dashboards/2">Prometheus Stats dashboard&lt;/a>, which shows the status of Prometheus itself.&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/grafana_datasource_dashboard.png" alt="Grafana Datasource Dashboard">&lt;/p>
&lt;p>You can check it out to see how your Prometheus is doing.&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/grafana_prometheus_stats.png" alt="Grafana Datasource Dashboard">&lt;/p>
&lt;p>Last, but not least we can import a sample &lt;a href="https://grafana.net/dashboards/597">Kubernetes cluster monitoring dashboard&lt;/a>, to get a first overview over our cluster metrics.&lt;/p>
&lt;ul>
&lt;li>&lt;code>Grafana UI / Dashboards / Import&lt;/code>
&lt;ul>
&lt;li>&lt;code>Grafana.net Dashboard&lt;/code>: &lt;code>https://grafana.net/dashboards/597&lt;/code>&lt;/li>
&lt;li>&lt;code>Load&lt;/code>&lt;/li>
&lt;li>&lt;code>Prometheus&lt;/code>: &lt;code>prometheus&lt;/code>&lt;/li>
&lt;li>&lt;code>Save &amp;amp; Open&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://xiaoping378.github.io/grafana_import_dashboard.png" alt="Grafana Import Dashboard">&lt;/p>
&lt;p>Voilá. You have a nice first dashboard with metrics of your Kubernetes cluster.&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/grafana_cluster_overview.png" alt="Grafana Import Dashboard">&lt;/p>
&lt;h2 id="next-steps">Next Steps&lt;/h2>
&lt;p>Next, you should get into the &lt;a href="http://docs.grafana.org/">Grafana&lt;/a> and &lt;a href="https://prometheus.io/docs/introduction/overview/">Prometheus&lt;/a> documentations to get to know the tools and either build your own dashboards or extend the samples from above.&lt;/p>
&lt;p>You can also check out grafana.net for some more example &lt;a href="https://grafana.net/dashboards">dashboards&lt;/a> and &lt;a href="https://grafana.net/plugins">plugins&lt;/a>.&lt;/p>
&lt;p>More Alertmanager documentations in &lt;a href="https://prometheus.io/docs/alerting/overview/">here&lt;/a>&lt;/p></description></item><item><title>Docs: 编译和目录结构介绍</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E6%BA%90%E7%A0%81-%E7%BC%96%E8%AF%91%E5%92%8C%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E6%BA%90%E7%A0%81-%E7%BC%96%E8%AF%91%E5%92%8C%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/</guid><description>
&lt;p>介绍openshift的源码编译和目录结构组织，为了方便代码调试和了解大型Golang项目的构建方式&lt;/p>
&lt;h3 id="编译">编译&lt;/h3>
&lt;p>无论是openshift还是Kubernetes等大型Golang项目都用到了&lt;code>Makefile&lt;/code>, 所以有必要从此开始说起，这里只说项目里用到的makefile特性，想了解更多的可以参考&lt;a href="http://scc.qibebt.cas.cn/docs/linux/base/%B8%FA%CE%D2%D2%BB%C6%F0%D0%B4Makefile-%B3%C2%F0%A9.pdf">跟我一起写Makefile&lt;/a>&lt;/p>
&lt;h4 id="makefile介绍">Makefile介绍&lt;/h4>
&lt;blockquote>
&lt;p>makefile 关系到了整个工程的编译规则。一个工程中的源文件不计数，其按类型、功能、
模块分别放在若干个目录中，makefile 定义了一系列的规则来指定，哪些文件需要先编译，
哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为
makefile 就像一个 Shell 脚本一样，其中也可以执行操作系统的命令。 makefile 带来的好
处就是——“自动化编译”，一旦写好，只需要一个 make 命令，整个工程完全自动编译，
极大的提高了软件开发的效率。&lt;/p>
&lt;/blockquote>
&lt;p>Makefile里的规则，就在做两件事，一个是指明依赖关系，另一个是生成目标的方法&lt;/p>
&lt;p>Golang项目里用到的Makefile规则比较简单，基本就是定义一个目标的生成方法，下面的示例是Openshift项目里makefile中定义的第一个目标。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-makefile" data-lang="makefile">&lt;span style="color:#00a000">all build&lt;/span>&lt;span style="color:#666">:&lt;/span>
hack/build-go.sh &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>WHAT&lt;span style="color:#a2f;font-weight:bold">)&lt;/span> &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>GOFLAGS&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;span style="color:#00a000">.PHONY&lt;/span>&lt;span style="color:#666">:&lt;/span> all build
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;p>&lt;code>all build&lt;/code>，是定义的目标，看到这个就知道可以在源码的根目录上执行&lt;code>make all build&lt;/code>来编译了&lt;/p>
&lt;/li>
&lt;li>
&lt;p>第二行说明生成目标的方法，就是去hack目录下执行build-go.sh脚本，这里还支持传入一些参数&lt;/p>
&lt;/li>
&lt;li>
&lt;p>第三行 &lt;code>.PHONY&lt;/code>，起到一个标识的作用，没什么实际意义，是用来告诉make命令，这里是个伪目标，也可以说成是默认目标，所以在openshift的根目录上直接执行&lt;code>make&lt;/code>, 等效于&lt;code>make all build&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>还可以自己决定是否编译出镜像或者rpm包（make release, make build-rpms）&lt;/p>
&lt;h4 id="编译openshift">编译openshift&lt;/h4>
&lt;p>上边介绍了，直接敲&lt;code>make&lt;/code>就可以自动编译出所有平台（linux, mac, windows）的二进制，编译前介绍两个hack方法，&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在hack/build-go.sh的第二行加上&lt;code>set -x&lt;/code>， 这样的话，shell脚本在运行时，里面的所有变量和执行路径会全部打印出来，一目了然，不用自己一行一行的加echo debug了&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如下修改hack/build-cross.sh，不然会编译出多平台的二进制，花的时间略长啊。。。&lt;/p>
&lt;pre tabindex="0">&lt;code># by default, build for these platforms
platforms=(
linux/amd64
# darwin/amd64
# windows/amd64
# linux/386
)
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>下面简易说下执行make后，都发生了什么，只会捡关键点说。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">➜ origin git:&lt;span style="color:#666">(&lt;/span>xxpDev&lt;span style="color:#666">)&lt;/span> ✗ make
hack/build-go.sh
&lt;span style="color:#080;font-style:italic"># 初始化一大堆变量，关键函数都在common.sh里实现的&lt;/span>
&lt;span style="color:#a2f">source&lt;/span> hack/common.sh hack/util.sh hack/lib目录下的所有脚本
&lt;span style="color:#080;font-style:italic"># 还会改动GOPATH,然后会在$GOPATH/src/github.com/openshift下建个软连指向origin目录&lt;/span>
&lt;span style="color:#a2f">export&lt;/span> &lt;span style="color:#b8860b">GOPATH&lt;/span>&lt;span style="color:#666">=&lt;/span>_output/local/go
&lt;span style="color:#080;font-style:italic"># 最终组合成下面一条最原始的命令，来进行编译&lt;/span>
go install &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> -pkgdir /home/xxp/Github/src/github.com/openshift/origin/_output/local/pkgdir/linux/amd64 &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> -tags &lt;span style="color:#b44">&amp;#39; &amp;#39;&lt;/span> &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> -ldflags &lt;span style="color:#b44">&amp;#39;-X github.com/openshift/origin/pkg/bootstrap/docker.defaultImageStreams=centos7 \
&lt;/span>&lt;span style="color:#b44"> -X github.com/openshift/origin/pkg/cmd/util/variable.DefaultImagePrefix=openshift/origin \
&lt;/span>&lt;span style="color:#b44"> -X github.com/openshift/origin/pkg/version.majorFromGit=3 \
&lt;/span>&lt;span style="color:#b44"> -X github.com/openshift/origin/pkg/version.minorFromGit=6+ \
&lt;/span>&lt;span style="color:#b44"> -X github.com/openshift/origin/pkg/version.versionFromGit=v3.6.0-alpha.0+83e3250-176-dirty \
&lt;/span>&lt;span style="color:#b44"> -X github.com/openshift/origin/pkg/version.commitFromGit=83e3250 \
&lt;/span>&lt;span style="color:#b44"> -X github.com/openshift/origin/pkg/version.buildDate=2017-04-06T05:34:29Z \
&lt;/span>&lt;span style="color:#b44"> -X github.com/openshift/origin/vendor/k8s.io/kubernetes/pkg/version.gitCommit=43a9be4 \
&lt;/span>&lt;span style="color:#b44"> -X github.com/openshift/origin/vendor/k8s.io/kubernetes/pkg/version.gitVersion=v1.5.2+43a9be4 \
&lt;/span>&lt;span style="color:#b44"> -X github.com/openshift/origin/vendor/k8s.io/kubernetes/pkg/version.buildDate=2017-04-06T05:34:29Z \
&lt;/span>&lt;span style="color:#b44"> -X github.com/openshift/origin/vendor/k8s.io/kubernetes/pkg/version.gitTreeState=clean&amp;#39;&lt;/span> &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> github.com/openshift/origin/cmd/openshift &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> github.com/openshift/origin/cmd/oc &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> github.com/openshift/origin/pkg/sdn/plugin/sdn-cni-plugin &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> github.com/openshift/origin/vendor/github.com/containernetworking/cni/plugins/ipam/host-local &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> github.com/openshift/origin/vendor/github.com/containernetworking/cni/plugins/main/loopback
&lt;/code>&lt;/pre>&lt;/div>&lt;p>可以看到openshift会编译出5个二进制来，其中3个和网络CNI接口有关，最后会放置到_output/local/bin/linux/amd64, 并作相关的软链接（oadm, kubelet）&lt;/p>
&lt;p>所以以后分析程序的切入点就从cmd/openshift和 cmd/oc入手就行了&lt;/p>
&lt;p>来看下编译成果&lt;/p>
&lt;pre tabindex="0">&lt;code>➜ origin git:(xxpDev) ✗ _output/local/bin/linux/amd64/oc version
oc v3.6.0-alpha.0+83e3250-176-dirty
kubernetes v1.5.2+43a9be4
features: Basic-Auth
&lt;/code>&lt;/pre>&lt;p>看到输出&lt;code>v3.6.0-alpha.0+83e3250-176-dirty&lt;/code>， 这就是上面编译时传进去的参数。&lt;/p>
&lt;p>&lt;code>-X github.com/openshift/origin/pkg/version.majorFromGit=3&lt;/code>,意思是说编译文件&lt;code>github.com/openshift/origin/pkg/version.go&lt;/code>时，对常量majorFromGit赋值为3&lt;/p>
&lt;h3 id="项目目录结构">项目目录结构&lt;/h3>
&lt;p>-- 未完待续&lt;/p></description></item><item><title>Docs: 配置harbor默认https访问</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/harbor_https/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/harbor_https/</guid><description>
&lt;p>因为使用自签证书（reg.300.cn），所以需要把中间过程生成的ca.crt拷贝到需要pull/push的node上 (懒的翻译了，很详细的文档，已验证OK)&lt;/p>
&lt;p>Because Harbor does not ship with any certificates, it uses HTTP by default to serve registry requests. This makes it relatively simple to configure. However, it is highly recommended that security be enabled for any production environment. Harbor has an Nginx instance as a reverse proxy for all services, you can configure Nginx to enable https.&lt;/p>
&lt;p>##Getting a certificate&lt;/p>
&lt;p>Assuming that your registry's &lt;strong>hostname&lt;/strong> is &lt;strong>reg.yourdomain.com&lt;/strong>, and that its DNS record points to the host where you are running Harbor. You first should get a certificate from a CA. The certificate usually contains a .crt file and a .key file, for example, &lt;strong>yourdomain.com.crt&lt;/strong> and &lt;strong>yourdomain.com.key&lt;/strong>.&lt;/p>
&lt;p>In a test or development environment, you may choose to use a self-signed certificate instead of the one from a CA. The below commands generate your own certificate:&lt;/p>
&lt;ol>
&lt;li>Create your own CA certificate:&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code> openssl req \
-newkey rsa:4096 -nodes -sha256 -keyout ca.key \
-x509 -days 365 -out ca.crt
&lt;/code>&lt;/pre>&lt;ol start="2">
&lt;li>Generate a Certificate Signing Request:&lt;/li>
&lt;/ol>
&lt;p>If you use FQDN like &lt;strong>reg.yourdomain.com&lt;/strong> to connect your registry host, then you must use &lt;strong>reg.yourdomain.com&lt;/strong> as CN (Common Name).
Otherwise, if you use IP address to connect your registry host, CN can be anything like your name and so on:&lt;/p>
&lt;pre tabindex="0">&lt;code> openssl req \
-newkey rsa:4096 -nodes -sha256 -keyout yourdomain.com.key \
-out yourdomain.com.csr
&lt;/code>&lt;/pre>&lt;ol start="3">
&lt;li>Generate the certificate of your registry host:&lt;/li>
&lt;/ol>
&lt;p>On Ubuntu, the config file of openssl locates at &lt;strong>/etc/ssl/openssl.cnf&lt;/strong>. Refer to openssl document for more information. The default CA directory of openssl is called demoCA. Let's create necessary directories and files:&lt;/p>
&lt;pre tabindex="0">&lt;code> mkdir demoCA
cd demoCA
touch index.txt
echo '01' &amp;gt; serial
cd ..
&lt;/code>&lt;/pre>&lt;p>If you're using FQDN like &lt;strong>reg.yourdomain.com&lt;/strong> to connect your registry host, then run this command to generate the certificate of your registry host:&lt;/p>
&lt;pre tabindex="0">&lt;code> openssl ca -in yourdomain.com.csr -out yourdomain.com.crt -cert ca.crt -keyfile ca.key -outdir .
&lt;/code>&lt;/pre>&lt;p>If you're using &lt;strong>IP&lt;/strong> to connect your registry host, you may instead run the command below:&lt;/p>
&lt;pre tabindex="0">&lt;code>
echo subjectAltName = IP:your registry host IP &amp;gt; extfile.cnf
openssl ca -in yourdomain.com.csr -out yourdomain.com.crt -cert ca.crt -keyfile ca.key -extfile extfile.cnf -outdir .
&lt;/code>&lt;/pre>&lt;p>##Configuration of Nginx
After obtaining the &lt;strong>yourdomain.com.crt&lt;/strong> and &lt;strong>yourdomain.com.key&lt;/strong> files, change the directory to Deploy/config/nginx in Harbor project.&lt;/p>
&lt;pre tabindex="0">&lt;code> cd Deploy/config/nginx
&lt;/code>&lt;/pre>&lt;p>Create a new directory cert/, if it does not exist. Then copy &lt;strong>yourdomain.com.crt&lt;/strong> and &lt;strong>yourdomain.com.key&lt;/strong> to cert/, e.g. :&lt;/p>
&lt;pre tabindex="0">&lt;code> cp yourdomain.com.crt cert/
cp yourdomain.com.key cert/
&lt;/code>&lt;/pre>&lt;p>Rename the existing configuration file of Nginx:&lt;/p>
&lt;pre tabindex="0">&lt;code> mv nginx.conf nginx.conf.bak
&lt;/code>&lt;/pre>&lt;p>Copy the template &lt;strong>nginx.https.conf&lt;/strong> as the new configuration file:&lt;/p>
&lt;pre tabindex="0">&lt;code> cp nginx.https.conf nginx.conf
&lt;/code>&lt;/pre>&lt;p>Edit the file nginx.conf and replace two occurrences of &lt;strong>harbordomain.com&lt;/strong> to your own host name, such as reg.yourdomain.com . If you use a customized port rather than the default port 443, replace the port &amp;quot;443&amp;quot; in the line &amp;quot;rewrite ^/(.*) https://$server_name:443/$1 permanent;&amp;quot; as well. Please refer to the &lt;a href="https://github.com/vmware/harbor/blob/master/docs/installation_guide.md">installation guide&lt;/a> for other required steps of port customization.&lt;/p>
&lt;pre tabindex="0">&lt;code> server {
listen 443 ssl;
server_name harbordomain.com;
...
server {
listen 80;
server_name harbordomain.com;
rewrite ^/(.*) https://$server_name:443/$1 permanent;
&lt;/code>&lt;/pre>&lt;p>Then look for the SSL section to make sure the files of your certificates match the names in the config file. Do not change the path of the files.&lt;/p>
&lt;pre tabindex="0">&lt;code> ...
# SSL
ssl_certificate /etc/nginx/cert/yourdomain.com.crt;
ssl_certificate_key /etc/nginx/cert/yourdomain.com.key;
&lt;/code>&lt;/pre>&lt;p>Save your changes in nginx.conf.&lt;/p>
&lt;p>##Installation of Harbor
Next, edit the file Deploy/harbor.cfg , update the hostname and the protocol:&lt;/p>
&lt;pre tabindex="0">&lt;code> #set hostname
hostname = reg.yourdomain.com
#set ui_url_protocol
ui_url_protocol = https
&lt;/code>&lt;/pre>&lt;p>Generate configuration files for Harbor:&lt;/p>
&lt;pre tabindex="0">&lt;code>./prepare
&lt;/code>&lt;/pre>&lt;p>If Harbor is already running, stop and remove the existing instance. Your image data remain in the file system&lt;/p>
&lt;pre tabindex="0">&lt;code> docker-compose stop
docker-compose rm
&lt;/code>&lt;/pre>&lt;p>Finally, restart Harbor:&lt;/p>
&lt;pre tabindex="0">&lt;code> docker-compose up -d
&lt;/code>&lt;/pre>&lt;p>After setting up HTTPS for Harbor, you can verify it by the following steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Open a browser and enter the address: &lt;a href="https://reg.yourdomain.com">https://reg.yourdomain.com&lt;/a> . It should display the user interface of Harbor.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>On a machine with Docker daemon, make sure the option &amp;quot;-insecure-registry&amp;quot; does not present, and you must copy ca.crt generated in the above step to &lt;strong>/etc/docker/certs.d/yourdomain.com&lt;/strong>(or your registry host IP), if the directory does not exist, create it.
If you mapped nginx port 443 to another port, then you should instead create the directory /etc/docker/certs.d/yourdomain.com:port(or your registry host IP:port). Then run any docker command to verify the setup, e.g.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;pre tabindex="0">&lt;code> docker login reg.yourdomain.com
&lt;/code>&lt;/pre>&lt;p>If you've mapped nginx 443 port to another, you need to add the port to login, like below:&lt;/p>
&lt;pre tabindex="0">&lt;code> docker login reg.yourdomain.com:port
&lt;/code>&lt;/pre>&lt;p>##Troubleshooting&lt;/p>
&lt;ol>
&lt;li>
&lt;p>You may get an intermediate certificate from a certificate issuer. In this case, you should merge the intermediate certificate with your own certificate to create a certificate bundle. You can achieve this by the below command:&lt;/p>
&lt;pre tabindex="0">&lt;code>cat intermediate-certificate.pem &amp;gt;&amp;gt; yourdomain.com.crt
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>On some systems where docker daemon runs, you may need to trust the certificate at OS level.
On Ubuntu, this can be done by below commands:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">cp youdomain.com.crt /usr/local/share/ca-certificates/reg.yourdomain.com.crt
update-ca-certificates
&lt;/code>&lt;/pre>&lt;/div>&lt;p>On Red Hat (CentOS etc), the commands are:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">cp yourdomain.com.crt /etc/pki/ca-trust/source/anchors/reg.yourdomain.com.crt
update-ca-trust
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol></description></item><item><title>Docs: 多负载均衡方案</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-router%E5%92%8Chaproxy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-router%E5%92%8Chaproxy/</guid><description>
&lt;p>haproxy在openshift里默认有两种用处，一个种负责master的高可用，一种是负责外部对内服务的访问（ingress controller）&lt;/p>
&lt;p>平台部署情况：&lt;/p>
&lt;ul>
&lt;li>3台master，etcd&lt;/li>
&lt;li>1台node&lt;/li>
&lt;li>1台lb（haproxy）&lt;/li>
&lt;/ul>
&lt;h2 id="haproxy负载均衡master的高可用">haproxy负载均衡master的高可用&lt;/h2>
&lt;p>lb负责master间的负载均衡，其实负载没那么大，更多得是用来避免单点故障&lt;/p>
&lt;h3 id="debug介绍">Debug介绍&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>默认安装haproxy1.5.18版本，开启debug方法&lt;/p>
&lt;pre tabindex="0">&lt;code># 默认systemd对haproxy做了封装，会以-Ds后台形式启动，debug信息是看不到的
systemctl stop harproxy
# vi /etc/haproxy/haproxy.cfg
log 127.0.0.1 local3 debug
# 手动启动haproxy
haproxy -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -d
&lt;/code>&lt;/pre>&lt;p>不知道是不是哪里还需要设置，打印出来的日志，信息并不是不太多&lt;/p>
&lt;p>另外浏览&lt;code>https://lbIP:9000&lt;/code>, 可以看到统计信息&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="配置介绍">配置介绍&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>使用&lt;a href="https://github.com/xiaoping378/openshift-deploy">openshift-ansible&lt;/a>部署后，harpxy的配置如下&lt;/p>
&lt;pre tabindex="0">&lt;code>[root@node4 ~]# cat /etc/haproxy/haproxy.cfg
# Global settings
#---------------------------------------------------------------------
global
chroot /var/lib/haproxy
pidfile /var/run/haproxy.pid
maxconn 20000
user haproxy
group haproxy
daemon
log /dev/log local0 info #定义debug级别
# turn on stats unix socket
stats socket /var/lib/haproxy/stats
#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults #默认配置，后面同KEY的设置会覆盖此处
mode http #工作在七层代理，客户端请求在转发至后端服务器之前将会被深度分板，所有不与RFC格式兼容的请求都会被拒绝，一些七层的过滤处理手段，可以使用。
log global #默认启用gloabl的日志设置
option httplog #默认日志类别为http日志格式
option dontlognull #不记录健康检查日志信息（端口扫描，空信息）
# option http-server-close
option forwardfor except 127.0.0.0/8 #如果上游服务器上的应用程序想记录客户端的真实IP地址，haproxy会把客户端的IP信息发送给上游服务器，在HTTP请求中添加”X-Forwarded-For”字段,但当是haproxy自身的健康检测机制去访问上游服务器时是不应该把这样的访问日志记录到日志中的，所以用except来排除127.0.0.0，即haproxy自身
option redispatch #代理的服务器挂掉后，强制定向到其他健康的服务器，避免cookie信息过时，仍可正常访问
retries 3 #3次连接失败就认为后端服务器不可用
timeout http-request 10s #默认客户端发送http请求的超时时间， 防DDOS攻击手段
timeout queue 1m #当后台服务器maxconn满了后，haproxy会把client发送来的请求放进一个队列中，一旦事件超过timeout queue，还没被处理，haproxy会自动返回503错误。
timeout connect 10s #haproxy与后端服务器连接超时时间，如果在同一个局域网可设置较小的时间
timeout client 300s #默认客户端与haproxy连接后，数据传输完毕，不再有数据传输，即非活动连接的超时时间
timeout server 300s #定义haproxy与后台服务器非活动连接的超时时间
timeout http-keep-alive 10s #默认新的http请求建立连接的超时时间，时间较短时可以尽快释放出资源，节约资源。和http-request配合使用
timeout check 10s #健康检测的时间的最大超时时间
maxconn 20000 #最大连接数
listen stats :9000
mode http
stats enable
stats uri /
frontend atomic-openshift-api
bind *:8443
default_backend atomic-openshift-api
mode tcp #在此模式下，客户端和服务器端之前将建立一个全双工的连接，不会对七层（http）报文做任何检查
option tcplog
backend atomic-openshift-api
balance source #是基于请求源IP的算法，此算法对请求的源IP时行hash运算，然后将结果除以后端服务器的权重总和，来判断转发至哪台后端服务器，这种方法可保证同一客户端IP的请求始终转发到固定定的后端服务器。
mode tcp
server master0 192.168.56.100:8443 check
server master1 192.168.56.101:8443 check
server master2 192.168.56.102:8443 check
&lt;/code>&lt;/pre>&lt;p>&lt;a href="http://cbonte.github.io/haproxy-dconv/1.5/configuration.html">官方文档&lt;/a>介绍的非常详细，感兴趣的可以继续深入研究&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="router负责外部对内服务的访问">Router负责外部对内服务的访问&lt;/h2>
&lt;h3 id="部署一个router并实现高可用">部署一个Router并实现高可用&lt;/h3>
&lt;p>router是由harpoxy来承担的， 可以理解成kubernetes里的ingress controller部分，默认跑在容器里。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>使能 default项目下router，可以访问hostnetwork&lt;/p>
&lt;pre>&lt;code>oc adm policy add-scc-to-user hostnetwork system:serviceaccount:default:router
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>使能其可以查看 label&lt;/p>
&lt;pre>&lt;code>oc adm policy add-cluster-role-to-user \
cluster-reader \
system:serviceaccount:default:router
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>部署1个router， 选择具有标签&lt;code>router=true&lt;/code>的节点&lt;/p>
&lt;pre>&lt;code># 对节点设置标签
oc label 192.168.56.110 router=true
# 部署并指定serviceaccount
oc adm router router --replicas=1 --selector='router=true' --service-account=router
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>设置router自身的高可用，参考&lt;a href="https://docs.openshift.org/latest/admin_guide/high_availability.html#admin-guide-high-availability">这里&lt;/a>&lt;/p>
&lt;p>默认使用keepalived实现多个router的高可用，访问router变成访问VIP地址，keepalived再根据权重和健康监测，利用VRRP通告外界后台到底那个router在服务。&lt;/p>
&lt;pre>&lt;code># 添加另一个node作为冗余
oc label no 192.168.56.111 router=true
oc scale dc router --replicas=2
#绑定serviceaccount特权，因为keepalived要操作iptables
oc adm policy add-scc-to-user privileged system:serviceaccount:default:ipfailover
#创建keepalived并指定VIP
oc adm ipfailover ha-router \
--replicas=2 --watch-port=80 \
--selector=&amp;quot;router=true&amp;quot; \
--virtual-ips=&amp;quot;192.168.56.170&amp;quot; \
--iptables-chain=&amp;quot;INPUT&amp;quot; \
--service-account=ipfailover --create
&lt;/code>&lt;/pre>
&lt;p>这样，刚才创建的router就自高可用了，通过&lt;code>192.168.56.170&lt;/code>来访问，有一点值得注意，&lt;/p>
&lt;ul>
&lt;li>按照现在的例子，如果以后还有router做高可用的话，要加上&lt;code>--vrrp-id-offset=1&lt;/code>，保证一个vip用一个独有的vrrp-id。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="router分片">Router分片&lt;/h3>
&lt;p>路由分片的概念，就是集群内有多个router，通过label来负责不同的routes。&lt;/p>
&lt;p>这样可以实现一个project独享一个router，或者某几个route独享一个router，再或者大型集群，更多样化的需求，用这个router sharding的概念也可以满足。&lt;/p>
&lt;p>我现在还没有具体的场景，先不实践，后续有机会会跟进更新下。&lt;/p></description></item><item><title>Docs: 镜像管理</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86/</guid><description>
&lt;p>刚接触docker时，第一个接触到的应该就是镜像了，docker之所以如此火热，个人认为一大部分原因就是这个镜像的提出，极大的促进了DevOps推广和软件复用的能力。&lt;/p>
&lt;p>而openshift对镜像的管理非常强大，直到写这篇blog，我才真正意识到这点，甚至犹豫是不是要放到开发实战篇后再来写&lt;code>镜像管理&lt;/code>。&lt;/p>
&lt;p>简要说下openshift里使用镜像的情况：&lt;/p>
&lt;ul>
&lt;li>首先openshift可以利用任何实现了&lt;code>Docker registry API&lt;/code>的镜像仓，比如，Vmware的Harbor项目，Docker hub以及集成镜像仓（ integrated registry）&lt;/li>
&lt;li>集成镜像仓，openshift内部的，可以动态生成，自动让用户编译的镜像有地方存， 其次它还负责通知openshift镜像的变动，然后openshift会根据策略去决定编译其他依赖镜像还是部署应用&lt;/li>
&lt;li>第三方镜像， 可通过命令&lt;code>oc import-image &amp;lt;stream&amp;gt;&lt;/code>来实时获取镜像tag信息并转换成镜像流，继而触发后续的编译或者部署。&lt;/li>
&lt;li>当然&lt;code>oc new-app&lt;/code>也支持直接从第三方镜像仓或者本地镜像里启动一个应用&lt;/li>
&lt;/ul>
&lt;p>文末有安装集成镜像仓的说明，先介绍image Streams 和 istag的概念和应用场景。&lt;/p>
&lt;h2 id="镜像管理">镜像管理&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>openshift基于docker的image概念又延伸出了Image Streams和Image Stream Tags概念&lt;/p>
&lt;p>默认openshift项目下会有一些镜像流，是供自带模板里用的，所以想加速部署模板的话，可以在改这里，通过istag指向本地镜像仓。&lt;/p>
&lt;pre tabindex="0">&lt;code>oc get is -n openshift
oc get istag -n openshift
&lt;/code>&lt;/pre>&lt;p>image，通俗讲就是对应用运行依赖（库，配置，运行环境）的一个打包。&lt;code>docker pull push&lt;/code>， 就是操作的镜像。
为什么openshift还要抽象出is和istag呢，主要是为了打通集成编译和部署环节（bc和dc），原生API就支持了DevOps理念。后面会细讲bc和dc&lt;/p>
&lt;p>is,开发人员可以理解成git的分支，每个分支都会编译很多临时版本出来，这个就是对应到is～=分支和istag～=版本号。
其实is和istag只是记录了一些映射关系，并不会存放实际镜像数据，比如is里记录了build后要output的镜像仓地址和所有tags，而istag里又记录了具体某个tag与image（可能是存于外部镜像仓，也能是某个is）的关系， 利用此实现了bc/dc和镜像的解耦。&lt;/p>
&lt;p>这里通过部署jenkins服务，来初步了解下具体的含义,&lt;/p>
&lt;ul>
&lt;li>创建ci项目&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>oc new-project ci
# 先拉取必要镜像
docker pull openshift/jenkins-1-centos7
#通过模板部署，下面一条命令就可以创建一个临时的jenkins服务的
#oc new-app jenkins-ephemeral
#跑之前我们先来注意几点
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>更改默认的is&lt;/li>
&lt;/ul>
&lt;p>先来查看默认的is&lt;/p>
&lt;pre tabindex="0">&lt;code>oc get template jenkins-ephemeral -n openshift -o json
...
&amp;quot;triggers&amp;quot;: [
{
&amp;quot;imageChangeParams&amp;quot;: {
&amp;quot;automatic&amp;quot;: true,
&amp;quot;containerNames&amp;quot;: [
&amp;quot;jenkins&amp;quot;
],
&amp;quot;from&amp;quot;: {
&amp;quot;kind&amp;quot;: &amp;quot;ImageStreamTag&amp;quot;,
&amp;quot;name&amp;quot;: &amp;quot;${JENKINS_IMAGE_STREAM_TAG}&amp;quot;,
&amp;quot;namespace&amp;quot;: &amp;quot;${NAMESPACE}&amp;quot;
},
&amp;quot;lastTriggeredImage&amp;quot;: &amp;quot;&amp;quot;
},
&amp;quot;type&amp;quot;: &amp;quot;ImageChange&amp;quot;
},
{
&amp;quot;type&amp;quot;: &amp;quot;ConfigChange&amp;quot;
}
]
...
{
&amp;quot;name&amp;quot;: &amp;quot;NAMESPACE&amp;quot;,
&amp;quot;displayName&amp;quot;: &amp;quot;Jenkins ImageStream Namespace&amp;quot;,
&amp;quot;description&amp;quot;: &amp;quot;The OpenShift Namespace where the Jenkins ImageStream resides.&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;openshift&amp;quot;
},
{
&amp;quot;name&amp;quot;: &amp;quot;JENKINS_IMAGE_STREAM_TAG&amp;quot;,
&amp;quot;displayName&amp;quot;: &amp;quot;Jenkins ImageStreamTag&amp;quot;,
&amp;quot;description&amp;quot;: &amp;quot;Name of the ImageStreamTag to be used for the Jenkins image.&amp;quot;,
&amp;quot;value&amp;quot;: &amp;quot;jenkins:latest&amp;quot;
}
...
&lt;/code>&lt;/pre>&lt;p>可以看到默认模板里部署jenkins时，会从openshfit的namespace里拉取jenkins:latest的镜像, 去openshift项目里找找看，确实存在对应的is&lt;/p>
&lt;pre tabindex="0">&lt;code>➜ ~ oc get is -n openshift | grep jenkins
jenkins 170.16.131.234:5000/openshift/jenkins latest,1 2 days ago
➜ ~ oc get istag -n openshift | grep jenkins:latest
jenkins:latest openshift/jenkins-1-centos7@sha256:ab590529e20470e53c1d4b6b970de5d4fd357d864320735a75c1df0e2fffde07 2 days ago sha256:ab590529e20470e53c1d4b6b970de5d4fd357d864320735a75c1df0e2fffde07
&lt;/code>&lt;/pre>&lt;p>上面的命令的输出，有两个点要阐述下，&lt;/p>
&lt;ul>
&lt;li>is里的&lt;code>170.16.131.234:5000/openshift/jenkins&lt;/code>是个可有可无的地址，一般系统会填写集成镜像仓的地址&lt;/li>
&lt;li>而istag里&lt;code>openshift/jenkins-1-centos7@sha256:ab&lt;/code>则是指明了对应tag的镜像来源，&lt;/li>
&lt;/ul>
&lt;p>这样的话，默认执行&lt;code>oc new-app jenkins-ephemeral&lt;/code>的话，会从docker.io那里拉取镜像 openshift/jenkins-1-centos7@sha256:ab590529e20470e53c1d4b6b970de5d4fd357d864320735a75c1df0e2fffde07&lt;/p>
&lt;p>为了加速部署，我们把刚才pull下来的镜像，push到集成镜像仓里&lt;/p>
&lt;pre tabindex="0">&lt;code>#添加用户
#htpasswd /etc/origin/master/htpasswd xxp
docker tag openshift/jenkins-1-centos7 hub2.300.cn/openshift/jenkins
docker login -u developer -p `oc whoami -t` hub2.300.cn
docker push hub2.300.cn/openshift/jenkins
&lt;/code>&lt;/pre>&lt;p>push完后再来看istag的变化，由以前的&lt;code>openshift/jenkins-1-centos&lt;/code>变为了&lt;code>170.16.131.234:5000/openshift/jenkins&lt;/code>&lt;/p>
&lt;pre tabindex="0">&lt;code>➜ ~ oc get istag -n openshift | grep jenkins:latest
jenkins:latest 170.16.131.234:5000/openshift/jenkins@sha256:dc0f434a492d11d6ae13711e77f87303a06a8fc0fb3a97ae327a4b88c33435b6 21 hours ago sha256:dc0f434a492d11d6ae13711e77f87303a06a8fc0fb3a97ae327a4b88c33435b6
&lt;/code>&lt;/pre>&lt;p>这里是push到集成镜像仓后，系统会自动更新对应的is和istag， 为了加速部署，更改istag还可以通过import-image从私有镜像仓（harbor）里来完成&lt;/p>
&lt;p>这样再来部署jenkins应用就快速多了&lt;/p>
&lt;p>还有很多更细致的东西，比如如何周期同步第三方镜像仓等等，有需要的查看&lt;a href="https://docs.openshift.org/latest/dev_guide/managing_images.html#creating-an-image-stream-by-manually-pushing-an-image">官文&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="安装独立镜像仓">安装独立镜像仓&lt;/h2>
&lt;p>可以通过openshift-ansible一次性安装OK，这里使用CLI安装，正好可以串一下openshift里的各种概念的使用场景。&lt;/p>
&lt;h3 id="部署">部署&lt;/h3>
&lt;pre tabindex="0">&lt;code>oc project default
oc adm policy add-scc-to-user privileged system:serviceaccount:default:registry
oc label node 192.168.56.102 registry=true
#使用hostPath存储,要在node上放行权限，默认registry用的1001 userID， 不然后续挂载进去的volume，没有写权限
sudo chown 1001:root /home/registry
#注意要指定使用的镜像版本，默认是拉取最新的， 一定要指明``--volume``参数，不然deploy，不会挂载主机目录，官文这里遗漏了。
oc adm registry --images=&amp;quot;openshift/origin-docker-registry:v1.4.1&amp;quot; --selector=&amp;quot;registry=true&amp;quot; --mount-host=&amp;quot;/home/registry&amp;quot; --service-account=registry --volume='/registry'```
中间莫名出现部署``error``的状态，重新部署了下``oc deploy docker-registry --retry``
### 加密镜像仓
- 先拿到serviceIP
```bash
[root@node0 master]# oc get svc docker-registry
NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE
docker-registry 170.16.132.252 &amp;lt;none&amp;gt; 5000/TCP 1h
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>创建自签名证书,如果已经有了，跳过此步&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>oc adm ca create-server-cert \
--signer-cert=/etc/origin/master/ca.crt \
--signer-key=/etc/origin/master/ca.key \
--signer-serial=/etc/origin/master/ca.serial.txt \
--hostnames='hub.example.com,docker-registry.default.svc.cluster.local,170.16.132.252:5000' \
--cert=/etc/secrets/registry.crt \
--key=/etc/secrets/registry.key
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>创建secret， secret是专门来保存敏感信息的，比如密码，sshkey，token信息等等&lt;/p>
&lt;p>更详细的介绍可以查看&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/secrets.md">这里&lt;/a>&lt;/p>
&lt;pre tabindex="0">&lt;code>oc secrets new registry-secret \
/etc/secrets/registry.crt \
/etc/secrets/registry.key
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>绑定secret到serviceaccount&lt;/p>
&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>oc secrets link registry registry-secret
oc secrets link default registry-secret
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>更新dc，添加volume，把新创建的secret挂进去&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>oc volume dc/docker-registry --add --type=secret \
--name=docker-registry --secret-name=registry-secret -m /etc/secrets
&lt;/code>&lt;/pre>&lt;p>如果想移除的话，如下&lt;/p>
&lt;pre tabindex="0">&lt;code>oc volume dc/docker-registry --remove --name=docker-registry
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>更新环境变量&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>oc set env dc/docker-registry \
REGISTRY_HTTP_TLS_CERTIFICATE=/etc/secrets/registry.crt \
REGISTRY_HTTP_TLS_KEY=/etc/secrets/registry.key
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>更新健康监测 HTTP-&amp;gt;HTTPS&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>oc patch dc/docker-registry -p '{&amp;quot;spec&amp;quot;: {&amp;quot;template&amp;quot;: {&amp;quot;spec&amp;quot;: {&amp;quot;containers&amp;quot;:[{
&amp;quot;name&amp;quot;:&amp;quot;registry&amp;quot;,
&amp;quot;livenessProbe&amp;quot;: {&amp;quot;httpGet&amp;quot;: {&amp;quot;scheme&amp;quot;:&amp;quot;HTTPS&amp;quot;}}
}]}}}}'
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>oc patch dc/docker-registry -p '{&amp;quot;spec&amp;quot;: {&amp;quot;template&amp;quot;: {&amp;quot;spec&amp;quot;: {&amp;quot;containers&amp;quot;:[{
&amp;quot;name&amp;quot;:&amp;quot;registry&amp;quot;,
&amp;quot;readinessProbe&amp;quot;: {&amp;quot;httpGet&amp;quot;: {&amp;quot;scheme&amp;quot;:&amp;quot;HTTPS&amp;quot;}}
}]}}}}'
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>验证是否OK&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>[root@node0 master]# oc logs dc/docker-registry | grep tls
time=&amp;quot;2017-03-02T16:01:41.113619323Z&amp;quot; level=info msg=&amp;quot;listening on :5000, tls&amp;quot; go.version=go1.7.4 instance.id=594aa09b-4540-4e38-a85a-851261cd1254
&lt;/code>&lt;/pre>&lt;h3 id="添加路由">添加路由&lt;/h3>
&lt;pre tabindex="0">&lt;code>oc create route passthrough registry --service=docker-registry --hostname=hub2.300.cn
&lt;/code>&lt;/pre>&lt;h3 id="登录镜像仓">登录镜像仓&lt;/h3>
&lt;pre tabindex="0">&lt;code>oc policy add-role-to-user admin developer -n default
oc login -u developer
docker login -u developer -p `oc whoami -t` hub2.300.cn
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>push镜像&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>docker push hub2.300.cn/default/busybox
&lt;/code>&lt;/pre>&lt;h3 id="安装镜像仓console">安装镜像仓console&lt;/h3>
&lt;ul>
&lt;li>oc利用官方模板安装&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>oc create -n default -f https://raw.githubusercontent.com/openshift/openshift-ansible/master/roles/openshift_hosted_templates/files/v1.4/origin/registry-console.yaml
oc create route passthrough --service registry-console \
--hostname hub3.300.cn \
-n default
oc new-app -n default --template=registry-console \
-p OPENSHIFT_OAUTH_PROVIDER_URL=&amp;quot;https://192.168.31.100:8443&amp;quot; \
-p REGISTRY_HOST=$(oc get route docker-registry -n default --template='{{ .spec.host }}') \
-p COCKPIT_KUBE_URL=$(oc get route registry-console -n default --template='https://{{ .spec.host }}')
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>登录浏览器打开&lt;code>https://hub3.300.cn/registry&lt;/code>,使用已有的账户登录，比如这里是默认的developer和developer。
&lt;img src="https://xiaoping378.github.io/openshift-registry-console.png" alt="界面">&lt;/li>
&lt;/ul></description></item><item><title>Docs: DEIS 开源PAAS平台实践</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/deis-%E5%BC%80%E6%BA%90paas%E5%B9%B3%E5%8F%B0%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/deis-%E5%BC%80%E6%BA%90paas%E5%B9%B3%E5%8F%B0%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/</guid><description>
&lt;p>DEIS（目前已被微软收购）的workflow是开源的Paas平台，基于kubernetes做了一层面向开发者的CLI和接口，做到了让开发者对容器无感知的情况下快速的开发和部署线上应用。&lt;/p>
&lt;blockquote>
&lt;p>workflow是 on top of k8s的，所有组件默认全是跑在pod里的，不像openshift那样对k8s的侵入性很大。&lt;/p>
&lt;/blockquote>
&lt;p>特性如下：&lt;/p>
&lt;ul>
&lt;li>S2I(自动识别源码直接编译成镜像)&lt;/li>
&lt;li>日志聚合&lt;/li>
&lt;li>应用管理（发布，回滚）&lt;/li>
&lt;li>认证&amp;amp;授权机制&lt;/li>
&lt;li>边界路由&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://xiaoping378.github.io/Workflow_Detail.png" alt="Workflow_Detail">&lt;/p>
&lt;p>下面从环境搭建，安装workflow及其基本使用做个梳理。&lt;/p>
&lt;h3 id="初始化k8s集群">初始化k8s集群&lt;/h3>
&lt;p>可以通过&lt;a href="https://github.com/xiaoping378/k8s-deploy">k8s-deploy&lt;/a>项目来离线安装高可用kubernetes集群，我这里是单机演示环境。&lt;/p>
&lt;pre>&lt;code>kubeadm init --kubernetes-version v1.6.2 --pod-network-cidr 12.240.0.0/12
#方便命令自动补全
source &amp;lt;(kubectl completion zsh)
#安装cni网络
cp /etc/kubernetes/admin.conf $HOME/.kube/config
kubectl apply -f kube-flannel-rbac.yml
kubectl apply -f kube-flannel.yml
#使能master可以被调度
kubectl taint node --all node-role.kubernetes.io/master-
#安装ingress-controller, 边界路由作用
kubectl create -f ingress-traefik-rbac.yml
kubectl create -f ingress-traefik-deploy.yml
&lt;/code>&lt;/pre>
&lt;h3 id="初始化helm">初始化helm&lt;/h3>
&lt;p>helm相当于kubernetes里的包管理器，类似yum和apt的作用，只不过它操作的是charts（各种k8s yaml文件的集合，额外还有Chart.yaml -- 包的描述文件）可以理解为基于k8s的应用模板管理类工具， 后面会用它来安装workflow到上面跑起来的k8s集群里。&lt;/p>
&lt;p>从k8s 1.6之后，kubeadm安装的集群，默认会开启RBAC机制，为了让helm可以安装任何应用，我们这里赋予tiller cluster-admin权限&lt;/p>
&lt;pre>&lt;code>kubectl create serviceaccount helm --namespace kube-system
kubectl create clusterrolebinding cluster-admin-helm --clusterrole=cluster-admin --serviceaccount=kube-system:helm
&lt;/code>&lt;/pre>
&lt;p>初始化helm：&lt;/p>
&lt;pre>&lt;code>➜ helm init --service-account helm
$HELM_HOME has been configured at /home/xxp/.helm.
Tiller (the helm server side component) has been installed into your Kubernetes Cluster.
Happy Helming!
➜ helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.4.1&amp;quot;, GitCommit:&amp;quot;46d9ea82e2c925186e1fc620a8320ce1314cbb02&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.4.1&amp;quot;, GitCommit:&amp;quot;46d9ea82e2c925186e1fc620a8320ce1314cbb02&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
&lt;/code>&lt;/pre>
&lt;p>安装后，默认导入了2个repos，后面安装和搜索应用时，都是从这2个仓里出的，当然也可以自己通过&lt;code>helm repo add&lt;/code>添加本地私有仓&lt;/p>
&lt;pre>&lt;code>➜ helm repo list
NAME URL
stable https://kubernetes-charts.storage.googleapis.com
local http://127.0.0.1:8879/charts
&lt;/code>&lt;/pre>
&lt;p>helm的使用基本流程如下:&lt;/p>
&lt;ul>
&lt;li>helm search: 搜索自己想要安装的应用（chart）&lt;/li>
&lt;li>helm fetch: 下载应用（chart）到本地，可以忽略此步&lt;/li>
&lt;li>helm install: 安装应用&lt;/li>
&lt;li>helm list: 查看已安装的应用情况&lt;/li>
&lt;/ul>
&lt;h3 id="安装workflow">安装workflow&lt;/h3>
&lt;p>添加workflow的repo仓&lt;/p>
&lt;pre>&lt;code>helm repo add deis https://charts.deis.com/workflow
&lt;/code>&lt;/pre>
&lt;p>开始安装workflow，因为RBAC的原因，同样要赋予workflow各组件相应的权限，yml文件在[这里]（https://gist.github.com/xiaoping378/798c39e0b607be4130db655f4873bd24）&lt;/p>
&lt;pre>&lt;code>kubectl apply -f workflow-rbac.yml --namespace deis
helm install deis/workflow --name workflow --namespace deis \
--set global.experimental_native_ingress=true,controller.platform_domain=192.168.31.49.xip.io
&lt;/code>&lt;/pre>
&lt;p>其中会拉取所需镜像，不出意外会有如下结果：&lt;/p>
&lt;pre>&lt;code>➜ kubectl --namespace=deis get pods
NAME READY STATUS RESTARTS AGE
deis-builder-1134410811-11xpp 1/1 Running 0 46m
deis-controller-2000207379-5wr10 1/1 Running 1 46m
deis-database-244447703-v2sh9 1/1 Running 0 46m
deis-logger-2533678197-pzmbs 1/1 Running 2 46m
deis-logger-fluentd-08hms 1/1 Running 0 42m
deis-logger-redis-1307646428-fz1kk 1/1 Running 0 46m
deis-minio-3195500219-tv7wz 1/1 Running 0 46m
deis-monitor-grafana-59098797-mdqh1 1/1 Running 0 46m
deis-monitor-influxdb-168332144-24ngs 1/1 Running 0 46m
deis-monitor-telegraf-vgbr9 1/1 Running 0 41m
deis-nsqd-1042535208-40fkm 1/1 Running 0 46m
deis-registry-2249489191-2jz3p 1/1 Running 2 46m
deis-registry-proxy-qsqc2 1/1 Running 0 46m
deis-router-3258454730-3rfpq 1/1 Running 0 41m
deis-workflow-manager-3582051402-m11zn 1/1 Running 0 46m
&lt;/code>&lt;/pre>
&lt;h3 id="注册管理用户">注册管理用户&lt;/h3>
&lt;p>由于我们是本地ingress-controller, 必须保障deis-builder.$host可以被解析, 自行创建ingress of deis-builder.&lt;/p>
&lt;pre>&lt;code>kubectl apply -f deis-buidler-ingress.yml
&lt;/code>&lt;/pre>
&lt;p>确保traefik有如下状态：&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/traefik-status.png" alt="traefik-status">&lt;/p>
&lt;p>如下操作注册，默认第一个用户为管理员用户，可操作所有其他用户。&lt;/p>
&lt;pre>&lt;code>➜ ~ kubectl get --namespace deis ingress
NAME HOSTS ADDRESS PORTS AGE
builder-api-server-ingress-http deis-builder.192.168.31.49.xip.io 80 18m
controller-api-server-ingress-http deis.192.168.31.49.xip.io 80 1h
➜ ~
➜ ~ deis register deis.192.168.31.49.xip.io
username: admin
password:
password (confirm):
email: xiaoping378@163.com
Registered admin
Logged in as admin
Configuration file written to /home/xxp/.deis/client.json
➜ ~
➜ ~ deis whoami
You are admin at http://deis.192.168.31.49.xip.io
&lt;/code>&lt;/pre>
&lt;h3 id="部署第一个应用">部署第一个应用&lt;/h3></description></item><item><title>Docs: k3s实践-01</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/k3s-%E5%BC%80%E7%AF%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/k3s-%E5%BC%80%E7%AF%87/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>本文主要介绍k3s的安装和核心组件解读。&lt;/p>
&lt;/div>
&lt;p>k3s是all-in-one的轻量k8s发行版，把所有k8s组件打包成一个不到100M的二进制文件了。具备如下显著特点：&lt;/p>
&lt;ul>
&lt;li>打包成单一二进制&lt;/li>
&lt;li>默认集成了sqlite3来替代etcd，也可以指定其他数据库：etcd3、mysql、postgres。&lt;/li>
&lt;li>默认内置Coredns、Metrics Server、Flannel、Traefik ingress、Local-path-provisioner等&lt;/li>
&lt;li>默认启用了TLS加密通信。&lt;/li>
&lt;/ul>
&lt;h2 id="安装">安装&lt;/h2>
&lt;p>官方提供了一键安装脚本&lt;a href="https://get.k3s.io">install.sh&lt;/a> ，执行&lt;code>curl -sfL https://get.k3s.io | sh -&lt;/code>可一键安装server端。此命令会从&lt;code>https://update.k3s.io/v1-release/channels/stable&lt;/code>取到最新的稳定版安装，可以通过&lt;code>INSTALL_K3S_VERSION&lt;/code>环境变量指定版本，本文将以1.19为例。&lt;/p>
&lt;p>&lt;strong>启动 k3s server端(master节点).&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl -sfL https://get.k3s.io | &lt;span style="color:#b8860b">INSTALL_K3S_VERSION&lt;/span>&lt;span style="color:#666">=&lt;/span>v1.19.16+k3s1 sh -
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>由于网络原因，可能会失败，自行想办法&lt;a href="https://github.com/k3s-io/k3s/releases/download/v1.19.16+k3s1/k3s">下载&lt;/a>下来，放置 &lt;code>/usr/local/bin/k3s&lt;/code>，附上执行权限&lt;code>chmod a+x /usr/local/bin/k3s&lt;/code>, 然后上面的命令加上&lt;code>INSTALL_K3S_SKIP_DOWNLOAD=true&lt;/code>再执行一遍即可。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>安装里log里会输出一些重要信息: &lt;code>kubectl、crictl、卸载脚本、systemd service&lt;/code>&lt;/p>
&lt;/blockquote>
&lt;p>不出意外，k3s server会被systemd启动，执行命令查看&lt;code>systemctl status k3s&lt;/code>或者通过软链的kubectl验证是否启动成功：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ kubectl get no
NAME STATUS ROLES AGE VERSION
gitlab-server Ready master 6m43s v1.19.16+k3s1
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>(Optional)&lt;/strong> 启动 k3s agent端 (添加worker节点).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">curl -sfL https://get.k3s.io | &lt;span style="color:#b8860b">K3S_URL&lt;/span>&lt;span style="color:#666">=&lt;/span>https://172.25.11.130:6443 &lt;span style="color:#b8860b">K3S_TOKEN&lt;/span>&lt;span style="color:#666">=&lt;/span>bulabula &lt;span style="color:#b8860b">INSTALL_K3S_VERSION&lt;/span>&lt;span style="color:#666">=&lt;/span>v1.19.16+k3s1 sh -
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;code>K3S_TOKEN&lt;/code>内容需要从server端的&lt;code>/var/lib/rancher/k3s/server/node-token&lt;/code>文件取出&lt;/li>
&lt;li>&lt;code>K3S_URL&lt;/code>中的IP是master节点的IP。&lt;/li>
&lt;/ul>
&lt;h2 id="集群访问">集群访问&lt;/h2>
&lt;p>默认kubectl通过localhost访问本地集群，所以上文敲kubectl是没问题的，如果要被外部访问或者纳管的话，可以把kubeconfig文件拷走，默认路径是 &lt;code>/etc/rancher/k3s/k3s.yaml&lt;/code>
。记得修改文件内的server字段，改成外部可访问到的IP。&lt;/p>
&lt;h2 id="架构说明">架构说明&lt;/h2>
&lt;p>TODO.&lt;/p></description></item><item><title>Docs: 性能优化指南</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97/</guid><description>
&lt;p>主要参考的官方&lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/scaling_and_performance_guide/">链接&lt;/a>， 本文是基于openshift 3.5说的。&lt;/p>
&lt;h2 id="概览">概览&lt;/h2>
&lt;p>本指南提供了如何提高OpenShift容器平台的集群性能和生产环境下的最佳实践。 主要包括建立，扩展和调优OpenShift集群的推荐做法。&lt;/p>
&lt;p>个人看法，其实性能这个东西是个权衡的过程，根据自身硬件条件和实际需求，选择适合自己的调优手段。&lt;/p>
&lt;h2 id="安装实践">安装实践&lt;/h2>
&lt;h3 id="网络依赖">网络依赖&lt;/h3>
&lt;p>首先安装自然要选择官方的&lt;a href="https://github.com/openshift/openshift-ansible">openshift-ansible项目&lt;/a>， 默认是rpm安装方式，需要依赖网络，比如要去联网下载&lt;code>atomic-openshift-*, iptables, 和 docker&lt;/code>包依赖，&lt;/p>
&lt;p>如果有不能联网的节点，可以参考我之前写的&lt;a href="https://github.com/xiaoping378/openshift-deploy">离线安装openshift&lt;/a>。&lt;/p>
&lt;h3 id="ansible优化">ansible优化&lt;/h3>
&lt;p>官方推荐使用ansible安装，这里说下针对ansible的优化，以提高安装效率，主要参考&lt;a href="https://www.ansible.com/blog/ansible-performance-tuning">ansible官方blog&lt;/a>,&lt;/p>
&lt;p>如果参考上文离线安装的话，不建议跨外网连接rpm仓或者镜像仓，下面是推荐的ansible配置&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># cat /etc/ansible/ansible.cfg&lt;/span>
&lt;span style="color:#080;font-style:italic"># config file for ansible -- http://ansible.com/&lt;/span>
&lt;span style="color:#080;font-style:italic"># ==============================================&lt;/span>
&lt;span style="color:#666">[&lt;/span>defaults&lt;span style="color:#666">]&lt;/span>
&lt;span style="color:#b8860b">forks&lt;/span> &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#666">20&lt;/span> &lt;span style="color:#080;font-style:italic"># 20个并发是理想值，太高的话中间会有概率出错&lt;/span>
&lt;span style="color:#b8860b">host_key_checking&lt;/span> &lt;span style="color:#666">=&lt;/span> False
&lt;span style="color:#b8860b">remote_user&lt;/span> &lt;span style="color:#666">=&lt;/span> root
&lt;span style="color:#b8860b">roles_path&lt;/span> &lt;span style="color:#666">=&lt;/span> roles/
&lt;span style="color:#b8860b">gathering&lt;/span> &lt;span style="color:#666">=&lt;/span> smart
&lt;span style="color:#b8860b">fact_caching&lt;/span> &lt;span style="color:#666">=&lt;/span> jsonfile
&lt;span style="color:#b8860b">fact_caching_connection&lt;/span> &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b8860b">$HOME&lt;/span>/ansible/facts
&lt;span style="color:#b8860b">fact_caching_timeout&lt;/span> &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#666">600&lt;/span>
&lt;span style="color:#b8860b">log_path&lt;/span> &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b8860b">$HOME&lt;/span>/ansible.log
&lt;span style="color:#b8860b">nocows&lt;/span> &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#666">1&lt;/span>
&lt;span style="color:#b8860b">callback_whitelist&lt;/span> &lt;span style="color:#666">=&lt;/span> profile_tasks
&lt;span style="color:#666">[&lt;/span>privilege_escalation&lt;span style="color:#666">]&lt;/span>
&lt;span style="color:#b8860b">become&lt;/span> &lt;span style="color:#666">=&lt;/span> False
&lt;span style="color:#666">[&lt;/span>ssh_connection&lt;span style="color:#666">]&lt;/span>
&lt;span style="color:#b8860b">ssh_args&lt;/span> &lt;span style="color:#666">=&lt;/span> -o &lt;span style="color:#b8860b">ControlMaster&lt;/span>&lt;span style="color:#666">=&lt;/span>auto -o &lt;span style="color:#b8860b">ControlPersist&lt;/span>&lt;span style="color:#666">=&lt;/span>600s
&lt;span style="color:#b8860b">control_path&lt;/span> &lt;span style="color:#666">=&lt;/span> %&lt;span style="color:#666">(&lt;/span>directory&lt;span style="color:#666">)&lt;/span>s/%%h-%%r
&lt;span style="color:#b8860b">pipelining&lt;/span> &lt;span style="color:#666">=&lt;/span> True &lt;span style="color:#080;font-style:italic"># 多路复用，减少了控制机和目标间的连接次数，加速了性能。&lt;/span>
&lt;span style="color:#b8860b">timeout&lt;/span> &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#666">10&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="网络配置">网络配置&lt;/h3>
&lt;p>这里必须要提下，一定要安装前做好网络规划，不然后面改起来很麻烦，&lt;/p>
&lt;p>默认是每个node上最多可跑110个pods，这个要看自身硬件条件，比如说我的环境全是高配物理机，我就改成了，每个节点可以跑1024个pods，这个主要改下面的地方。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#b8860b">openshift_node_kubelet_args&lt;/span>&lt;span style="color:#666">={&lt;/span>&lt;span style="color:#b44">&amp;#39;pods-per-core&amp;#39;&lt;/span>: &lt;span style="color:#666">[&lt;/span>&lt;span style="color:#b44">&amp;#39;0&amp;#39;&lt;/span>&lt;span style="color:#666">]&lt;/span>, &lt;span style="color:#b44">&amp;#39;max-pods&amp;#39;&lt;/span>: &lt;span style="color:#666">[&lt;/span>&lt;span style="color:#b44">&amp;#39;1024&amp;#39;&lt;/span>&lt;span style="color:#666">]&lt;/span>, &lt;span style="color:#b44">&amp;#39;image-gc-high-threshold&amp;#39;&lt;/span>: &lt;span style="color:#666">[&lt;/span>&lt;span style="color:#b44">&amp;#39;90&amp;#39;&lt;/span>&lt;span style="color:#666">]&lt;/span>, &lt;span style="color:#b44">&amp;#39;image-gc-low-threshold&amp;#39;&lt;/span>: &lt;span style="color:#666">[&lt;/span>&lt;span style="color:#b44">&amp;#39;80&amp;#39;&lt;/span>&lt;span style="color:#666">]}&lt;/span>
&lt;span style="color:#b8860b">osm_host_subnet_length&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">10&lt;/span>
&lt;span style="color:#b8860b">osm_cluster_network_cidr&lt;/span>&lt;span style="color:#666">=&lt;/span>12.1.0.0/12
&lt;/code>&lt;/pre>&lt;/div>&lt;p>关于网络的更多优化项，后面有单独介绍。&lt;/p>
&lt;h2 id="主机节点优化">主机节点优化&lt;/h2>
&lt;p>openhshift集群里，除了pod间的网络通信外，最大的开销就是master和etcd间的通信了，openshift的master集成了k8s里的api-server, master主要通过etcd来交互node状态，网络配置，secrets和卷挂载等等信息&lt;/p>
&lt;h3 id="master侧">master侧&lt;/h3>
&lt;p>主要优化点包括：&lt;/p>
&lt;ul>
&lt;li>master和etcd尽量部署在一起.&lt;/li>
&lt;li>高可用集群里，master尽量部署在低延迟的网络里.&lt;/li>
&lt;li>确保**/etc/origin/master/master-config.yaml**里的etcds，第一个是本地的etcd实例.&lt;/li>
&lt;/ul>
&lt;h3 id="node侧">node侧&lt;/h3>
&lt;p>node节点的配置主要在**/etc/origin/node/node-config.yaml**里， 优化点视具体情况定，主要可以优化的点有：&lt;/p>
&lt;ul>
&lt;li>iptables synchronization period,&lt;/li>
&lt;li>MTU值&lt;/li>
&lt;li>代理模式&lt;/li>
&lt;/ul>
&lt;p>配合自文件里还可以配置kubelet的启动参数，主要关注两点&lt;code>pods-per-core 和 max-pods&lt;/code>，这两个决定了node节点的pod数，两者不一致时，&lt;code>取值小的&lt;/code>。如果数值过大（严重超卖）会导致：&lt;/p>
&lt;ul>
&lt;li>增加cpu消耗，主要是docker和openshift自身管理消耗的&lt;/li>
&lt;li>降低pod调度效率&lt;/li>
&lt;li>加大了OOM的风险&lt;/li>
&lt;li>分配pod ip出异常（可能地址池不够了，默认254个ip）&lt;/li>
&lt;li>影响应用的性能&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>有一点要注意，k8s体系的平台，跑一个pod，实际会启动两个容器，一个pause先于业务容器启动，主要负责网络事项，所以跑10个pods，实际上会运行20个容器&lt;/p>
&lt;/blockquote>
&lt;h3 id="etcd节点">etcd节点&lt;/h3>
&lt;p>etcd是一个分布式的key-value存储，所以有条件的话，存储读写性能的提升，上ssd最好了。&lt;/p>
&lt;p>其次是网络的优化，比如和masters部署在一起，或者提供专网连接。&lt;/p>
&lt;blockquote>
&lt;p>etcd实际使用中，最好的提升手段，是关注内存，这个官网有个换算公式的，多少pods推荐多大内存的使用&lt;/p>
&lt;/blockquote>
&lt;h3 id="内核优化">内核优化&lt;/h3>
&lt;p>上面的所有节点，内核层面都需要做些优化，这里推荐使用tuned工具来做，这点属于常规运维优化了，具体可以参考&lt;a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html-single/Performance_Tuning_Guide/index.html#chap-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Tuned">这里&lt;/a>来做， 不想明白原理的，可以如下 快速操作，redhat的人已经自动化了。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">yum install tuned
systemctl start tune
systemctl &lt;span style="color:#a2f">enable&lt;/span> tuned
tuned-adm profile throughput-performance&lt;span style="color:#666">)&lt;/span>来做
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="资源优化">资源优化&lt;/h2>
&lt;h3 id="超卖现象">超卖现象&lt;/h3>
&lt;p>主要是资源管理这块儿的注意点， 我以前有&lt;a href="../openshift%E5%AE%9E%E8%B7%B5-%E6%9D%83%E9%99%90%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86">blog&lt;/a>专门介绍过，主要值得一提的是，这里有个隐形的QoS级别&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-qos.png" alt="">&lt;/p>
&lt;p>Guaranteed类型的pod是优先级最高的，是有保证的，只会在程序本身“异常”超出limits（一般的应用在pod层设置了limits，就不会超过该限制的，除非是java系的，其需要用环境变量来控制），才会被杀掉，其他类型的配额在集群资源紧张时会被kill掉的。&lt;/p>
&lt;p>这块儿更多的细节也可以参考&lt;a href="https://docs.openshift.org/latest/admin_guide/overcommit.html">官文&lt;/a>&lt;/p>
&lt;h3 id="镜像">镜像&lt;/h3>
&lt;p>这里需要注意的是，可以提前把需要的基础镜像先pull到node节点上，比如&lt;code>origin-pod&lt;/code>镜像等，还有其他自定义的&lt;code>Gold 镜像&lt;/code>，这样可以减少应用部署时间。&lt;/p>
&lt;p>如果是采用镜像方式部署集群的话，也可以采取提前pull镜像的方式，当然有私有镜像仓的，可以忽略。&lt;/p>
&lt;p>主要是现在默认的镜像拉取策略就是&lt;code>IfNotPresent&lt;/code>，才能完成加速部署的效果&lt;/p>
&lt;h3 id="线上debug容器">线上debug容器&lt;/h3>
&lt;p>线上容器环境可能很&lt;code>干净&lt;/code>， 如何调试一个线上正在运行的容器，估计困扰过很多开发人员，这个其实利用docker原生特性，可以很easy的做到&lt;/p>
&lt;p>比如你自己build一个工具包镜像tools，里面装有&lt;code>tcpdump，perf，strace&lt;/code>等等debug工具，如下可以很方便的动态的嵌入到运行的线上容器中。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">docker run -t --pid&lt;span style="color:#666">=&lt;/span>container:production &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --net&lt;span style="color:#666">=&lt;/span>container:production &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --cap-add sys_admin &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --cap-add sys_ptrace &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> tools
&lt;/code>&lt;/pre>&lt;/div>&lt;p>当然万能的日志调式，也是OK的。&lt;/p>
&lt;h2 id="存储优化">存储优化&lt;/h2>
&lt;p>这里的存储说的是docker的graph驱动（ Device Mapper, Overlay, 和 Btrfs），首先overlay在启停容器速度方面要优于devicemapper，其还能带来更优良的页面缓存共享，但存在POSIX兼容性问题，比如不支持SELinux。&lt;/p>
&lt;p>官方是推荐使用thin devicemapper的，但需要额外的独立块盘才能搞定。如果系统是7.2的话，使用overlay亦可，关闭selinux的代价就是牺牲部分容器安全。&lt;/p>
&lt;h2 id="路由和网络优化">路由和网络优化&lt;/h2>
&lt;p>openshift里的Router是基于haproxy做的，等价于k8s里的nginx ingress服务，提供集群内的service供外访问能力。&lt;/p>
&lt;p>一般一个4 vCPU/16GB RAM的虚机，可以提供7000-32000 HTTP keep-alive连接请求，这取决于连接是否加密和页面大小，如果是物理机的话，性能会翻倍。&lt;/p>
&lt;p>可通过Router sharding的技术来扩展性能。下图各种配置是统计性能（默认100个routes）&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/haproxy-perf.png" alt="">&lt;/p>
&lt;h3 id="网络优化">网络优化&lt;/h3>
&lt;p>默认openshift提供了一个基于ovs的sdn方案，其中涉及到了vxlan, OpenFlow和iptables，当然这些相关的优化项社区已经有很成熟的优化点和方法了，比如增大MTU，UDP-offload，多路复用等等，&lt;/p>
&lt;p>这里重点说下vxlan， 基于二层网络，vxlan从4096提升到了16百万多个，vxlan就是把原报文封装进UDP报文，以提供所有pods间通信的能力，自然这样会增加cpu解封包的开销，具体网络吞吐取决于cpu的性能，另外还会额外增加延时响应。&lt;/p>
&lt;p>直白的说，现在云主机或物理机的cpu都可以打满千兆网卡，如果是万兆网卡，那vxlan网络的吞吐带宽会卡在CPU上，这是所有overlay网络的现状。&lt;/p>
&lt;p>如果你的主机用用万兆或者40Gbps, 那就要考虑网络的性能优化了：&lt;/p>
&lt;ul>
&lt;li>通过直接路由，负责pod间通信，不过需要手动维护node节点添加删除时的路由变化。&lt;/li>
&lt;li>条件允许的话，可以考虑BGP的calico网络方案&lt;/li>
&lt;li>另外就是购置支持udp-offload的网卡&lt;/li>
&lt;/ul>
&lt;p>值得注意点是，及时使用了udp-offload的网卡，和非overlay网络比，延迟是不会减少的，只是减少了cpu开销，从而提高了带宽吞吐。&lt;/p>
&lt;h3 id="子网大小">子网大小&lt;/h3>
&lt;p>现在openshift-ansible项目默认的安装出来的配置是：&lt;/p>
&lt;ul>
&lt;li>集群里内最多1024个节点&lt;/li>
&lt;li>每个节点最多可以跑510个pods&lt;/li>
&lt;li>支持65,536个service&lt;/li>
&lt;/ul>
&lt;p>比如我要搞一个8192个节点的集群，每个节点允许510个pods运行：&lt;/p>
&lt;pre tabindex="0">&lt;code>[OSE3:vars]
osm_cluster_network_cidr=10.128.0.0/10
&lt;/code>&lt;/pre>&lt;h2 id="监控">监控&lt;/h2>
&lt;p>都知道k8s里的弹性伸缩，依赖于Heapster, 而openshift内置的监控系统又是用的自家的Haw系列，导致监控镜像相当的大&lt;/p>
&lt;p>在opneshift里有两点要提的是&lt;code>METRICS_RESOLUTION&lt;/code>和&lt;code>METRICS_DURATION&lt;/code>变量，前者是默认是30s,指的是监控时间间隔，后者默认是7天，指的是监控数据保留时长（过期就会删掉）。&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/openshift-monitor%E7%A9%BA%E9%97%B4.png" alt="">&lt;/p>
&lt;p>默认的监控体系(Cassandra/Hawkular/Heapster)可以监控25000个pods。&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>其实openshift基于k8s提供了一站式解决方案, 如果公司不具备k8s二次开发能力，openshift足矣。&lt;/p></description></item><item><title>Docs: 网络整理</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E7%BD%91%E7%BB%9C%E6%95%B4%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E7%BD%91%E7%BB%9C%E6%95%B4%E7%90%86/</guid><description>
&lt;p>介绍利用openshift-ansible项目安装后的生产环境里的网络情况。&lt;/p>
&lt;p>待整理。。。&lt;/p></description></item><item><title>Docs: 监控梳理</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E7%9B%91%E6%8E%A7%E6%A2%B3%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E7%9B%91%E6%8E%A7%E6%A2%B3%E7%90%86/</guid><description>
&lt;p>未完搞 ...&lt;/p></description></item><item><title>Docs: 日志分析</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/</guid><description>
&lt;p>未完搞 ...&lt;/p></description></item><item><title>Docs: kubeshere 自研-01</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/kubesphere-01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/kubesphere-01/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>kubesphere 自研环境篇&lt;/p>
&lt;/div>
&lt;h2 id="心态">心态&lt;/h2>
&lt;p>首先调整心态，这是一个新的生态，秉承学习的心态。&lt;/p>
&lt;h2 id="准备环境">准备环境&lt;/h2>
&lt;ul>
&lt;li>clone代码&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">git clone https://github.com/kubesphere/kubesphere.git
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>准备开发环境&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://github.com/kubesphere/community/blob/master/developer-guide/development/quickstart.md">启动快速开发环境&lt;/a>&lt;/p></description></item><item><title>Docs: TKEStack all-in-one入坑指南</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/tkestack-allinone/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/tkestack-allinone/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>本文主要介绍当前最新版本TkeStack 1.8.1 的TKEStack的all-in-one安装、多租户和多集群管理功能解读。&lt;/p>
&lt;/div>
&lt;h2 id="安装实录">安装实录&lt;/h2>
&lt;p>官方推荐至少需要2节点方可安装，配置如下，&lt;strong>硬盘空间&lt;/strong>一定要保障。也支持ALL-in-ONE的方式安装，但有BUG。&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/images/TKEStack-allinone-2022-01-25-08-47-30.png" alt="">&lt;/p>
&lt;h2 id="启动init服务">启动init服务&lt;/h2>
&lt;p>启动init服务，即安装tke-installer和registry服务，安装命令行如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#b8860b">arch&lt;/span>&lt;span style="color:#666">=&lt;/span>amd64 &lt;span style="color:#b8860b">version&lt;/span>&lt;span style="color:#666">=&lt;/span>v1.8.1 &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> wget https://tke-release-1251707795.cos.ap-guangzhou.myqcloud.com/tke-installer-linux-&lt;span style="color:#b8860b">$arch&lt;/span>-&lt;span style="color:#b8860b">$version&lt;/span>.run&lt;span style="color:#666">{&lt;/span>,.sha256&lt;span style="color:#666">}&lt;/span> &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> sha256sum --check --status tke-installer-linux-&lt;span style="color:#b8860b">$arch&lt;/span>-&lt;span style="color:#b8860b">$version&lt;/span>.run.sha256 &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> chmod +x tke-installer-linux-&lt;span style="color:#b8860b">$arch&lt;/span>-&lt;span style="color:#b8860b">$version&lt;/span>.run &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> ./tke-installer-linux-&lt;span style="color:#b8860b">$arch&lt;/span>-&lt;span style="color:#b8860b">$version&lt;/span>.run  
&lt;/code>&lt;/pre>&lt;/div>&lt;p>如上命令执行后，会下载8G左右的安装包，并执行解压后的install.sh脚本，启动3个容器：1个为tke-installer和另2个为registry仓，且为containerd容器，需要使用&lt;code>nerdctl [images | ps]&lt;/code>等命令查看相关信息。&lt;/p>
&lt;p>通过查看脚本，上文启动的本地registry的启动命令等效如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">nerdctl run --name registry-https -d --net&lt;span style="color:#666">=&lt;/span>host --restart&lt;span style="color:#666">=&lt;/span>always -p 443:443 &lt;span style="color:#b62;font-weight:bold">\ &lt;/span> 
-v /opt/tke-installer/registry:/var/lib/registry &lt;span style="color:#b62;font-weight:bold">\ &lt;/span> 
-v registry-certs:/certs &lt;span style="color:#b62;font-weight:bold">\ &lt;/span> 
-e &lt;span style="color:#b8860b">REGISTRY_HTTP_ADDR&lt;/span>&lt;span style="color:#666">=&lt;/span>0.0.0.0:443 &lt;span style="color:#b62;font-weight:bold">\ &lt;/span> 
-e &lt;span style="color:#b8860b">REGISTRY_HTTP_TLS_CERTIFICATE&lt;/span>&lt;span style="color:#666">=&lt;/span>/certs/server.crt &lt;span style="color:#b62;font-weight:bold">\ &lt;/span> 
-e &lt;span style="color:#b8860b">REGISTRY_HTTP_TLS_KEY&lt;/span>&lt;span style="color:#666">=&lt;/span>/certs/server.key  &lt;span style="color:#b62;font-weight:bold">\ &lt;/span> 
tkestack/registry-amd64:2.7.1  
&lt;/code>&lt;/pre>&lt;/div>&lt;p>还有个http 80的registry，这里不贴了，后面的部分坑，就是这里埋下的，预先占用了节点的80和443端口，后面tke的gateway pod会启动失败。&lt;/p>
&lt;h2 id="启动tke集群">启动TKE集群&lt;/h2>
&lt;p>上章节执行完后，会启动tke-installer（一个web操作台），通过访问本地8080端口，可访问界面操作安装global集群。按照官方指引操作就行，此处不表。另外需要说明的是在安装过程中，如果要查看本地容器，不能使用&lt;code>docker ps&lt;/code>了，需要使用&lt;code>nerdctl -n k8s.io ps&lt;/code>。整个安装过程是使用ansible和kubeadm完成的，kubelet是通过systemd启动的，k8s组件为静态pod。&lt;/p>
&lt;p>因为我是使用的ALL-in-ONE安装，遇到了不少问题，可详见FAQ如何解决。安装成功后会提示如下指引：
&lt;img src="https://xiaoping378.github.io/images/TKEStack-allinone-2022-01-25-09-10-56.png" alt="">&lt;/p>
&lt;p>默认初始安装后，很多pod是双副本的，我这里仅是验证功能使用，全部改成了单副本。&lt;/p>
&lt;h2 id="多租户管理">多租户管理&lt;/h2>
&lt;p>tkestack采用&lt;a href="https://xiaoping378.github.io/docs/3-devops/casbin">Casbin框架&lt;/a>实现的权限管理功能，默认集成的Model，查看&lt;a href="https://github.com/tkestack/tke/blob/a024c064880d9180dc8b6d615ffc58b64bb7f903/api/auth/types.go#L633">源码&lt;/a>得知：&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-Conf" data-lang="Conf">[request_definition]
r = sub, dom, obj, act
[policy_definition]
p = sub, dom, obj, act, eft
[role_definition]
g = _, _, _
[policy_effect]
e = some(where (p.eft == allow)) &amp;amp;&amp;amp; !some(where (p.eft == deny))
[matchers]
m = g(r.sub, p.sub, r.dom) &amp;amp;&amp;amp; keyMatchCustom(r.obj, p.obj) &amp;amp;&amp;amp; keyMatchCustom(r.act, p.act)
&lt;/code>&lt;/pre>&lt;p>实现了多租户级的RBAC权限模型。&lt;/p>
&lt;h2 id="faq">FAQ&lt;/h2>
&lt;h3 id="安装过程出现循环等待apiserver启动">安装过程出现循环等待apiserver启动&lt;/h3>
&lt;pre tabindex="0">&lt;code class="language-log" data-lang="log">2022-01-19 14:43:32.225 error   tke-installer.ClusterProvider.OnCreate.EnsureKubeadmInitPhaseWaitControlPlane   check healthz error {&amp;quot;statusCode&amp;quot;: 0, &amp;quot;error&amp;quot;: &amp;quot;Get \&amp;quot;https://****:6443/healthz?timeout=30s\&amp;quot;: net/http: TLS handshake timeout&amp;quot;}
&lt;/code>&lt;/pre>&lt;p>我这里是因为在installer上指定的master的IP为外网IP（我使用外网IP是有原因的，穷... 后面需要跨云厂商组集群），通过查看kubelet日志提示本机找不到IP，如下开启网卡多IP，可通过。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">ip addr add 118.*.*.* dev eth0
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="gateway-pod启动失败">Gateway POD启动失败&lt;/h3>
&lt;p>我这里是因为init节点和gobal master节点，共用了一个，本registry服务占用了80和443端口，需要修改gateway hostNetwork为false，另外可以通过修改svc 为nodePort，还需要修改targetPort，官方现在这里有bug，不知道为指到944*的端口上，我这里设置的30080来访问安装好的集群。&lt;/p>
&lt;h3 id="页面登录错误unregistered-redirect-uri">页面登录错误Unregistered redirect_uri&lt;/h3>
&lt;p>官方没有相关说明，一切都是ALL-in-ONE的原因，我改动了默认集群console的访问端口为30080。。。 通过查看源码发现是每次认证时dex会校验tke-auth-api向它注册过的合法client地址。于是我就修改了tke命名空间下tke-auth-api的相关configmap：&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/images/TKEStack-allinone-2022-01-25-09-47-16.png" alt="">&lt;/p>
&lt;p>重启tke-auth-api后，问题依旧存在，继续源码走查，发现这玩意儿叫init真的只发挥一次作用，改完配置，不会重新读取，细读逻辑发现etcd中不存在这个key，会重新读取写入一次，于是决定删除etcd中的相关key。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">etcdctl --cacert&lt;span style="color:#666">=&lt;/span>/etc/kubernetes/pki/etcd/ca.crt --cert&lt;span style="color:#666">=&lt;/span>/etc/kubernetes/pki/apiserver-etcd-client.crt --key&lt;span style="color:#666">=&lt;/span>/etc/kubernetes/pki/apiserver-etcd-client.key del /tke/auth-api/client/default  --prefix
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="添加节点的过程中failed-无法删除节点重试">添加节点的过程中failed，无法删除节点重试&lt;/h3>
&lt;p>ssh信息设置完后，如果中间出问题，会陷入无限重试...&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/images/TKEStack-allinone-2022-01-25-09-48-43.png" alt="">&lt;/p>
&lt;p>遇事不决，看日志，找不到日志，看源码...&lt;/p>
&lt;p>通过翻找源码，发现是&lt;code>platform&lt;/code>相关组件在负责，查看相关日志&lt;code>kubectl -n logs tke-platform-controller-*** --tail 100 -f&lt;/code>，定位问题，我这里是以前各种安装的残留信息，导致添加节点初始化失败。删除之... 解决。&lt;/p>
&lt;p>为避免添加节点&lt;code>no clean&lt;/code>再次出现问题，建议预先执行下&lt;a href="https://tke-release-1251707795.cos.ap-guangzhou.myqcloud.com/tools/clean.sh">clean.sh&lt;/a>脚本。&lt;/p>
&lt;h2 id="小技巧">小技巧&lt;/h2>
&lt;p>如下使用，可以愉快的敲命令了，因为我是用oh-my-zsh的shell主题(没有自动加载kubectl plugin)，kubectl的命令补全使用zsh，可根据实际情况调整。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#a2f">source&lt;/span> &amp;lt;&lt;span style="color:#666">(&lt;/span>nerdctl completion bash&lt;span style="color:#666">)&lt;/span>  
&lt;span style="color:#a2f">source&lt;/span> &amp;lt;&lt;span style="color:#666">(&lt;/span>kubectl completion zsh&lt;span style="color:#666">)&lt;/span> 
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: 使用kind本地启动多集群</title><link>https://xiaoping378.github.io/docs/4-cloud/kubesphere/kind-multicluster-dev/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/kubesphere/kind-multicluster-dev/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>我本地4c/8G的小本儿，跑了两个集群，组建了多集群环境，还行，能玩动...&lt;/p>
&lt;/div>
&lt;h2 id="环境篇">环境篇&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>kind&lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start/">安装&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>镜像准备&lt;/p>
&lt;p>视网络情况，可以把依赖镜像&lt;code>kindest/node&lt;/code>提起pull到本地&lt;/p>
&lt;/li>
&lt;li>
&lt;p>docker的data-root目录&lt;/p>
&lt;p>尽量不要放到/var目录下，kind起的集群容器会占用比较大的空间&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="实操">实操&lt;/h2>
&lt;ol>
&lt;li>创建集群&lt;/li>
&lt;/ol>
&lt;p>执行完如下命令后，docker ps可以看到本地启动了两个容器，一个容器对应一个集群。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kind create cluster --image kindest/node:v1.19.16 --name host
kind create cluster --image kindest/node:v1.19.16 --name member
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>kubectl config use-context [kind-host | kind-member]&lt;/code>，可以切换kubecl执行的上下文&lt;/p>
&lt;ol start="2">
&lt;li>安装kubesphere&lt;/li>
&lt;/ol>
&lt;p>分别在两个集群各自安装ks组件&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># 集群1安装&lt;/span>
kubectl config use-context kind-host
kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/kubesphere-installer.yaml
kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/cluster-configuration.yaml
&lt;span style="color:#080;font-style:italic"># 集群2安装&lt;/span>
kubectl config use-context kind-member
kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/kubesphere-installer.yaml
kubectl apply -f https://github.com/kubesphere/ks-installer/releases/download/v3.2.1/cluster-configuration.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>纳管集群&lt;/li>
&lt;/ol>
&lt;p>可以在上面的初始化阶段直接改好主和成员集群的关系，这里参考&lt;a href="https://kubesphere.com.cn/docs/multicluster-management/enable-multicluster/direct-connection/">官文&lt;/a>即可&lt;/p>
&lt;p>host集群的UI地址，可以通过&lt;code>host容器IP:30880&lt;/code>来访问，主集群的容器ip，可以如下获取：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">docker inspect --format &lt;span style="color:#b44">&amp;#39;{{ .NetworkSettings.Networks.kind.IPAddress }}&amp;#39;&lt;/span> host-control-plane
&lt;/code>&lt;/pre>&lt;/div>&lt;p>实操&lt;code>添加&lt;/code>集群时，需要member集群的kubeconfig，可以用如下命令获取到&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kind get kubeconfig --name member
&lt;/code>&lt;/pre>&lt;/div>&lt;p>记得把kubeconfig中的&lt;code>server&lt;/code>地址中改成&lt;code>member容器ip:6443&lt;/code>，这样host集群才能访问到member集群&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>验证功能、测试开发，挺方便的，可以视本地资源紧张情况停掉监控的ns。&lt;/p>
&lt;p>现在kind启动的集群默认使用了containerd的runtime，若想进一步调试查看集群内的情况，可以内部集成的&lt;code>crictl&lt;/code>代替熟悉的docker工具。&lt;/p></description></item><item><title>Docs: 论Kubesphere的异地多活方案</title><link>https://xiaoping378.github.io/docs/4-cloud/kubesphere/multihosts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/kubesphere/multihosts/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>遇到这样一个场景，在同一套环境中需要存在多个host控制面集群...bulabula... 因此想探索下kubesphere的异地多活混合容器云管理方案&lt;/p>
&lt;/div>
&lt;h2 id="集群角色介绍">集群角色介绍&lt;/h2>
&lt;p>一个兼容原生的k8s集群，可通过&lt;code>ks-installer&lt;/code>来初始化完成安装，成为一个QKE集群。QKE集群分为多种角色，默认是none角色（standalone模式），开启多集群功能时，可以设置为host或者member角色。&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/images/kcp-%E5%A4%9A%E9%9B%86%E7%BE%A4.png" alt="多集群">&lt;/p>
&lt;ul>
&lt;li>none角色，是最小化安装的默认模式，会安装必要的ks-apiserver, ks-controller-manager, ks-console和其他组件
&lt;ul>
&lt;li>ks-apiserver, kcp的API网关，包含审计、认证、权限校验等功能&lt;/li>
&lt;li>ks-controller, 各类自定义crd的控制器和平台管理逻辑的实现&lt;/li>
&lt;li>ks-console, 前端界面UI&lt;/li>
&lt;li>ks-installer, 初始化安装和变更QKE集群的工具，由shell-operator触发ansible-playbooks来工作。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>member角色，承载工作负载的业务集群，和none模式的组件安装情况一致&lt;/li>
&lt;li>host角色，整个混合云管理平台的控制面，会在none的基础上，再额外安装tower，kubefed-controller-manager， kubefed-admission-webhook等组件
&lt;ul>
&lt;li>tower，代理业务集群通信的server端，常用于不能直连member集群api-server的情况&lt;/li>
&lt;li>kubefed-controller-manager，社区的&lt;a href="https://github.com/kubernetes-sigs/kubefed">kubefed&lt;/a>联邦资源的控制器&lt;/li>
&lt;li>kubefed-admission-webhook， 社区的kubefed联邦资源的动态准入校验器&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="多集群管理原理">多集群管理原理&lt;/h2>
&lt;p>上段提到QKE有3种角色，可通过修改&lt;code>cc&lt;/code>配置文件的&lt;code>clusterRole&lt;/code>来使能, ks-installer监听到配置变化的事件，会初始化对应集群角色的功能。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl edit cc ks-installer -n kubesphere-system
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>角色不要改来改去，会出现莫名问题，主要是背后ansible维护的逻辑有疏漏，没闭环&lt;/p>
&lt;/blockquote>
&lt;h3 id="host集群">host集群&lt;/h3>
&lt;p>host角色的主集群会被创建25种联邦资源类型Kind，如下命令可查看，还会额外安装kubefed stack组件。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ kubectl get FederatedTypeConfig -A
&lt;/code>&lt;/pre>&lt;/div>&lt;p>此外api-server被重启后，会根据配置内容的变化，做两件事，注册多集群相关的路由和缓存同步部分联邦资源。&lt;/p>
&lt;ul>
&lt;li>添加url里包含&lt;code>clusters/{cluster}&lt;/code>路径的agent路由和转发的功能，要访问业务集群的信息，这样可以直接转发过去。&lt;/li>
&lt;li>cacheSync，缓存同步联邦资源，这里是个同步的操作。&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>当开启多集群后，如果某个member出现异常导致不可通信，那host的api-server此时遇到故障要重启，会卡在cacheSync这一步，导致无法启动，进而整个平台无法访问。&lt;/p>
&lt;/blockquote>
&lt;p>controller-manager被重启后，同样会根据配置的变化，把部分资源类型自动转化成联邦资源的逻辑，也就是说，在host集群创建的这部分资源会自动同步到所有成员集群，实际的多集群同步靠kubefed-controller-manager来执行。以下资源会被自动创建联邦资源下发：&lt;/p>
&lt;ul>
&lt;li>users.iam.kubesphere.io -&amp;gt; federatedusers.types.kubefed.io&lt;/li>
&lt;li>workspacetemplates.tenant.kubesphere.io -&amp;gt; federatedworkspaces.types.kubefed.io&lt;/li>
&lt;li>workspaceroles.iam.kubesphere.io -&amp;gt; federatedworkspaceroles.types.kubefed.io&lt;/li>
&lt;li>workspacerolebindings.iam.kubesphere.io -&amp;gt; federatedworkspacerolebindings.types.kubefed.io&lt;/li>
&lt;/ul>
&lt;p>此外还会启动cluster、group和一些globalRole*相关资源的控制器逻辑，同上也会通过kubefed自动下发到所有集群，&lt;code>clusters.cluster.kubesphere.io&lt;/code>资源除外。&lt;/p>
&lt;blockquote>
&lt;p>如果以上资源包含了&lt;code>kubefed.io/managed: false&lt;/code>标签，kubefed就不会再做下发同步，而host集群下发完以上资源后，都会自动加上该标签，防止进入死循环&lt;/p>
&lt;/blockquote>
&lt;h3 id="member集群">member集群&lt;/h3>
&lt;p>修改为member集群时，需要cc中的&lt;strong>jwtSecret&lt;/strong>与host集群的保持一致(若该值为空的话，ks-installer默认会随机生成)，提取host集群的该值时，需要去cm里找，如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl -n kubesphere-system get cm kubesphere-config -o yaml | grep -v &lt;span style="color:#b44">&amp;#34;apiVersion&amp;#34;&lt;/span> | grep jwtSecret
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>jwtSecret要保持一致，主要是为了在host集群&lt;strong>签发&lt;/strong>的用户token，在用户访问业务集群时token&lt;strong>校验&lt;/strong>能通过。&lt;/p>
&lt;/blockquote>
&lt;h3 id="添加集群">添加集群&lt;/h3>
&lt;p>本文只关注&lt;code>直接连接&lt;/code>这种情况，当填好成员集群的kubeconfig信息，点击&lt;code>添加&lt;/code>集群后,会做如下校验：&lt;/p>
&lt;ul>
&lt;li>通过kubeconfig信息先校验下是否会添加已存在的重复集群&lt;/li>
&lt;li>校验成员集群的网络连通性&lt;/li>
&lt;li>校验成员集群是否安装了ks-apiserver&lt;/li>
&lt;li>校验成员集群的&lt;code>jwtSecret&lt;/code>是否和主集群的一致&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>写稿时，此处有个问题，需要修复，如果kubeconfig使用了&lt;code>insecure-skip-tls-verify: true&lt;/code>会导致该集群添加失败，经定位主要是kubefed 空指针panic了，后续有时间我会去fix一下，已提&lt;a href="https://github.com/kubesphere/kubesphere/issues/4891">issue&lt;/a>。&lt;/p>
&lt;/blockquote>
&lt;p>校验完必要信息后，就执行实质动作&lt;code>joinFederation&lt;/code>加入联邦，kubesphere多集群纳管，实质上是先组成联邦集群:&lt;/p>
&lt;ul>
&lt;li>在成员集群创建ns kube-federation-system&lt;/li>
&lt;li>在上面的命名空间中创建serviceAccount [clusterName]-kubesphere, 并绑定最高权限&lt;/li>
&lt;li>在主集群的kube-federation-system的命名空间创建&lt;code>kubefedclusters.core.kubefed.io&lt;/code>，由kubefed stack驱动联邦的建立&lt;/li>
&lt;li>加入联邦后，主机群的联邦资源会通过kubefed stack同步过来&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>上述一顿操作，等效于 &lt;code>kubefedctl join member-cluster --cluster-context member-cluster --host-cluster-context host-cluster&lt;/code>&lt;/p>
&lt;/blockquote>
&lt;h2 id="异地多活方案设计">异地多活方案设计&lt;/h2>
&lt;p>异地多活的方案主要是多个主集群能同时存在，且保证数据双向同步，经过上面的原理分析，可知多个主集群是可以同时存在的，也就是一个成员集群可以和多个主集群组成联邦。整体方案示意图设计如下：&lt;/p>
&lt;p>&lt;img src="https://xiaoping378.github.io/images/kcp-multi-hostclusters.png" alt="">&lt;/p>
&lt;blockquote>
&lt;p>以下操作假设本地已具备三个QKE集群，如果不具备的可按照&lt;a href="https://xiaoping378.github.io/docs/4-cloud/kubesphere/kind-multicluster-dev/">此处&lt;/a>快速搭建&lt;code>host、host2、member&lt;/code>3个集群&lt;/p>
&lt;/blockquote>
&lt;p>大致实现逻辑的前提介绍：&lt;/p>
&lt;ol>
&lt;li>三个集群的&lt;code>jwtSecret&lt;/code>得保持一致&lt;/li>
&lt;li>两个主集群都去&lt;code>添加&lt;/code>纳管同一个member集群&lt;/li>
&lt;li>利用&lt;code>etcdctl make-mirror&lt;/code>实现双向同步&lt;/li>
&lt;/ol>
&lt;h3 id="验证下可行性">验证下可行性&lt;/h3>
&lt;p>实操双活前，先验证下可行性&lt;/p>
&lt;p>&lt;strong>实验1：&lt;/strong>&lt;/p>
&lt;p>在两边创建一个同名用户，用户所有信息一致，可以添加成功，然后再修改一边的用户信息，使两边不一致&lt;/p>
&lt;p>可以看到member集群的用户xxp，一直会被两边不断的更新...&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">root@member-control-plane:/# kubectl get user xxp -w
NAME EMAIL STATUS
xxp xxp@163.com Active
xxp xxp-2@163.com Active
xxp xxp@163.com Active
xxp xxp-2@163.com Active
... 周而复始 ...
&lt;/code>&lt;/pre>&lt;/div>&lt;p>这个实验，即使在创建用户时，页面表单上两边信息填的都一样，也会出现互相刷新覆盖的情况，因为yaml里的uid和time信息不一致&lt;/p>
&lt;p>&lt;strong>实验2：&lt;/strong>&lt;/p>
&lt;p>在两边添加一个同名用户，但两边用户信息（用户角色）不一致，可以创建成功，但后创建者的kube-federa会同步失败, 到这里还能接受，毕竟有冲突直接就同步失败了&lt;/p>
&lt;p>但member集群上该用户的关联角色会出现上文的情况，被两边的主集群持续反复地修改...&lt;/p>
&lt;p>&lt;strong>实验3：&lt;/strong>&lt;/p>
&lt;p>在一侧的主集群上尝试修复冲突资源，即删除有冲突的用户资源，可以删除成功，但对应的联邦资源会出现删失败的情况&lt;/p>
&lt;pre tabindex="0">&lt;code>➜ ~ kubectl get users.iam.kubesphere.io
NAME EMAIL STATUS
admin admin@kubesphere.io Active
xxp3 xxp3@163.com Active
➜ ~
➜ ~ kubectl get federatedusers.types.kubefed.io
NAME AGE
admin 5h33m
xxp 65m #这里是个删不掉的资源，fed controller会重复做失败尝试
xxp3 61m
&lt;/code>&lt;/pre>&lt;p>这样就会出现，两个主集群：一个要删，一个要同步，member集群上：持续上演“一会儿消失，一会儿又出现了”的奇观。&lt;/p>
&lt;h3 id="总结">总结&lt;/h3>
&lt;p>两个主集群可以同时工作，一旦出现同名冲突资源，处理起来会非常麻烦，尤其是背后的Dependent附属资源出现冲突时，往往问题点隐藏的更深，修复起来也棘手...&lt;/p>
&lt;p>后来调研也发现：目前的社区方案make-mirror只支持单向同步，适合用来做灾备方案。&lt;/p>
&lt;p>所以容器云平台的双活，除非具备跨AZ的etcd集群，否则需要二次开发改造类make-mirror方案来支持了。我最开始要考虑的问题答案也就显而易见了：如果要多个host集群共存，必须考虑通过行政管理手段，来尽量避免同名资源冲突。&lt;/p></description></item><item><title>Docs: 搭建kubesphere的开发调试环境</title><link>https://xiaoping378.github.io/docs/4-cloud/kubesphere/kcp-dev/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/kubesphere/kcp-dev/</guid><description>
&lt;h2 id="依赖工具介绍">依赖工具介绍&lt;/h2>
&lt;ol>
&lt;li>vscode，个人日常使用vscode开发，标配&lt;code>Remote Development&lt;/code>插件&lt;/li>
&lt;li>kt-connect，本地开发使用的流量代理工具，可以双向代理，本地可以直接访问pod和svc，也可以转发访问pod的流量到本地，相关&lt;a href="https://github.com/alibaba/kt-connect/blob/master/docs/zh-cn/reference/mechanism.md">原理介绍&lt;/a>。&lt;/li>
&lt;/ol>
&lt;h2 id="准备调试环境">准备调试环境&lt;/h2>
&lt;h3 id="连接集群网络">连接集群网络&lt;/h3>
&lt;p>使用kt-connect，打通网络，本地可直接访问Kubernetes集群内网&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">sudo ktctl connect --context kind-host --portForwardTimeout &lt;span style="color:#666">300&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;code>ktctl&lt;/code>采用本地&lt;code>kubectl&lt;/code>工具的集群配置，默认为~/.kube/config文件中配置的集群。&lt;/li>
&lt;li>如果kubeconfig有多个集群，可以通过&lt;code>--context&lt;/code>指定要连接的具体集群&lt;/li>
&lt;li>如果本地环境是&lt;code>kind&lt;/code>集群，需要修改kubeconfig中server的127地址为&lt;code>容器IP:6443&lt;/code>&lt;/li>
&lt;li>如果网络比较差，会遇到错误&lt;code>ERR Exit: pod kt-rectifier-tcxjk failed to start&lt;/code>，可适当增加等待时间&lt;code>portForwardTimeout&lt;/code>&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>另外注意的是，kt-connect需要root权限，上条命令会默认读取&lt;code>/root/.kube/config&lt;/code>文件，自行copy或者另通过&lt;code>-c&lt;/code>指定文件&lt;/p>
&lt;/blockquote>
&lt;h3 id="clone代码">clone代码&lt;/h3>
&lt;p>不表.&lt;/p>
&lt;h3 id="编辑调试的配置文件">编辑调试的配置文件&lt;/h3>
&lt;p>首先编辑vscode的调试配置文件, 我是如下配置的：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ kubesphere git:&lt;span style="color:#666">(&lt;/span>master&lt;span style="color:#666">)&lt;/span> cat .vscode/launch.json
&lt;span style="color:#666">{&lt;/span>
// Use IntelliSense to learn about possible attributes.
// Hover to view descriptions of existing attributes.
// For more information, visit: https://go.microsoft.com/fwlink/?linkid&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">830387&lt;/span>
&lt;span style="color:#b44">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;0.2.0&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;configurations&amp;#34;&lt;/span>: &lt;span style="color:#666">[&lt;/span>
&lt;span style="color:#666">{&lt;/span>
&lt;span style="color:#b44">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ks-apiserver&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;go&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;request&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;launch&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;mode&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;program&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b68;font-weight:bold">${&lt;/span>&lt;span style="color:#b8860b">workspaceFolder&lt;/span>&lt;span style="color:#b68;font-weight:bold">}&lt;/span>&lt;span style="color:#b44">/cmd/ks-apiserver/apiserver.go&amp;#34;&lt;/span>
&lt;span style="color:#666">}&lt;/span>,
&lt;span style="color:#666">{&lt;/span>
&lt;span style="color:#b44">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;controller-manager&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;go&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;request&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;launch&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;mode&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;auto&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;program&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b68;font-weight:bold">${&lt;/span>&lt;span style="color:#b8860b">workspaceFolder&lt;/span>&lt;span style="color:#b68;font-weight:bold">}&lt;/span>&lt;span style="color:#b44">/cmd/controller-manager/controller-manager.go&amp;#34;&lt;/span>
&lt;span style="color:#666">}&lt;/span>
&lt;span style="color:#666">]&lt;/span>
&lt;span style="color:#666">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="准备调试环境-1">准备调试环境&lt;/h3>
&lt;p>按F5，启动调试，左上角选择要调试的组件，我这里以controller-manager举例（需要hack的注意点比较多）。&lt;/p>
&lt;p>过会儿会发现出现错误，错误提示很明显，因缺少配置文件，导致无法启动，通过查看&lt;code>deployment yaml&lt;/code>, 发现后面还会缺失&lt;code>Admission Webhooks&lt;/code>的证书，可如下统一提取到本地：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># 提取启动的配置文件(调试apiserver的时候也需要这一步，但要把文件放到对应cmd/ks-apiserver目录下)&lt;/span>
kubectl -n kubesphere-system get cm kubesphere-config -ojsonpath&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;{.data.kubesphere\.yaml}&amp;#39;&lt;/span> &amp;gt; cmd/controller-manager/kubesphere.yaml
&lt;span style="color:#080;font-style:italic"># 提取webhook用到的证书&lt;/span>
mkdir -p /tmp/k8s-webhook-server/serving-certs/
&lt;span style="color:#a2f">export&lt;/span> &lt;span style="color:#b8860b">controller_pod&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">`&lt;/span>kubectl -n kubesphere-system get pods -l &lt;span style="color:#b8860b">app&lt;/span>&lt;span style="color:#666">=&lt;/span>ks-controller-manager -o &lt;span style="color:#b8860b">jsonpath&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;{.items[0].metadata.name}&amp;#39;&lt;/span>&lt;span style="color:#b44">`&lt;/span>
kubectl -n kubesphere-system &lt;span style="color:#a2f">exec&lt;/span> -it &lt;span style="color:#b68;font-weight:bold">${&lt;/span>&lt;span style="color:#b8860b">controller_pod&lt;/span>&lt;span style="color:#b68;font-weight:bold">}&lt;/span> -- cat /tmp/k8s-webhook-server/serving-certs/ca.crt &amp;gt; /tmp/k8s-webhook-server/serving-certs/ca.crt
kubectl -n kubesphere-system &lt;span style="color:#a2f">exec&lt;/span> -it &lt;span style="color:#b68;font-weight:bold">${&lt;/span>&lt;span style="color:#b8860b">controller_pod&lt;/span>&lt;span style="color:#b68;font-weight:bold">}&lt;/span> -- cat /tmp/k8s-webhook-server/serving-certs/tls.crt &amp;gt; /tmp/k8s-webhook-server/serving-certs/tls.crt
kubectl -n kubesphere-system &lt;span style="color:#a2f">exec&lt;/span> -it &lt;span style="color:#b68;font-weight:bold">${&lt;/span>&lt;span style="color:#b8860b">controller_pod&lt;/span>&lt;span style="color:#b68;font-weight:bold">}&lt;/span> -- cat /tmp/k8s-webhook-server/serving-certs/tls.key &amp;gt; /tmp/k8s-webhook-server/serving-certs/tls.key
&lt;/code>&lt;/pre>&lt;/div>&lt;p>继续启动，发现还会有缺文件的错误，应该是编译镜像时，内置了些文件，通过查看&lt;code>build/ks-controller-manager/Dockerfile&lt;/code>，发现后面会缺的东西还是比较多的，推荐直接从运行中的pod直接copy到本地：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">sudo mkdir /var/helm-charts/
sudo chmod -R a+rw /var/helm-charts
kubectl -n kubesphere-system cp &lt;span style="color:#b68;font-weight:bold">${&lt;/span>&lt;span style="color:#b8860b">controller_pod&lt;/span>&lt;span style="color:#b68;font-weight:bold">}&lt;/span>:/var/helm-charts /var/helm-charts/
&lt;/code>&lt;/pre>&lt;/div>&lt;p>继续启动，成功 ！&lt;/p>
&lt;h2 id="开始调试">开始调试&lt;/h2>
&lt;p>利用ktctl替换集群中的ks-controller-manager的服务为本地服务。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">sudo ktctl exchange ks-controller-manager --namespace kubesphere-system --mode scale --recoverWaitTime &lt;span style="color:#666">300&lt;/span> --expose 8443:8443
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>如果本地集群只有一个节点，上述命令会一直pending，可以通过如下命令替代&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">sudo ktctl exchange ks-controller-manager --namespace kubesphere-system --expose 8443:8443
kubectl -n kubesphere-system scale deployment ks-controller-manager --replicas&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">0&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>结束调试，记得还原刚才缩容&lt;code>replicas&lt;/code>的设置。&lt;/p>
&lt;/blockquote>
&lt;p>后续就是vscode的正常断点调试或者本地开发验证了，有时间在整理贴图...&lt;/p></description></item></channel></rss>