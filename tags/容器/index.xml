<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>容器 on 现代技能栈</title><link>https://xiaoping378.github.io/tags/%E5%AE%B9%E5%99%A8/</link><description>Recent content in 容器 on 现代技能栈</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://xiaoping378.github.io/tags/%E5%AE%B9%E5%99%A8/index.xml" rel="self" type="application/rss+xml"/><item><title>百宝箱脚本</title><link>https://xiaoping378.github.io/docs/4-cloud/containers/%E7%99%BE%E5%AE%9D%E7%AE%B1%E8%84%9A%E6%9C%AC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/containers/%E7%99%BE%E5%AE%9D%E7%AE%B1%E8%84%9A%E6%9C%AC/</guid><description>&lt;ul&gt;
&lt;li&gt;备份所有docker镜像&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;mkdir -p images &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style="color:#a2f"&gt;cd&lt;/span&gt; images
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#a2f;font-weight:bold"&gt;for&lt;/span&gt; image in &lt;span style="color:#b44"&gt;`&lt;/span&gt;docker images | grep -v REPOSITORY | awk &lt;span style="color:#b44"&gt;&amp;#39;{print $1&amp;#34;:&amp;#34;$2}&amp;#39;&lt;/span&gt;&lt;span style="color:#b44"&gt;`&lt;/span&gt;; &lt;span style="color:#a2f;font-weight:bold"&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#a2f"&gt;echo&lt;/span&gt; &lt;span style="color:#b44"&gt;&amp;#34;saving the image of &lt;/span&gt;&lt;span style="color:#b68;font-weight:bold"&gt;${&lt;/span&gt;&lt;span style="color:#b8860b"&gt;image&lt;/span&gt;&lt;span style="color:#b68;font-weight:bold"&gt;}&lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; docker save &lt;span style="color:#b68;font-weight:bold"&gt;${&lt;/span&gt;&lt;span style="color:#b8860b"&gt;image&lt;/span&gt;&lt;span style="color:#b68;font-weight:bold"&gt;}&lt;/span&gt; &amp;gt; &lt;span style="color:#b68;font-weight:bold"&gt;${&lt;/span&gt;&lt;span style="color:#b8860b"&gt;image&lt;/span&gt;////-&lt;span style="color:#b68;font-weight:bold"&gt;}&lt;/span&gt;.tar
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#a2f"&gt;echo&lt;/span&gt; -e &lt;span style="color:#b44"&gt;&amp;#34;finished saving the image of \033[32m &lt;/span&gt;&lt;span style="color:#b68;font-weight:bold"&gt;${&lt;/span&gt;&lt;span style="color:#b8860b"&gt;image&lt;/span&gt;&lt;span style="color:#b68;font-weight:bold"&gt;}&lt;/span&gt;&lt;span style="color:#b44"&gt; \033[0m&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#a2f;font-weight:bold"&gt;done&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;批量加载本地tar镜像&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#a2f;font-weight:bold"&gt;for&lt;/span&gt; image in &lt;span style="color:#b44"&gt;`&lt;/span&gt;ls *.tar&lt;span style="color:#b44"&gt;`&lt;/span&gt;; &lt;span style="color:#a2f;font-weight:bold"&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#a2f"&gt;echo&lt;/span&gt; &lt;span style="color:#b44"&gt;&amp;#34;loading the image of &lt;/span&gt;&lt;span style="color:#b68;font-weight:bold"&gt;${&lt;/span&gt;&lt;span style="color:#b8860b"&gt;image&lt;/span&gt;&lt;span style="color:#b68;font-weight:bold"&gt;}&lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; docker load &amp;lt; &lt;span style="color:#b68;font-weight:bold"&gt;${&lt;/span&gt;&lt;span style="color:#b8860b"&gt;image&lt;/span&gt;&lt;span style="color:#b68;font-weight:bold"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; &lt;span style="color:#a2f"&gt;echo&lt;/span&gt; -e &lt;span style="color:#b44"&gt;&amp;#34;finished loading the image of \033[32m &lt;/span&gt;&lt;span style="color:#b68;font-weight:bold"&gt;${&lt;/span&gt;&lt;span style="color:#b8860b"&gt;image&lt;/span&gt;&lt;span style="color:#b68;font-weight:bold"&gt;}&lt;/span&gt;&lt;span style="color:#b44"&gt; \033[0m&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#a2f;font-weight:bold"&gt;done&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;批量杀死僵尸进程&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;ps -A -o stat,ppid,pid,cmd | grep -e &lt;span style="color:#b44"&gt;&amp;#39;^[Zz]&amp;#39;&lt;/span&gt; | awk &lt;span style="color:#b44"&gt;&amp;#39;{print $2}&amp;#39;&lt;/span&gt; | xargs &lt;span style="color:#a2f"&gt;kill&lt;/span&gt; -9
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description></item><item><title>构建生产环境级的docker Swarm集群-1</title><link>https://xiaoping378.github.io/docs/4-cloud/containers/swarm/docker-swarm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/containers/swarm/docker-swarm/</guid><description>&lt;p&gt;此文档适用于低于1.12版本的docker，之后swarm已内置于docker-engine里。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;硬件需求&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;至少5台PC服务器, 分别如下作用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;manager0&lt;/li&gt;
&lt;li&gt;manager1&lt;/li&gt;
&lt;li&gt;consul0&lt;/li&gt;
&lt;li&gt;node0&lt;/li&gt;
&lt;li&gt;node1&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;每台PC上安装docker-engine&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一台一台的ssh上去执行，或者使用ansible批量部署工具。&lt;/p&gt;
&lt;p&gt;安装docker-engine&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;curl -sSL https://get.docker.com/ | sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动之，并使之监听2375端口&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;sudo docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;亦可修改配置，使之永久生效&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;mkdir /etc/systemd/system/docker.service.d
cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt;/etc/systemd/system/docker.service.d/docker.conf
[Service]
 ExecStart=
 ExecStart=/usr/bin/docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --dns 180.76.76.76 --insecure-registry registry.cecf.com -g /home/Docker/docker
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;ol start="3"&gt;
&lt;li&gt;启动discovery后台&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在consul0上启动consul服务，manager用其来认证node连接并存储node状态， 理应建立discovery的高可用，这里简化之&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;docker run -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap
&lt;/code&gt;&lt;/pre&gt;&lt;ol start="4"&gt;
&lt;li&gt;创建Swarm集群&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在manager0上创建the primary manager， 自行替换manager0_ip和consul0_ip的真实IP地址。&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;docker run -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &amp;lt;manager0_ip&amp;gt;:4000 consul://&amp;lt;consul0_ip&amp;gt;:8500
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在manager1上启动replica manger&lt;/p&gt;</description></item><item><title>快速安装</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85/</guid><description>&lt;p&gt;不知道为什么openshift在国内热度这么低，那些要做自己容器云的公司，不知道有openshift项目的存在么？完全满足我的需求。&lt;/p&gt;
&lt;p&gt;docker负责应用的隔离打包，k8s提供集群管理和容器的编排服务，而openshfit则负责整个应用的生命周期：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;源码管理，CI&amp;amp;CD能力&lt;/li&gt;
&lt;li&gt;多租户管理, 支持LDAP和Oauth&lt;/li&gt;
&lt;li&gt;集成监控日志于web console&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先说下自接触到openshift项目就遇到的一个困惑，就是openshift origin/enterprise /online/dedicated/ocp之间的关系： &lt;code&gt;orgin相当于Fedora， 其他的相当于RHEL&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;接下来谈下我用自己的笔记本实践的过程与感受：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;快速安装&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本人日常基于ubuntu16.04办公，所以用oc直接上, oc相当于kubectl&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/openshift/origin/releases"&gt;这里&lt;/a&gt;直接下载oc客户端，或者自行编译, 编译结果在_output目录下&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;git clone --depth=1 https://github.com/openshift/origin.git
cd origin &amp;amp;&amp;amp; make
mv _output/local/bin/linux/amd64/oc /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动openshift, 默认开启监控并初始安装自最新版本，当前是v1.5.0-alpha.2&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;oc cluster up --metrics=true --version=latest --insecure-skip-tls-verify=true --public-hostname=air13
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;过程中会拉取所需镜像, 我这里显示比较多，之前已经做了些实验&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;➜ ~ docker images | grep openshift | awk &amp;#39;{print $1}&amp;#39;
openshift/node
openshift/origin-sti-builder
openshift/origin-docker-builder
openshift/origin-deployer
openshift/origin-gitserver
openshift/origin-docker-registry
openshift/origin-haproxy-router
openshift/origin
openshift/hello-openshift
openshift/openvswitch
openshift/origin-pod
openshift/origin-metrics-cassandra
openshift/origin-metrics-hawkular-metrics
openshift/origin-metrics-heapster
openshift/origin-metrics-deployer
openshift/mysql-55-centos7
openshift/origin-logging-curator
openshift/origin-logging-fluentd
openshift/origin-logging-deployment
openshift/origin-logging-elasticsearch
openshift/origin-logging-kibana
openshift/origin-logging-auth-proxy
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动后，会打印如下信息&lt;/p&gt;</description></item><item><title>源码部署K8S</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2k8s/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2k8s/</guid><description>&lt;div class="pageinfo pageinfo-primary"&gt;
&lt;p&gt;此文应该不能运行成功了，，，陈年老文，有待验证。&lt;/p&gt;

&lt;/div&gt;

&lt;h4 id="一-先介绍最省事的部署方法-直接从官网下release版本安装"&gt;一. 先介绍最省事的部署方法，直接从官网下release版本安装:&lt;a class="td-heading-self-link" href="#%e4%b8%80-%e5%85%88%e4%bb%8b%e7%bb%8d%e6%9c%80%e7%9c%81%e4%ba%8b%e7%9a%84%e9%83%a8%e7%bd%b2%e6%96%b9%e6%b3%95-%e7%9b%b4%e6%8e%a5%e4%bb%8e%e5%ae%98%e7%bd%91%e4%b8%8brelease%e7%89%88%e6%9c%ac%e5%ae%89%e8%a3%85" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;git clone 代码步骤省略 ...&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载各依赖的release版本&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过修改配置文件 &lt;strong&gt;cluster/centos/config-build.sh&lt;/strong&gt;， 可自定义（k8s, docker, flannel, etcd）各自的下载地址和版本， 不同的版本的依赖可能会需要小改下脚本（版本变更有些打包路径发生了变化，兼容性问题）&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;cd cluster/centos &amp;amp;&amp;amp; ./build.sh all
&lt;/code&gt;&lt;/pre&gt;&lt;ol start="2"&gt;
&lt;li&gt;安装并启动k8s集群环境&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过修改配置文件 &lt;strong&gt;cluster/centos/config-default.sh&lt;/strong&gt;，定义你环境里的设备的IP和其他参数，推荐运行脚本前先通过ssh-copy-id做好免密钥认证；&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;export KUBERNETES_PROVIDER=centos &amp;amp;&amp;amp; cluster/kube-up.sh
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id="二-源码级编译安装"&gt;二. 源码级编译安装&lt;a class="td-heading-self-link" href="#%e4%ba%8c-%e6%ba%90%e7%a0%81%e7%ba%a7%e7%bc%96%e8%af%91%e5%ae%89%e8%a3%85" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;本步骤基于上一大步来说,
先来看下载各依赖的release后，cluster/centos下目录发生了什么变化&lt;/p&gt;
&lt;p&gt;&lt;img src="https://xiaoping378.github.io/k8s-binaries-tree.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;多了一个binaries的目录，里面是各master和minion上各依赖的二进制文件， 所以我们只要源码编译的结果，替换到这里来， 然后继续上一大步的第2小步即可。&lt;/p&gt;
&lt;p&gt;这里说下，本地编译k8s的话，需要设置安装godep，然后命令本地化。&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;export PATH=$PATH:$GOPATH/bin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最后只需要去源码根目录下执行， 编译结果在_output目录下&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;make
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;替换到相应的binaries目录下，重新运行kube-up.sh即可。&lt;/p&gt;</description></item><item><title>docker网络</title><link>https://xiaoping378.github.io/docs/4-cloud/containers/docker-network/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/containers/docker-network/</guid><description>&lt;p&gt;自去年就开始推动公司业务使用docker了， 至今也一年多了，但对docker网络的认知一直一知半解。。。&lt;/p&gt;
&lt;p&gt;主要是太忙，加上线上业务也没出过关于网络吞吐性能方面的问题，就没太大动力去搞明白， 现在闲下来了，搞之！&lt;/p&gt;
&lt;h3 id="环境声明"&gt;环境声明&lt;a class="td-heading-self-link" href="#%e7%8e%af%e5%a2%83%e5%a3%b0%e6%98%8e" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;以下内容只针对OS: Ubuntu16.04 docker: 1.10.3的环境， 写本文时docker最新的release版本是1.11.2，还有什么CoreOS，Unikernel 之类的（表示都没玩过）。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;docker更新迭代速度太快了，公司业务只用到基本功能，所以没动力跟进它的更新了
各种新时代下的产物频出啊， CoreOS为linux的发行版， 没需求，好遗憾.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="docker的网络模式"&gt;docker的网络模式&lt;a class="td-heading-self-link" href="#docker%e7%9a%84%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%bc%8f" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;一开始安装完docker， 它就会默认创建3个网络， 使用__docker network ls__查看&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;➜ blog git:&lt;span style="color:#666"&gt;(&lt;/span&gt;master&lt;span style="color:#666"&gt;)&lt;/span&gt; docker network ls
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;NETWORK ID NAME DRIVER
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;46416a43fbc6 bridge bridge 
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;45398901e9f0 none null 
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;9440a8140e68 host host
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;当启动一个容器时， 默认使用bridge模式， 可以通过 --net 指定其它模式。&lt;/p&gt;
&lt;p&gt;下面先简要说明下各自的概念&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bridge 模式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;容器间之所以能通信，就靠宿主机上的docker0了， docker0就是bridge模式下默认创建的虚拟设备名称&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;➜ blog git:&lt;span style="color:#666"&gt;(&lt;/span&gt;master&lt;span style="color:#666"&gt;)&lt;/span&gt; ✗ ifconfig docker0
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;docker0 Link encap:Ethernet HWaddr 02:42:49:56:7c:3b 
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; inet addr:172.17.0.1 Bcast:0.0.0.0 Mask:255.255.0.0
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; inet6 addr: fe80::42:49ff:fe56:7c3b/64 Scope:Link
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; RX packets:78103 errors:0 dropped:0 overruns:0 frame:0
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; TX packets:47578 errors:0 dropped:0 overruns:0 carrier:0
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; collisions:0 txqueuelen:0
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; RX bytes:17485434 &lt;span style="color:#666"&gt;(&lt;/span&gt;17.4 MB&lt;span style="color:#666"&gt;)&lt;/span&gt; TX bytes:82163889 &lt;span style="color:#666"&gt;(&lt;/span&gt;82.1 MB&lt;span style="color:#666"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;ifocnfig可以看到很多信息， mac地址，IP等这些也可以通过参数指定成别的。&lt;/p&gt;</description></item><item><title>构建生产环境级的docker Swarm集群-2</title><link>https://xiaoping378.github.io/docs/4-cloud/containers/swarm/docker-swarm2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/containers/swarm/docker-swarm2/</guid><description>&lt;p&gt;此文档适用于不低于1.12版本的docker，因为swarm已内置于docker-engine里。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;硬件需求&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里以5台PC服务器为例, 分别如下作用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;manager0&lt;/li&gt;
&lt;li&gt;manager1&lt;/li&gt;
&lt;li&gt;node0&lt;/li&gt;
&lt;li&gt;node1&lt;/li&gt;
&lt;li&gt;node2&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="2"&gt;
&lt;li&gt;每台PC上安装docker-engine&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一台一台的ssh上去执行，或者使用ansible批量部署工具。&lt;/p&gt;
&lt;p&gt;安装docker-engine&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;curl -sSL https://get.docker.com/ | sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动之，并使之监听2375端口&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;sudo docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;亦可修改配置，使之永久生效&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;mkdir /etc/systemd/system/docker.service.d
cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt;/etc/systemd/system/docker.service.d/docker.conf
[Service]
 ExecStart=
 ExecStart=/usr/bin/docker daemon -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --dns 180.76.76.76 --insecure-registry registry.cecf.com -g /home/Docker/docker
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果开启了防火墙，需要开启如下端口&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TCP port 2377&lt;/strong&gt; for cluster management communications&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TCP&lt;/strong&gt; and &lt;strong&gt;UDP port 7946&lt;/strong&gt; for communication among nodes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TCP&lt;/strong&gt; and &lt;strong&gt;UDP port 4789&lt;/strong&gt; for overlay network traffic&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;创建swarm&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;docker swarm init --advertise-addr &amp;lt;MANAGER-IP&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我的实例里如下：&lt;/p&gt;</description></item><item><title>离线安装kubernetes</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/kuberbetes-1.5-%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/kuberbetes-1.5-%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2/</guid><description>&lt;div class="pageinfo pageinfo-primary"&gt;
&lt;p&gt;虽然距离当前主流版本已经差之千里，但其中的思想仍记得借鉴。&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;经常遇到全新初始安装k8s集群的问题，所以想着搞成离线模式，本着最小依赖原则，提高安装速度&lt;/p&gt;
&lt;p&gt;基于Centos7-1511-minimal, 非此版本脚本应该会运行出错，自行修改吧&lt;/p&gt;
&lt;p&gt;本离线安装所有的依赖都打包放到了&lt;a href="https://pan.baidu.com/s/1i5jusip"&gt;百度网盘&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;为了便于维护，已建立独立项目&lt;a href="https://github.com/xiaoping378/k8s-deploy"&gt;k8s-deploy&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="第一步"&gt;第一步&lt;a class="td-heading-self-link" href="#%e7%ac%ac%e4%b8%80%e6%ad%a5" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;基本思路是，在k8s-deploy目录下，临时启个http server， node节点上会从此拉取所依赖镜像和rpms&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;# python -m SimpleHTTPServer
Serving HTTP on 0.0.0.0 port 8000 ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;windows上可以用hfs临时启个http server， 自行百度如何使用&lt;/p&gt;
&lt;h2 id="master侧"&gt;master侧&lt;a class="td-heading-self-link" href="#master%e4%be%a7" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;运行以下命令，初始化master&lt;/p&gt;
&lt;p&gt;192.168.56.1:8000 是我的http-server, 注意要将k8s-deploy.sh 里的HTTP-SERVER变量也改下&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;curl -L http://192.168.56.1:8000/k8s-deploy.sh | bash -s master
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id="minion侧"&gt;minion侧&lt;a class="td-heading-self-link" href="#minion%e4%be%a7" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;视自己的情况而定&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;curl -L http://192.168.56.1:8000/k8s-deploy.sh | bash -s join --token=6669b1.81f129bc847154f9 192.168.56.100
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id="总结"&gt;总结&lt;a class="td-heading-self-link" href="#%e6%80%bb%e7%bb%93" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;整个脚本实现比较简单， 坑都在脚本里解决了。脚本文件在&lt;a href="https://gist.github.com/xiaoping378/3a129aa6c81eaecae199a50236ad8bf7"&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;就一个master-up和node-up， 基本一个函数只做一件事，很清晰，可以自己查看具体过程。&lt;/p&gt;
&lt;p&gt;1.5 与 1.3给我感觉最大的变化是网络部分， 1.5启用了cni网络插件
不需要像以前一样非要把flannel和docker绑在一起了（先启flannel才能启docker）。&lt;/p&gt;
&lt;p&gt;具体可以看这里
&lt;a href="https://github.com/containernetworking/cni/blob/master/Documentation/flannel.md"&gt;https://github.com/containernetworking/cni/blob/master/Documentation/flannel.md&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;master侧如果是单核的话，会因资源不足， dns安装失败。&lt;/p&gt;</description></item><item><title>权限资源管理</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%9D%83%E9%99%90%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%9D%83%E9%99%90%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</guid><description>&lt;p&gt;重点介绍 project，limitRange，resourceQuta和 user, group, rule，role，policy，policybinding的关系,
我刚接触时，这几个概念老搞不太清楚，这里梳理下&lt;/p&gt;
&lt;h2 id="资源管理说明"&gt;资源管理说明&lt;a class="td-heading-self-link" href="#%e8%b5%84%e6%ba%90%e7%ae%a1%e7%90%86%e8%af%b4%e6%98%8e" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;可以对计算资源的大小和对象类型的数量来进行配额限制。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ResourceQuota&lt;/code&gt;是面向project（namespace的基础上加了些注解）层面的，只有集群管理员可以基于namespace设置。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;limtRange&lt;/code&gt;是面向pod和container级别的，openshift额外还可以限制 image， imageStream和pvc，
也是只有集群管理员才可以基于project设置，而开发人员只能基于pod（container）设置cpu和内存的requests/limits。&lt;/p&gt;
&lt;h3 id="resourcequota"&gt;ResourceQuota&lt;a class="td-heading-self-link" href="#resourcequota" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;看看具体可以管理哪些资源，期待网络相关的也加进来.简单来讲，可以基于project来限制可消耗的内存大小和可创建的pods数量&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-go" data-lang="go"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;// The following identify resource constants for Kubernetes object types&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;&lt;/span&gt;&lt;span style="color:#a2f;font-weight:bold"&gt;const&lt;/span&gt;&lt;span style="color:#bbb"&gt; &lt;/span&gt;(&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// Pods, number&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourcePods&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;pods&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// Services, number&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceServices&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;services&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// ReplicationControllers, number&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceReplicationControllers&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;replicationcontrollers&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// ResourceQuotas, number&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceQuotas&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;resourcequotas&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// ResourceSecrets, number&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceSecrets&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;secrets&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// ResourceConfigMaps, number&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceConfigMaps&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;configmaps&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// ResourcePersistentVolumeClaims, number&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourcePersistentVolumeClaims&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;persistentvolumeclaims&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// ResourceServicesNodePorts, number&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceServicesNodePorts&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;services.nodeports&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// ResourceServicesLoadBalancers, number&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceServicesLoadBalancers&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;services.loadbalancers&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// CPU request, in cores. (500m = .5 cores)&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceRequestsCPU&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;requests.cpu&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// Memory request, in bytes. (500Gi = 500GiB = 500 * 1024 * 1024 * 1024)&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceRequestsMemory&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;requests.memory&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// Storage request, in bytes&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceRequestsStorage&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;requests.storage&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// CPU limit, in cores. (500m = .5 cores)&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceLimitsCPU&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;limits.cpu&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;// Memory limit, in bytes. (500Gi = 500GiB = 500 * 1024 * 1024 * 1024)&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;	&lt;/span&gt;ResourceLimitsMemory&lt;span style="color:#bbb"&gt; &lt;/span&gt;ResourceName&lt;span style="color:#bbb"&gt; &lt;/span&gt;=&lt;span style="color:#bbb"&gt; &lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;limits.memory&amp;#34;&lt;/span&gt;&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#bbb"&gt;&lt;/span&gt;)&lt;span style="color:#bbb"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;openshift额外支持的images相关的限制策略&lt;/p&gt;</description></item><item><title>k8s的各组件和特性扫盲</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/k8s%E5%90%84%E7%BB%84%E4%BB%B6%E7%9A%84%E8%AE%A4%E7%9F%A5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/k8s%E5%90%84%E7%BB%84%E4%BB%B6%E7%9A%84%E8%AE%A4%E7%9F%A5/</guid><description>&lt;div class="pageinfo pageinfo-primary"&gt;
&lt;p&gt;了解一个工具的特性可以从它的参数入手&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id="api-server"&gt;api-server&lt;a class="td-heading-self-link" href="#api-server" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;在k8s内发挥的网关和api&lt;/p&gt;
&lt;p&gt;CSR特性&lt;/p&gt;
&lt;h2 id="网络"&gt;网络&lt;a class="td-heading-self-link" href="#%e7%bd%91%e7%bb%9c" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="flannel"&gt;flannel&lt;a class="td-heading-self-link" href="#flannel" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;flannel的设计就是为集群中所有节点能重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，
并让属于不同节点上的容器能够直接通过内网IP通信。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;实际上就是给每个节点的docker重新设置容器上可分配的IP段， &lt;code&gt;--bip&lt;/code&gt;的妙用。
这恰好迎合了k8s的设计，即一个pod（container）在集群中拥有唯一、可路由到的IP，带来的好处就是减少跨主机容器间通信要port mapping的复杂性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;原理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;flannle需要运行一个叫flanned的agent，其用etcd来存储网络配置、已经分配的子网、和辅助信息（主机IP),如下&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;root@master1 ~&lt;span style="color:#666"&gt;]&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;# etcdctl ls /coreos.com/network&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;/coreos.com/network/config
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;/coreos.com/network/subnets
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;root@master1 ~&lt;span style="color:#666"&gt;]&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;root@master1 ~&lt;span style="color:#666"&gt;]&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;# etcdctl get /coreos.com/network/config&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;{&lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;Network&amp;#34;&lt;/span&gt;:&lt;span style="color:#b44"&gt;&amp;#34;172.16.0.0/16&amp;#34;&lt;/span&gt;&lt;span style="color:#666"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;root@master1 ~&lt;span style="color:#666"&gt;]&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;root@master1 ~&lt;span style="color:#666"&gt;]&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;# etcdctl ls /coreos.com/network/subnets&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;/coreos.com/network/subnets/172.16.29.0-24
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;/coreos.com/network/subnets/172.16.40.0-24
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;/coreos.com/network/subnets/172.16.60.0-24
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;root@master1 ~&lt;span style="color:#666"&gt;]&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;root@master1 ~&lt;span style="color:#666"&gt;]&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;# etcdctl get /coreos.com/network/subnets/172.16.29.0-24&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;{&lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;PublicIP&amp;#34;&lt;/span&gt;:&lt;span style="color:#b44"&gt;&amp;#34;192.168.1.129&amp;#34;&lt;/span&gt;&lt;span style="color:#666"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;flannel0 还负责解封装报文,或者创建路由。
flannel有多种方式可以完成报文的转发。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;UDP&lt;/li&gt;
&lt;li&gt;vxlan&lt;/li&gt;
&lt;li&gt;host-gw&lt;/li&gt;
&lt;li&gt;aws-vpc&lt;/li&gt;
&lt;li&gt;gce&lt;/li&gt;
&lt;li&gt;alloc&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是经典的UDP封装方式数据流图
&lt;img src="https://xiaoping378.github.io/flannel-packet-01.png" alt="UDP"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>从docker迁移到containerd</title><link>https://xiaoping378.github.io/docs/4-cloud/containers/containerd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/containers/containerd/</guid><description>&lt;div class="pageinfo pageinfo-primary"&gt;
&lt;p&gt;记录个人从docker迁移到containerd的事项&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id="介绍"&gt;介绍&lt;a class="td-heading-self-link" href="#%e4%bb%8b%e7%bb%8d" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/containerd/containerd"&gt;https://github.com/containerd/containerd&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TODO.&lt;/p&gt;
&lt;h2 id="安装containerd"&gt;安装containerd&lt;a class="td-heading-self-link" href="#%e5%ae%89%e8%a3%85containerd" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;下载必要组件：https://github.com/containerd/nerdctl/releases，&lt;/p&gt;</description></item><item><title>构建生产环境级的docker Swarm集群-3</title><link>https://xiaoping378.github.io/docs/4-cloud/containers/swarm/docker-sarm3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/containers/swarm/docker-sarm3/</guid><description>&lt;p&gt;如前文所述，默认已经搭建好环境，基于docker1.12版本。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;root@manager0 ~&lt;span style="color:#666"&gt;]&lt;/span&gt;&lt;span style="color:#080;font-style:italic"&gt;# docker node ls&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;0bbmd3r7aphs374qaea4zcieo node2 Ready Active
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;3qmxzyauc0bz4kjqvld9uogz5 manager1 Ready Active Reachable
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;5ewbdtvaopj4ltwqx0a4i65nt * manager0 Ready Drain Leader
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;5oxxpgk69fnwe5w210kovrqi9 node1 Ready Active
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;7s1ilay2wkjgt09bp2z0743m7 node0 Ready Active
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;创建第一个服务，以redis为例
swarm里容器间通信需要使用overlay模式，所以需要提前建立一个&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;docker network create -d overlay --subnet 10.254.0.0/16 --gateway 10.254.0.1 mynet1
docker service create --name redis --network mynet1 redis
&lt;/code&gt;&lt;/pre&gt;&lt;ol start="2"&gt;
&lt;li&gt;在manager上查看服务部署情况&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;[root@manager0 ~]# docker service ps redis
ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR
9avksjfqr2gxm413dfrezrmgr redis.1 redis node1 Running Running 17 seconds ago
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;实例里，同样可以去node1上用&lt;code&gt;docker ps&lt;/code&gt;查看&lt;/p&gt;</description></item><item><title>项目开发实战</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98/</guid><description>&lt;p&gt;下面的所有操作，都可以通过cli，web console，RestFul API实现，默认使用cli说明&lt;/p&gt;
&lt;h3 id="创建项目"&gt;创建项目&lt;a class="td-heading-self-link" href="#%e5%88%9b%e5%bb%ba%e9%a1%b9%e7%9b%ae" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;这里是接着oc cluster up后，来说的， 默认&lt;code&gt;oc whoami&lt;/code&gt;是 developer,拥有admin的Role角色，俗称项目经理（管理员）&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;删除默认创建的项目，并创建一个实际中的项目&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-shell" data-lang="shell"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;oc delete project myproject
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;oc new-project eshop --display-name&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;电商项目&amp;#34;&lt;/span&gt; --description&lt;span style="color:#666"&gt;=&lt;/span&gt;&lt;span style="color:#b44"&gt;&amp;#34;一个神奇的网站&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;现在项目管理员可以创建任意多个项目，从前面的源码可以看到目前是没法针对项目管理员去限制可创建项目上限的。&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;查看项目状态&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;#oc status
In project 电商项目 (eshop) on server https://192.168.31.49:8443

You have no services, deployment configs, or build configs.
Run &amp;#39;oc new-app&amp;#39; to create an application.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;空空如也，有提示语句提示可通过&lt;code&gt;oc new-app&lt;/code&gt;去创建具体应用的&lt;/p&gt;
&lt;h3 id="创建应用"&gt;创建应用&lt;a class="td-heading-self-link" href="#%e5%88%9b%e5%bb%ba%e5%ba%94%e7%94%a8" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;前面也说过，openshift的核心就是围绕应用的整个生命周期来的，所以从new-app说起&lt;/p&gt;
&lt;p&gt;new-app的入口是&lt;code&gt;NewCmdNewApplication()&lt;/code&gt;, 大部分实现是 &lt;code&gt;func (c *AppConfig) Run() (*AppResult, error)&lt;/code&gt; 感兴趣的可以根据源码来理解openshift的devops理念。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建应用的方式
现在可以通过3种方式（源码， docker镜像， 模板）来创建一个应用。&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;# oc new-app -h
#此处省略。。。
Usage:
 oc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [options]
#此处省略。。。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有很多灵活简便的方式来创建应用，甚至可以直接&lt;code&gt;oc new-app mysql&lt;/code&gt;来创建一个mysql服务&lt;/p&gt;</description></item><item><title>DevOps实战-0</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-0/</guid><description>&lt;p&gt;主要涉及到&lt;code&gt;一键发布&lt;/code&gt;，&lt;code&gt;快速回滚&lt;/code&gt;，&lt;code&gt;弹性伸缩&lt;/code&gt;，&lt;code&gt;蓝绿部署&lt;/code&gt;方面。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;启动openshift&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc cluster up --version=v1.5.0-rc.0 --metrics --use-existing-config=true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;默认负责监控的pods占用资源太大了，可以这样限制下，或者cluster up时不加 &lt;code&gt;--metrics&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc login -u system:admin
oc env rc hawkular-cassandra-1 MAX_HEAP_SIZE=1024M -n openshift-infra

#重建下,变量才会生效
oc scale rc hawkular-cassandra-1 --replicas 0 -n openshift-infra
oc scale rc hawkular-cassandra-1 --replicas 1 -n openshift-infra
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;建立本地Git仓&lt;/p&gt;
&lt;p&gt;默认官方给出的例子基本都需要和Github结合，实在不好本地实战演示，所以本地要来一个&lt;code&gt;gogs&lt;/code&gt;代码仓。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc login -u devloper
oc new-project ci

#先拉取所依赖镜像
docker pull openshiftdemos/gogs:0.9.97
docker pull centos/postgresql-94-centos7

#创建gogs服务，并禁用webhook时的TLS校验，不然无法触发build
oc new-app -f https://raw.githubusercontent.com/xiaoping378/gogs-openshift-docker/master/openshift/gogs-persistent-template.yaml -p SKIP_TLS_VERIFY=true -p HOSTNAME=gogs-ci.192.168.31.49.xip.io
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面的HOSTNAME，注意要换成自己宿主机的IPv4地址，默认创建的其他服务的路由都是这个形式的，&lt;/p&gt;
&lt;p&gt;有个有意思的地方，为什么默认路由会是这种 &lt;code&gt;name+IP+xip.io&lt;/code&gt; 形式呢，奥秘在 &lt;a href="http://xip.io"&gt;http://xip.io&lt;/a&gt; 的公共服务上。
这其实是个特殊的域DNS server，比如我们查询域名&lt;code&gt;gogs-ci.192.168.31.49.xip.io&lt;/code&gt;时 ，会返回192.168.31.49的地址回来，
而这个地址恰好是我们Router的地址，这样子Router会根据route的配置负责负载到对应的POD上。自己试验下就知道怎么回事了。&lt;/p&gt;</description></item><item><title>Helm模板介绍</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/kubernetes-helm%E6%A8%A1%E6%9D%BF%E6%80%BB%E7%BB%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/kubernetes-helm%E6%A8%A1%E6%9D%BF%E6%80%BB%E7%BB%93/</guid><description>&lt;h3 id="概要"&gt;概要&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%a6%81" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Helm是一个管理kubernetes集群内应用的工具，提供了一系列管理应用的快捷方式，例如 inspect， install， upgrade， delete等，经验可以沿用以前apt，yum，homebrew的,区别就是helm管理的是kubernetes集群内的应用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;还有一个概念必须得提，就是&lt;code&gt;chart&lt;/code&gt;， 它代表的就是被helm管理的应用包，里面具体就是放一些预先配置的Kubernetes资源(pod, rc, deployment, service, ingress)，一个包描述文件(&lt;code&gt;Chart.yaml&lt;/code&gt;), 还可以通过指定依赖来组织成更复杂的应用，支持go template语法，可参数化模板，让使用者定制化安装
charts可以存放在本地，也可以放在远端，这点理解成yum仓很合适。。。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里有个&lt;a href="https://kubeapps.com"&gt;应用市场&lt;/a&gt; ，里面罗列了各种应用charts。由开源项目&lt;a href="https://github.com/helm/monocular"&gt;monocular&lt;/a&gt;支撑&lt;/p&gt;
&lt;p&gt;下面主要介绍helm的基本使用流程和具体场景的实践。&lt;/p&gt;
&lt;h3 id="初始化k8s集群v1-6-2"&gt;初始化k8s集群v1.6.2&lt;a class="td-heading-self-link" href="#%e5%88%9d%e5%a7%8b%e5%8c%96k8s%e9%9b%86%e7%be%a4v1-6-2" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;先来准备k8s环境，可以通过&lt;a href="https://github.com/xiaoping378/k8s-deploy"&gt;k8s-deploy&lt;/a&gt;项目来离线安装高可用kubernetes集群，我这里是单机演示环境。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;kubeadm init --kubernetes-version v1.6.2 --pod-network-cidr 12.240.0.0/12
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;#方便命令自动补全&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#a2f"&gt;source&lt;/span&gt; &amp;lt;&lt;span style="color:#666"&gt;(&lt;/span&gt;kubectl completion zsh&lt;span style="color:#666"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;#安装cni网络&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;cp /etc/kubernetes/admin.conf &lt;span style="color:#b8860b"&gt;$HOME&lt;/span&gt;/.kube/config
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;kubectl apply -f kube-flannel-rbac.yml
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;kubectl apply -f kube-flannel.yml
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;#使能master可以被调度&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;kubectl taint node --all node-role.kubernetes.io/master-
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;#安装ingress-controller, 边界路由作用&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;kubectl create -f ingress-traefik-rbac.yml
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;kubectl create -f ingress-traefik-deploy.yml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这样一个比较完整的k8s环境就具备了，另外监控和日志不在此文的讨论范围内。&lt;/p&gt;
&lt;h3 id="初始化helm环境"&gt;初始化Helm环境&lt;a class="td-heading-self-link" href="#%e5%88%9d%e5%a7%8b%e5%8c%96helm%e7%8e%af%e5%a2%83" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;由于刚才创建的k8s集群默认启用RBAC机制，个人认为这个特性是k8s真正走向成熟的一大标志，废话不表，为了helm可以安装任何应用，我们先给他最高权限。&lt;/p&gt;</description></item><item><title>DevOps实战-1</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-1/</guid><description>&lt;p&gt;本文主要介绍基于openshift如何完成&lt;code&gt;开发-&amp;gt;测试-&amp;gt;线上&lt;/code&gt;场景的变更，这是一个典型的应用生产流程，来看看openshift是如何利用容器优雅的完成整个过程的吧&lt;/p&gt;
&lt;p&gt;下文基于上篇&lt;a href="../openshift%E5%AE%9E%E8%B7%B5-devops%E5%AE%9E%E6%88%98-0"&gt;DevOps实战-0&lt;/a&gt; 的&lt;code&gt;nodejs-ex&lt;/code&gt;项目来说, 假设到这里，你本地已经有了nodejs-ex项目&lt;/p&gt;
&lt;h3 id="准备3个project"&gt;准备3个project&lt;a class="td-heading-self-link" href="#%e5%87%86%e5%a4%873%e4%b8%aaproject" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;用这3个project来模拟开发，测试，线上环境&lt;/p&gt;
&lt;p&gt;现实中一般各个场景的服务器都是物理隔离的，这里可以利用&lt;code&gt;--node-selector&lt;/code&gt;，来指定项目可以跑在哪些节点上。&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;oc login -u sysetm:admin

#晚上在笔记本上写此blog，没合适的环境，单机模拟多台 -- start
oc label node 192.168.31.49 web-prod=true web-dev=true web-test=true
#晚上在笔记本上写此blog，没合适的环境，单机模拟多台 -- end

#1.创建web-dev项目
#2.授权developer为开发组项目管理员
#3.授权测试和运维人员可以从开发组拉取镜像
oc adm new-project web-dev --node-selector=&amp;#39;web-dev=true&amp;#39;
oc policy add-role-to-user admin developer
oc policy add-role-to-group system:image-puller system:serviceaccounts:web-test -n web-dev
oc policy add-role-to-group system:image-puller system:serviceaccounts:web-prod -n web-dev

oc adm new-project web-test --node-selector=&amp;#39;web-test=true&amp;#39;
oc policy add-role-to-user admin tester

oc adm new-project web-prod --node-selector=&amp;#39;web-prod=true&amp;#39;
oc policy add-role-to-user admin ops
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;你可能会注意到，这里用的&lt;code&gt;new-project&lt;/code&gt; 前面还加了adm， 其实&lt;code&gt;oc adm&lt;/code&gt;等效于&lt;code&gt;oadm&lt;/code&gt;， 一般管理集群相关的用这个命令，这里是因为需要读取节点的标签（label）信息。&lt;/p&gt;</description></item><item><title>k8s的监控方案</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/kuerbernetes%E7%9A%84%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/kuerbernetes%E7%9A%84%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88/</guid><description>&lt;h2 id="方案选型"&gt;方案选型&lt;a class="td-heading-self-link" href="#%e6%96%b9%e6%a1%88%e9%80%89%e5%9e%8b" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;如果已存在完善的监控系统的话，推荐使用k8s原生的&lt;strong&gt;heapster&lt;/strong&gt;，比较轻量，容易集成。&lt;/p&gt;
&lt;p&gt;我选择的是&lt;strong&gt;prometheus&lt;/strong&gt;, 它是比较完善的云平台级监控方案，继k8s之后同样已被列入&lt;a href="https://cncf.io/projects"&gt;云计算基金会&lt;/a&gt;项目, 除了具备heapster的能力之外，还支持监控广泛的应用(mysql, JMX, HAProxy等)和灵活的告警的能力，并具备多IDC federation的能力，兼容多种开源监控系统（StatsD, Ganglia, collectd, nagios等）。&lt;/p&gt;
&lt;p&gt;本文主要参考&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/kubernetes/heapster/issues/645"&gt;prometheus和heapster开发者之间的对话&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CoreOS的blog&lt;a href="https://coreos.com/blog/monitoring-kubernetes-with-prometheus.html"&gt;Monitoring Kubernetes with Prometheus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面分别介绍下两种方案&lt;/p&gt;
&lt;h3 id="heapster"&gt;heapster&lt;a class="td-heading-self-link" href="#heapster" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;heapster的介绍:&lt;/p&gt;
&lt;p&gt;通过向kubelet拉取stats的方式， 可提供15分钟内的缓存供k8s的dashboard用，也支持第三方存储，如influxdb等，还具备REST API(经我实验，这个API还不完善 &lt;a href="https://github.com/kubernetes/heapster/issues/1155"&gt;缺少diskIO API&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;heapster的监控范围&lt;/p&gt;
&lt;p&gt;可监控的内容包括集群内的Container, Pod, Node 和 Namespace的性能或配置信息，
目前container级别还不支持网络和硬盘信息，具体性能项如下&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;Metric Name&lt;/th&gt;
 &lt;th&gt;Description&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;cpu/limit&lt;/td&gt;
 &lt;td&gt;CPU hard limit in millicores.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;cpu/node_capacity&lt;/td&gt;
 &lt;td&gt;Cpu capacity of a node.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;cpu/node_allocatable&lt;/td&gt;
 &lt;td&gt;Cpu allocatable of a node.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;cpu/node_reservation&lt;/td&gt;
 &lt;td&gt;Share of cpu that is reserved on the node allocatable.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;cpu/node_utilization&lt;/td&gt;
 &lt;td&gt;CPU utilization as a share of node allocatable.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;cpu/request&lt;/td&gt;
 &lt;td&gt;CPU request (the guaranteed amount of resources) in millicores.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;cpu/usage&lt;/td&gt;
 &lt;td&gt;Cumulative CPU usage on all cores.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;cpu/usage_rate&lt;/td&gt;
 &lt;td&gt;CPU usage on all cores in millicores.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;filesystem/usage&lt;/td&gt;
 &lt;td&gt;Total number of bytes consumed on a filesystem.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;filesystem/limit&lt;/td&gt;
 &lt;td&gt;The total size of filesystem in bytes.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;filesystem/available&lt;/td&gt;
 &lt;td&gt;The number of available bytes remaining in a the filesystem&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/limit&lt;/td&gt;
 &lt;td&gt;Memory hard limit in bytes.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/major_page_faults&lt;/td&gt;
 &lt;td&gt;Number of major page faults.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/major_page_faults_rate&lt;/td&gt;
 &lt;td&gt;Number of major page faults per second.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/node_capacity&lt;/td&gt;
 &lt;td&gt;Memory capacity of a node.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/node_allocatable&lt;/td&gt;
 &lt;td&gt;Memory allocatable of a node.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/node_reservation&lt;/td&gt;
 &lt;td&gt;Share of memory that is reserved on the node allocatable.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/node_utilization&lt;/td&gt;
 &lt;td&gt;Memory utilization as a share of memory allocatable.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/page_faults&lt;/td&gt;
 &lt;td&gt;Number of page faults.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/page_faults_rate&lt;/td&gt;
 &lt;td&gt;Number of page faults per second.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/request&lt;/td&gt;
 &lt;td&gt;Memory request (the guaranteed amount of resources) in bytes.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/usage&lt;/td&gt;
 &lt;td&gt;Total memory usage.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;memory/working_set&lt;/td&gt;
 &lt;td&gt;Total working set usage. Working set is the memory being used and not easily dropped by the kernel.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;network/rx&lt;/td&gt;
 &lt;td&gt;Cumulative number of bytes received over the network.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;network/rx_errors&lt;/td&gt;
 &lt;td&gt;Cumulative number of errors while receiving over the network.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;network/rx_errors_rate&lt;/td&gt;
 &lt;td&gt;Number of errors while receiving over the network per second.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;network/rx_rate&lt;/td&gt;
 &lt;td&gt;Number of bytes received over the network per second.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;network/tx&lt;/td&gt;
 &lt;td&gt;Cumulative number of bytes sent over the network&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;network/tx_errors&lt;/td&gt;
 &lt;td&gt;Cumulative number of errors while sending over the network&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;network/tx_errors_rate&lt;/td&gt;
 &lt;td&gt;Number of errors while sending over the network&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;network/tx_rate&lt;/td&gt;
 &lt;td&gt;Number of bytes sent over the network per second.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;uptime&lt;/td&gt;
 &lt;td&gt;Number of milliseconds since the container was started.&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="prometheus"&gt;Prometheus&lt;a class="td-heading-self-link" href="#prometheus" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Prometheus集成了数据采集，存储，异常告警多项功能，是一款一体化的完整方案。 它针对大规模的集群环境设计了拉取式的数据采集方式、多维度数据存储格式以及服务发现等创新功能。&lt;/p&gt;</description></item><item><title> 编译和目录结构介绍</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E6%BA%90%E7%A0%81-%E7%BC%96%E8%AF%91%E5%92%8C%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E6%BA%90%E7%A0%81-%E7%BC%96%E8%AF%91%E5%92%8C%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/</guid><description>&lt;p&gt;介绍openshift的源码编译和目录结构组织，为了方便代码调试和了解大型Golang项目的构建方式&lt;/p&gt;
&lt;h3 id="编译"&gt;编译&lt;a class="td-heading-self-link" href="#%e7%bc%96%e8%af%91" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;无论是openshift还是Kubernetes等大型Golang项目都用到了&lt;code&gt;Makefile&lt;/code&gt;, 所以有必要从此开始说起，这里只说项目里用到的makefile特性，想了解更多的可以参考&lt;a href="http://scc.qibebt.cas.cn/docs/linux/base/%B8%FA%CE%D2%D2%BB%C6%F0%D0%B4Makefile-%B3%C2%F0%A9.pdf"&gt;跟我一起写Makefile&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="makefile介绍"&gt;Makefile介绍&lt;a class="td-heading-self-link" href="#makefile%e4%bb%8b%e7%bb%8d" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;makefile 关系到了整个工程的编译规则。一个工程中的源文件不计数，其按类型、功能、
模块分别放在若干个目录中，makefile 定义了一系列的规则来指定，哪些文件需要先编译，
哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为
makefile 就像一个 Shell 脚本一样，其中也可以执行操作系统的命令。 makefile 带来的好
处就是——“自动化编译”，一旦写好，只需要一个 make 命令，整个工程完全自动编译，
极大的提高了软件开发的效率。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Makefile里的规则，就在做两件事，一个是指明依赖关系，另一个是生成目标的方法&lt;/p&gt;
&lt;p&gt;Golang项目里用到的Makefile规则比较简单，基本就是定义一个目标的生成方法，下面的示例是Openshift项目里makefile中定义的第一个目标。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-makefile" data-lang="makefile"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#00a000"&gt;all build&lt;/span&gt;&lt;span style="color:#666"&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;	hack/build-go.sh &lt;span style="color:#a2f;font-weight:bold"&gt;$(&lt;/span&gt;WHAT&lt;span style="color:#a2f;font-weight:bold"&gt;)&lt;/span&gt; &lt;span style="color:#a2f;font-weight:bold"&gt;$(&lt;/span&gt;GOFLAGS&lt;span style="color:#a2f;font-weight:bold"&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#00a000"&gt;.PHONY&lt;/span&gt;&lt;span style="color:#666"&gt;:&lt;/span&gt; all build
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;all build&lt;/code&gt;，是定义的目标，看到这个就知道可以在源码的根目录上执行&lt;code&gt;make all build&lt;/code&gt;来编译了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二行说明生成目标的方法，就是去hack目录下执行build-go.sh脚本，这里还支持传入一些参数&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第三行 &lt;code&gt;.PHONY&lt;/code&gt;，起到一个标识的作用，没什么实际意义，是用来告诉make命令，这里是个伪目标，也可以说成是默认目标，所以在openshift的根目录上直接执行&lt;code&gt;make&lt;/code&gt;, 等效于&lt;code&gt;make all build&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;还可以自己决定是否编译出镜像或者rpm包（make release, make build-rpms）&lt;/p&gt;
&lt;h4 id="编译openshift"&gt;编译openshift&lt;a class="td-heading-self-link" href="#%e7%bc%96%e8%af%91openshift" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;上边介绍了，直接敲&lt;code&gt;make&lt;/code&gt;就可以自动编译出所有平台（linux, mac, windows）的二进制，编译前介绍两个hack方法，&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在hack/build-go.sh的第二行加上&lt;code&gt;set -x&lt;/code&gt;， 这样的话，shell脚本在运行时，里面的所有变量和执行路径会全部打印出来，一目了然，不用自己一行一行的加echo debug了&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如下修改hack/build-cross.sh，不然会编译出多平台的二进制，花的时间略长啊。。。&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;# by default, build for these platforms
platforms=(
 linux/amd64
 # darwin/amd64
 # windows/amd64
 # linux/386
)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面简易说下执行make后，都发生了什么，只会捡关键点说。&lt;/p&gt;</description></item><item><title>配置harbor默认https访问</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/harbor_https/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/harbor_https/</guid><description>&lt;p&gt;因为使用自签证书（reg.300.cn），所以需要把中间过程生成的ca.crt拷贝到需要pull/push的node上 (懒的翻译了，很详细的文档，已验证OK)&lt;/p&gt;
&lt;p&gt;Because Harbor does not ship with any certificates, it uses HTTP by default to serve registry requests. This makes it relatively simple to configure. However, it is highly recommended that security be enabled for any production environment. Harbor has an Nginx instance as a reverse proxy for all services, you can configure Nginx to enable https.&lt;/p&gt;
&lt;p&gt;##Getting a certificate&lt;/p&gt;
&lt;p&gt;Assuming that your registry's &lt;strong&gt;hostname&lt;/strong&gt; is &lt;strong&gt;reg.yourdomain.com&lt;/strong&gt;, and that its DNS record points to the host where you are running Harbor. You first should get a certificate from a CA. The certificate usually contains a .crt file and a .key file, for example, &lt;strong&gt;yourdomain.com.crt&lt;/strong&gt; and &lt;strong&gt;yourdomain.com.key&lt;/strong&gt;.&lt;/p&gt;</description></item><item><title>多负载均衡方案</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-router%E5%92%8Chaproxy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-router%E5%92%8Chaproxy/</guid><description>&lt;p&gt;haproxy在openshift里默认有两种用处，一个种负责master的高可用，一种是负责外部对内服务的访问（ingress controller）&lt;/p&gt;
&lt;p&gt;平台部署情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3台master，etcd&lt;/li&gt;
&lt;li&gt;1台node&lt;/li&gt;
&lt;li&gt;1台lb（haproxy）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="haproxy负载均衡master的高可用"&gt;haproxy负载均衡master的高可用&lt;a class="td-heading-self-link" href="#haproxy%e8%b4%9f%e8%bd%bd%e5%9d%87%e8%a1%a1master%e7%9a%84%e9%ab%98%e5%8f%af%e7%94%a8" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;lb负责master间的负载均衡，其实负载没那么大，更多得是用来避免单点故障&lt;/p&gt;
&lt;h3 id="debug介绍"&gt;Debug介绍&lt;a class="td-heading-self-link" href="#debug%e4%bb%8b%e7%bb%8d" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;默认安装haproxy1.5.18版本，开启debug方法&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;# 默认systemd对haproxy做了封装，会以-Ds后台形式启动，debug信息是看不到的
systemctl stop harproxy

# vi /etc/haproxy/haproxy.cfg
 log 127.0.0.1 local3 debug

# 手动启动haproxy
haproxy -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;不知道是不是哪里还需要设置，打印出来的日志，信息并不是不太多&lt;/p&gt;
&lt;p&gt;另外浏览&lt;code&gt;https://lbIP:9000&lt;/code&gt;, 可以看到统计信息&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="配置介绍"&gt;配置介绍&lt;a class="td-heading-self-link" href="#%e9%85%8d%e7%bd%ae%e4%bb%8b%e7%bb%8d" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;使用&lt;a href="https://github.com/xiaoping378/openshift-deploy"&gt;openshift-ansible&lt;/a&gt;部署后，harpxy的配置如下&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;[root@node4 ~]# cat /etc/haproxy/haproxy.cfg
# Global settings
#---------------------------------------------------------------------
global
 chroot /var/lib/haproxy
 pidfile /var/run/haproxy.pid
 maxconn 20000
 user haproxy
 group haproxy
 daemon
 log /dev/log local0 info #定义debug级别

 # turn on stats unix socket
 stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the &amp;#39;listen&amp;#39; and &amp;#39;backend&amp;#39; sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults #默认配置，后面同KEY的设置会覆盖此处
 mode http #工作在七层代理，客户端请求在转发至后端服务器之前将会被深度分板，所有不与RFC格式兼容的请求都会被拒绝，一些七层的过滤处理手段，可以使用。
 log global #默认启用gloabl的日志设置
 option httplog #默认日志类别为http日志格式
 option dontlognull #不记录健康检查日志信息（端口扫描，空信息）
# option http-server-close
 option forwardfor except 127.0.0.0/8 #如果上游服务器上的应用程序想记录客户端的真实IP地址，haproxy会把客户端的IP信息发送给上游服务器，在HTTP请求中添加”X-Forwarded-For”字段,但当是haproxy自身的健康检测机制去访问上游服务器时是不应该把这样的访问日志记录到日志中的，所以用except来排除127.0.0.0，即haproxy自身
 option redispatch #代理的服务器挂掉后，强制定向到其他健康的服务器，避免cookie信息过时，仍可正常访问
 retries 3 #3次连接失败就认为后端服务器不可用
 timeout http-request 10s #默认客户端发送http请求的超时时间， 防DDOS攻击手段
 timeout queue 1m #当后台服务器maxconn满了后，haproxy会把client发送来的请求放进一个队列中，一旦事件超过timeout queue，还没被处理，haproxy会自动返回503错误。
 timeout connect 10s #haproxy与后端服务器连接超时时间，如果在同一个局域网可设置较小的时间
 timeout client 300s #默认客户端与haproxy连接后，数据传输完毕，不再有数据传输，即非活动连接的超时时间
 timeout server 300s #定义haproxy与后台服务器非活动连接的超时时间
 timeout http-keep-alive 10s #默认新的http请求建立连接的超时时间，时间较短时可以尽快释放出资源，节约资源。和http-request配合使用
 timeout check 10s #健康检测的时间的最大超时时间
 maxconn 20000 #最大连接数

listen stats :9000
 mode http
 stats enable
 stats uri /

frontend atomic-openshift-api
 bind *:8443
 default_backend atomic-openshift-api
 mode tcp #在此模式下，客户端和服务器端之前将建立一个全双工的连接，不会对七层（http）报文做任何检查
 option tcplog

backend atomic-openshift-api
 balance source #是基于请求源IP的算法，此算法对请求的源IP时行hash运算，然后将结果除以后端服务器的权重总和，来判断转发至哪台后端服务器，这种方法可保证同一客户端IP的请求始终转发到固定定的后端服务器。
 mode tcp
 server master0 192.168.56.100:8443 check
 server master1 192.168.56.101:8443 check
 server master2 192.168.56.102:8443 check
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;a href="http://cbonte.github.io/haproxy-dconv/1.5/configuration.html"&gt;官方文档&lt;/a&gt;介绍的非常详细，感兴趣的可以继续深入研究&lt;/p&gt;</description></item><item><title>镜像管理</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E9%95%9C%E5%83%8F%E7%AE%A1%E7%90%86/</guid><description>&lt;p&gt;刚接触docker时，第一个接触到的应该就是镜像了，docker之所以如此火热，个人认为一大部分原因就是这个镜像的提出，极大的促进了DevOps推广和软件复用的能力。&lt;/p&gt;
&lt;p&gt;而openshift对镜像的管理非常强大，直到写这篇blog，我才真正意识到这点，甚至犹豫是不是要放到开发实战篇后再来写&lt;code&gt;镜像管理&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;简要说下openshift里使用镜像的情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先openshift可以利用任何实现了&lt;code&gt;Docker registry API&lt;/code&gt;的镜像仓，比如，Vmware的Harbor项目，Docker hub以及集成镜像仓（ integrated registry）&lt;/li&gt;
&lt;li&gt;集成镜像仓，openshift内部的，可以动态生成，自动让用户编译的镜像有地方存， 其次它还负责通知openshift镜像的变动，然后openshift会根据策略去决定编译其他依赖镜像还是部署应用&lt;/li&gt;
&lt;li&gt;第三方镜像， 可通过命令&lt;code&gt;oc import-image &amp;lt;stream&amp;gt;&lt;/code&gt;来实时获取镜像tag信息并转换成镜像流，继而触发后续的编译或者部署。&lt;/li&gt;
&lt;li&gt;当然&lt;code&gt;oc new-app&lt;/code&gt;也支持直接从第三方镜像仓或者本地镜像里启动一个应用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;文末有安装集成镜像仓的说明，先介绍image Streams 和 istag的概念和应用场景。&lt;/p&gt;
&lt;h2 id="镜像管理"&gt;镜像管理&lt;a class="td-heading-self-link" href="#%e9%95%9c%e5%83%8f%e7%ae%a1%e7%90%86" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;openshift基于docker的image概念又延伸出了Image Streams和Image Stream Tags概念&lt;/p&gt;
&lt;p&gt;默认openshift项目下会有一些镜像流，是供自带模板里用的，所以想加速部署模板的话，可以在改这里，通过istag指向本地镜像仓。&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;oc get is -n openshift
oc get istag -n openshift
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;image，通俗讲就是对应用运行依赖（库，配置，运行环境）的一个打包。&lt;code&gt;docker pull push&lt;/code&gt;， 就是操作的镜像。
为什么openshift还要抽象出is和istag呢，主要是为了打通集成编译和部署环节（bc和dc），原生API就支持了DevOps理念。后面会细讲bc和dc&lt;/p&gt;
&lt;p&gt;is,开发人员可以理解成git的分支，每个分支都会编译很多临时版本出来，这个就是对应到is～=分支和istag～=版本号。
其实is和istag只是记录了一些映射关系，并不会存放实际镜像数据，比如is里记录了build后要output的镜像仓地址和所有tags，而istag里又记录了具体某个tag与image（可能是存于外部镜像仓，也能是某个is）的关系， 利用此实现了bc/dc和镜像的解耦。&lt;/p&gt;
&lt;p&gt;这里通过部署jenkins服务，来初步了解下具体的含义,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;创建ci项目&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;oc new-project ci
# 先拉取必要镜像
docker pull openshift/jenkins-1-centos7

#通过模板部署，下面一条命令就可以创建一个临时的jenkins服务的
#oc new-app jenkins-ephemeral
#跑之前我们先来注意几点
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;更改默认的is&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先来查看默认的is&lt;/p&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;oc get template jenkins-ephemeral -n openshift -o json
...
&amp;#34;triggers&amp;#34;: [
 {
 &amp;#34;imageChangeParams&amp;#34;: {
 &amp;#34;automatic&amp;#34;: true,
 &amp;#34;containerNames&amp;#34;: [
 &amp;#34;jenkins&amp;#34;
 ],
 &amp;#34;from&amp;#34;: {
 &amp;#34;kind&amp;#34;: &amp;#34;ImageStreamTag&amp;#34;,
 &amp;#34;name&amp;#34;: &amp;#34;${JENKINS_IMAGE_STREAM_TAG}&amp;#34;,
 &amp;#34;namespace&amp;#34;: &amp;#34;${NAMESPACE}&amp;#34;
 },
 &amp;#34;lastTriggeredImage&amp;#34;: &amp;#34;&amp;#34;
 },
 &amp;#34;type&amp;#34;: &amp;#34;ImageChange&amp;#34;
 },
 {
 &amp;#34;type&amp;#34;: &amp;#34;ConfigChange&amp;#34;
 }
 ]
...
{
 &amp;#34;name&amp;#34;: &amp;#34;NAMESPACE&amp;#34;,
 &amp;#34;displayName&amp;#34;: &amp;#34;Jenkins ImageStream Namespace&amp;#34;,
 &amp;#34;description&amp;#34;: &amp;#34;The OpenShift Namespace where the Jenkins ImageStream resides.&amp;#34;,
 &amp;#34;value&amp;#34;: &amp;#34;openshift&amp;#34;
},
{
 &amp;#34;name&amp;#34;: &amp;#34;JENKINS_IMAGE_STREAM_TAG&amp;#34;,
 &amp;#34;displayName&amp;#34;: &amp;#34;Jenkins ImageStreamTag&amp;#34;,
 &amp;#34;description&amp;#34;: &amp;#34;Name of the ImageStreamTag to be used for the Jenkins image.&amp;#34;,
 &amp;#34;value&amp;#34;: &amp;#34;jenkins:latest&amp;#34;
}
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看到默认模板里部署jenkins时，会从openshfit的namespace里拉取jenkins:latest的镜像, 去openshift项目里找找看，确实存在对应的is&lt;/p&gt;</description></item><item><title>k3s实践-01</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/k3s-%E5%BC%80%E7%AF%87/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/k3s-%E5%BC%80%E7%AF%87/</guid><description>&lt;div class="pageinfo pageinfo-primary"&gt;
&lt;p&gt;本文主要介绍k3s的安装和核心组件解读。&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;k3s是all-in-one的轻量k8s发行版，把所有k8s组件打包成一个不到100M的二进制文件了。具备如下显著特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;打包成单一二进制&lt;/li&gt;
&lt;li&gt;默认集成了sqlite3来替代etcd，也可以指定其他数据库：etcd3、mysql、postgres。&lt;/li&gt;
&lt;li&gt;默认内置Coredns、Metrics Server、Flannel、Traefik ingress、Local-path-provisioner等&lt;/li&gt;
&lt;li&gt;默认启用了TLS加密通信。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="安装"&gt;安装&lt;a class="td-heading-self-link" href="#%e5%ae%89%e8%a3%85" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;官方提供了一键安装脚本&lt;a href="https://get.k3s.io"&gt;install.sh&lt;/a&gt; ，执行&lt;code&gt;curl -sfL https://get.k3s.io | sh -&lt;/code&gt;可一键安装server端。此命令会从&lt;code&gt;https://update.k3s.io/v1-release/channels/stable&lt;/code&gt;取到最新的稳定版安装，可以通过&lt;code&gt;INSTALL_K3S_VERSION&lt;/code&gt;环境变量指定版本，本文将以1.19为例。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;启动 k3s server端(master节点).&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;curl -sfL https://get.k3s.io | &lt;span style="color:#b8860b"&gt;INSTALL_K3S_VERSION&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;v1.19.16+k3s1 sh -
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;由于网络原因，可能会失败，自行想办法&lt;a href="https://github.com/k3s-io/k3s/releases/download/v1.19.16&amp;#43;k3s1/k3s"&gt;下载&lt;/a&gt;下来，放置 &lt;code&gt;/usr/local/bin/k3s&lt;/code&gt;，附上执行权限&lt;code&gt;chmod a+x /usr/local/bin/k3s&lt;/code&gt;, 然后上面的命令加上&lt;code&gt;INSTALL_K3S_SKIP_DOWNLOAD=true&lt;/code&gt;再执行一遍即可。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;安装里log里会输出一些重要信息: &lt;code&gt;kubectl、crictl、卸载脚本、systemd service&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;不出意外，k3s server会被systemd启动，执行命令查看&lt;code&gt;systemctl status k3s&lt;/code&gt;或者通过软链的kubectl验证是否启动成功：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;➜ kubectl get no
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;NAME STATUS ROLES AGE VERSION
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;gitlab-server Ready master 6m43s v1.19.16+k3s1
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;(Optional)&lt;/strong&gt; 启动 k3s agent端 (添加worker节点).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;curl -sfL https://get.k3s.io | &lt;span style="color:#b8860b"&gt;K3S_URL&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;https://172.25.11.130:6443 &lt;span style="color:#b8860b"&gt;K3S_TOKEN&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;bulabula &lt;span style="color:#b8860b"&gt;INSTALL_K3S_VERSION&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;v1.19.16+k3s1 sh -
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;K3S_TOKEN&lt;/code&gt;内容需要从server端的&lt;code&gt;/var/lib/rancher/k3s/server/node-token&lt;/code&gt;文件取出&lt;/li&gt;
&lt;li&gt;&lt;code&gt;K3S_URL&lt;/code&gt;中的IP是master节点的IP。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="集群访问"&gt;集群访问&lt;a class="td-heading-self-link" href="#%e9%9b%86%e7%be%a4%e8%ae%bf%e9%97%ae" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;默认kubectl通过localhost访问本地集群，所以上文敲kubectl是没问题的，如果要被外部访问或者纳管的话，可以把kubeconfig文件拷走，默认路径是 &lt;code&gt;/etc/rancher/k3s/k3s.yaml&lt;/code&gt;
。记得修改文件内的server字段，改成外部可访问到的IP。&lt;/p&gt;</description></item><item><title>性能优化指南</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97/</guid><description>&lt;p&gt;主要参考的官方&lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/scaling_and_performance_guide/"&gt;链接&lt;/a&gt;， 本文是基于openshift 3.5说的。&lt;/p&gt;
&lt;h2 id="概览"&gt;概览&lt;a class="td-heading-self-link" href="#%e6%a6%82%e8%a7%88" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;本指南提供了如何提高OpenShift容器平台的集群性能和生产环境下的最佳实践。 主要包括建立，扩展和调优OpenShift集群的推荐做法。&lt;/p&gt;
&lt;p&gt;个人看法，其实性能这个东西是个权衡的过程，根据自身硬件条件和实际需求，选择适合自己的调优手段。&lt;/p&gt;
&lt;h2 id="安装实践"&gt;安装实践&lt;a class="td-heading-self-link" href="#%e5%ae%89%e8%a3%85%e5%ae%9e%e8%b7%b5" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="网络依赖"&gt;网络依赖&lt;a class="td-heading-self-link" href="#%e7%bd%91%e7%bb%9c%e4%be%9d%e8%b5%96" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;首先安装自然要选择官方的&lt;a href="https://github.com/openshift/openshift-ansible"&gt;openshift-ansible项目&lt;/a&gt;， 默认是rpm安装方式，需要依赖网络，比如要去联网下载&lt;code&gt;atomic-openshift-*, iptables, 和 docker&lt;/code&gt;包依赖，&lt;/p&gt;
&lt;p&gt;如果有不能联网的节点，可以参考我之前写的&lt;a href="https://github.com/xiaoping378/openshift-deploy"&gt;离线安装openshift&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id="ansible优化"&gt;ansible优化&lt;a class="td-heading-self-link" href="#ansible%e4%bc%98%e5%8c%96" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;官方推荐使用ansible安装，这里说下针对ansible的优化，以提高安装效率，主要参考&lt;a href="https://www.ansible.com/blog/ansible-performance-tuning"&gt;ansible官方blog&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;如果参考上文离线安装的话，不建议跨外网连接rpm仓或者镜像仓，下面是推荐的ansible配置&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# cat /etc/ansible/ansible.cfg&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# config file for ansible -- http://ansible.com/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# ==============================================&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;defaults&lt;span style="color:#666"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;forks&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#666"&gt;20&lt;/span&gt; &lt;span style="color:#080;font-style:italic"&gt;# 20个并发是理想值，太高的话中间会有概率出错&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;host_key_checking&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; False
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;remote_user&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; root
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;roles_path&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; roles/
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;gathering&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; smart
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;fact_caching&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; jsonfile
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;fact_caching_connection&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#b8860b"&gt;$HOME&lt;/span&gt;/ansible/facts
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;fact_caching_timeout&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#666"&gt;600&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;log_path&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#b8860b"&gt;$HOME&lt;/span&gt;/ansible.log
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;nocows&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#666"&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;callback_whitelist&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; profile_tasks
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;privilege_escalation&lt;span style="color:#666"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;become&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; False
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#666"&gt;[&lt;/span&gt;ssh_connection&lt;span style="color:#666"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;ssh_args&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; -o &lt;span style="color:#b8860b"&gt;ControlMaster&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;auto -o &lt;span style="color:#b8860b"&gt;ControlPersist&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;600s
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;control_path&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; %&lt;span style="color:#666"&gt;(&lt;/span&gt;directory&lt;span style="color:#666"&gt;)&lt;/span&gt;s/%%h-%%r
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;pipelining&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; True &lt;span style="color:#080;font-style:italic"&gt;# 多路复用，减少了控制机和目标间的连接次数，加速了性能。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;timeout&lt;/span&gt; &lt;span style="color:#666"&gt;=&lt;/span&gt; &lt;span style="color:#666"&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="网络配置"&gt;网络配置&lt;a class="td-heading-self-link" href="#%e7%bd%91%e7%bb%9c%e9%85%8d%e7%bd%ae" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;这里必须要提下，一定要安装前做好网络规划，不然后面改起来很麻烦，&lt;/p&gt;</description></item><item><title>k8s controllers工程化实践</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/k8s-controlelrs%E5%B7%A5%E7%A8%8B%E5%8C%96%E5%BC%80%E5%8F%91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/k8s-controlelrs%E5%B7%A5%E7%A8%8B%E5%8C%96%E5%BC%80%E5%8F%91/</guid><description>&lt;h1 id="controllers工程化"&gt;controllers工程化&lt;a class="td-heading-self-link" href="#controllers%e5%b7%a5%e7%a8%8b%e5%8c%96" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="工程化的目标"&gt;工程化的目标&lt;a class="td-heading-self-link" href="#%e5%b7%a5%e7%a8%8b%e5%8c%96%e7%9a%84%e7%9b%ae%e6%a0%87" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;controller工程化的定义，建立一个可持续迭代的工程，包括但不限于以下目标。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持多group资源（多个controller）&lt;/li&gt;
&lt;li&gt;更改CR字段后，可无缝升级（重新生产CR和API）&lt;/li&gt;
&lt;li&gt;API文档化&lt;/li&gt;
&lt;li&gt;CR部署初始化&lt;/li&gt;
&lt;li&gt;ARM多架构编译和镜像构建&lt;/li&gt;
&lt;li&gt;单元测试覆盖率，golang-ci代码扫描。&lt;/li&gt;
&lt;li&gt;暴露关键的监控指标和事件日志&lt;/li&gt;
&lt;li&gt;高可用&lt;/li&gt;
&lt;li&gt;关注规模性能&lt;/li&gt;
&lt;li&gt;安全问题
&lt;ul&gt;
&lt;li&gt;webhook证书，统一管理&lt;/li&gt;
&lt;li&gt;组件Token权限&lt;/li&gt;
&lt;li&gt;CR幂等性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="创建一个operator"&gt;创建一个Operator&lt;a class="td-heading-self-link" href="#%e5%88%9b%e5%bb%ba%e4%b8%80%e4%b8%aaoperator" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;利用kubebuilder初始化一个Operator，背后依赖controller-runtime和controller-gen&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;mkdir -p ~/app &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style="color:#a2f"&gt;cd&lt;/span&gt; ~/app
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;kubebuilder init --domain cebpaas.io --repo cebpaas.io/appmanager
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Writing kustomize manifests for you to edit...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Writing scaffold for you to edit...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Get controller runtime:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# $ go get sigs.k8s.io/controller-runtime@v0.11.2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Update dependencies:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# $ go mod tidy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Next: define a resource with:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# $ kubebuilder create api&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;#可选命令, 本处执行的话，可省略下文的“多个controller合并”&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;#kubebuilder edit --multigroup=true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;kubebuilder create api --group apps --version v1 --kind Application
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Create Resource [y/n]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Create Controller [y/n]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# y&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Writing kustomize manifests for you to edit...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Writing scaffold for you to edit...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# api/v1/application_types.go&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# controllers/application_controller.go&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Update dependencies:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# $ go mod tidy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Running make:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# $ make generate&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# mkdir -p /root/app/bin&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# GOBIN=/root/app/bin go install sigs.k8s.io/controller-tools/cmd/controller-gen@v0.8.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# /root/app/bin/controller-gen object:headerFile=&amp;#34;hack/boilerplate.go.txt&amp;#34; paths=&amp;#34;./...&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# Next: implement your new API and generate the manifests (e.g. CRDs,CRs) with:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# $ make manifests&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;make manifests
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#080;font-style:italic"&gt;# /root/app/bin/controller-gen rbac:roleName=manager-role crd webhook paths=&amp;#34;./...&amp;#34; output:crd:artifacts:config=config/crd/bases&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="多个group-controller合并"&gt;多个group controller合并&lt;a class="td-heading-self-link" href="#%e5%a4%9a%e4%b8%aagroup-controller%e5%90%88%e5%b9%b6" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;开启多controller操作，&lt;/p&gt;</description></item><item><title>DEIS 开源PAAS平台实践</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/deis-%E5%BC%80%E6%BA%90paas%E5%B9%B3%E5%8F%B0%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/deis-%E5%BC%80%E6%BA%90paas%E5%B9%B3%E5%8F%B0%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93/</guid><description>&lt;p&gt;DEIS（目前已被微软收购）的workflow是开源的Paas平台，基于kubernetes做了一层面向开发者的CLI和接口，做到了让开发者对容器无感知的情况下快速的开发和部署线上应用。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;workflow是 on top of k8s的，所有组件默认全是跑在pod里的，不像openshift那样对k8s的侵入性很大。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;特性如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S2I(自动识别源码直接编译成镜像)&lt;/li&gt;
&lt;li&gt;日志聚合&lt;/li&gt;
&lt;li&gt;应用管理（发布，回滚）&lt;/li&gt;
&lt;li&gt;认证&amp;amp;授权机制&lt;/li&gt;
&lt;li&gt;边界路由&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="https://xiaoping378.github.io/Workflow_Detail.png" alt="Workflow_Detail"&gt;&lt;/p&gt;
&lt;p&gt;下面从环境搭建，安装workflow及其基本使用做个梳理。&lt;/p&gt;
&lt;h3 id="初始化k8s集群"&gt;初始化k8s集群&lt;a class="td-heading-self-link" href="#%e5%88%9d%e5%a7%8b%e5%8c%96k8s%e9%9b%86%e7%be%a4" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;可以通过&lt;a href="https://github.com/xiaoping378/k8s-deploy"&gt;k8s-deploy&lt;/a&gt;项目来离线安装高可用kubernetes集群，我这里是单机演示环境。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm init --kubernetes-version v1.6.2 --pod-network-cidr 12.240.0.0/12
#方便命令自动补全
source &amp;lt;(kubectl completion zsh)

#安装cni网络
cp /etc/kubernetes/admin.conf $HOME/.kube/config
kubectl apply -f kube-flannel-rbac.yml
kubectl apply -f kube-flannel.yml

#使能master可以被调度
kubectl taint node --all node-role.kubernetes.io/master-

#安装ingress-controller, 边界路由作用
kubectl create -f ingress-traefik-rbac.yml
kubectl create -f ingress-traefik-deploy.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="初始化helm"&gt;初始化helm&lt;a class="td-heading-self-link" href="#%e5%88%9d%e5%a7%8b%e5%8c%96helm" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;helm相当于kubernetes里的包管理器，类似yum和apt的作用，只不过它操作的是charts（各种k8s yaml文件的集合，额外还有Chart.yaml -- 包的描述文件）可以理解为基于k8s的应用模板管理类工具， 后面会用它来安装workflow到上面跑起来的k8s集群里。&lt;/p&gt;
&lt;p&gt;从k8s 1.6之后，kubeadm安装的集群，默认会开启RBAC机制，为了让helm可以安装任何应用，我们这里赋予tiller cluster-admin权限&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create serviceaccount helm --namespace kube-system
kubectl create clusterrolebinding cluster-admin-helm --clusterrole=cluster-admin --serviceaccount=kube-system:helm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;初始化helm：&lt;/p&gt;</description></item><item><title>网络整理</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E7%BD%91%E7%BB%9C%E6%95%B4%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E7%BD%91%E7%BB%9C%E6%95%B4%E7%90%86/</guid><description>&lt;p&gt;介绍利用openshift-ansible项目安装后的生产环境里的网络情况。&lt;/p&gt;
&lt;p&gt;待整理。。。&lt;/p&gt;</description></item><item><title>监控梳理</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E7%9B%91%E6%8E%A7%E6%A2%B3%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E7%9B%91%E6%8E%A7%E6%A2%B3%E7%90%86/</guid><description>&lt;p&gt;未完搞 ...&lt;/p&gt;</description></item><item><title>日志分析</title><link>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/openshift/openshift%E5%AE%9E%E8%B7%B5-%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90/</guid><description>&lt;p&gt;未完搞 ...&lt;/p&gt;</description></item><item><title>kubeshere 自研-01</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/kubesphere-01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/kubesphere-01/</guid><description>&lt;div class="pageinfo pageinfo-primary"&gt;
&lt;p&gt;kubesphere 自研环境篇&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id="心态"&gt;心态&lt;a class="td-heading-self-link" href="#%e5%bf%83%e6%80%81" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;首先调整心态，这是一个新的生态，秉承学习的心态。&lt;/p&gt;
&lt;h2 id="准备环境"&gt;准备环境&lt;a class="td-heading-self-link" href="#%e5%87%86%e5%a4%87%e7%8e%af%e5%a2%83" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;clone代码&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;git clone https://github.com/kubesphere/kubesphere.git
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;准备开发环境&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://github.com/kubesphere/community/blob/master/developer-guide/development/quickstart.md"&gt;启动快速开发环境&lt;/a&gt;&lt;/p&gt;</description></item><item><title>TKEStack all-in-one入坑指南</title><link>https://xiaoping378.github.io/docs/4-cloud/k8s/tkestack-allinone/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://xiaoping378.github.io/docs/4-cloud/k8s/tkestack-allinone/</guid><description>&lt;div class="pageinfo pageinfo-primary"&gt;
&lt;p&gt;本文主要介绍当前最新版本TkeStack 1.8.1 的TKEStack的all-in-one安装、多租户和多集群管理功能解读。&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id="安装实录"&gt;安装实录&lt;a class="td-heading-self-link" href="#%e5%ae%89%e8%a3%85%e5%ae%9e%e5%bd%95" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;官方推荐至少需要2节点方可安装，配置如下，&lt;strong&gt;硬盘空间&lt;/strong&gt;一定要保障。也支持ALL-in-ONE的方式安装，但有BUG。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://xiaoping378.github.io/images/TKEStack-allinone-2022-01-25-08-47-30.png" alt=""&gt;&lt;/p&gt;
&lt;h2 id="启动init服务"&gt;启动init服务&lt;a class="td-heading-self-link" href="#%e5%90%af%e5%8a%a8init%e6%9c%8d%e5%8a%a1" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;启动init服务，即安装tke-installer和registry服务，安装命令行如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b8860b"&gt;arch&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;amd64 &lt;span style="color:#b8860b"&gt;version&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;v1.8.1 &lt;span style="color:#b62;font-weight:bold"&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b62;font-weight:bold"&gt;&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; wget https://tke-release-1251707795.cos.ap-guangzhou.myqcloud.com/tke-installer-linux-&lt;span style="color:#b8860b"&gt;$arch&lt;/span&gt;-&lt;span style="color:#b8860b"&gt;$version&lt;/span&gt;.run&lt;span style="color:#666"&gt;{&lt;/span&gt;,.sha256&lt;span style="color:#666"&gt;}&lt;/span&gt; &lt;span style="color:#b62;font-weight:bold"&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b62;font-weight:bold"&gt;&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; sha256sum --check --status tke-installer-linux-&lt;span style="color:#b8860b"&gt;$arch&lt;/span&gt;-&lt;span style="color:#b8860b"&gt;$version&lt;/span&gt;.run.sha256 &lt;span style="color:#b62;font-weight:bold"&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b62;font-weight:bold"&gt;&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; chmod +x tke-installer-linux-&lt;span style="color:#b8860b"&gt;$arch&lt;/span&gt;-&lt;span style="color:#b8860b"&gt;$version&lt;/span&gt;.run &lt;span style="color:#b62;font-weight:bold"&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#b62;font-weight:bold"&gt;&lt;/span&gt; &lt;span style="color:#666"&gt;&amp;amp;&amp;amp;&lt;/span&gt; ./tke-installer-linux-&lt;span style="color:#b8860b"&gt;$arch&lt;/span&gt;-&lt;span style="color:#b8860b"&gt;$version&lt;/span&gt;.run  
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如上命令执行后，会下载8G左右的安装包，并执行解压后的install.sh脚本，启动3个容器：1个为tke-installer和另2个为registry仓，且为containerd容器，需要使用&lt;code&gt;nerdctl [images | ps]&lt;/code&gt;等命令查看相关信息。&lt;/p&gt;
&lt;p&gt;通过查看脚本，上文启动的本地registry的启动命令等效如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;nerdctl run --name registry-https -d --net&lt;span style="color:#666"&gt;=&lt;/span&gt;host --restart&lt;span style="color:#666"&gt;=&lt;/span&gt;always -p 443:443 &lt;span style="color:#b62;font-weight:bold"&gt;\ &lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; -v /opt/tke-installer/registry:/var/lib/registry &lt;span style="color:#b62;font-weight:bold"&gt;\ &lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; -v registry-certs:/certs &lt;span style="color:#b62;font-weight:bold"&gt;\ &lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; -e &lt;span style="color:#b8860b"&gt;REGISTRY_HTTP_ADDR&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;0.0.0.0:443 &lt;span style="color:#b62;font-weight:bold"&gt;\ &lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; -e &lt;span style="color:#b8860b"&gt;REGISTRY_HTTP_TLS_CERTIFICATE&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;/certs/server.crt &lt;span style="color:#b62;font-weight:bold"&gt;\ &lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; -e &lt;span style="color:#b8860b"&gt;REGISTRY_HTTP_TLS_KEY&lt;/span&gt;&lt;span style="color:#666"&gt;=&lt;/span&gt;/certs/server.key  &lt;span style="color:#b62;font-weight:bold"&gt;\ &lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt; tkestack/registry-amd64:2.7.1  
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;还有个http 80的registry，这里不贴了，后面的部分坑，就是这里埋下的，预先占用了节点的80和443端口，后面tke的gateway pod会启动失败。&lt;/p&gt;
&lt;h2 id="启动tke集群"&gt;启动TKE集群&lt;a class="td-heading-self-link" href="#%e5%90%af%e5%8a%a8tke%e9%9b%86%e7%be%a4" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;上章节执行完后，会启动tke-installer（一个web操作台），通过访问本地8080端口，可访问界面操作安装global集群。按照官方指引操作就行，此处不表。另外需要说明的是在安装过程中，如果要查看本地容器，不能使用&lt;code&gt;docker ps&lt;/code&gt;了，需要使用&lt;code&gt;nerdctl -n k8s.io ps&lt;/code&gt;。整个安装过程是使用ansible和kubeadm完成的，kubelet是通过systemd启动的，k8s组件为静态pod。&lt;/p&gt;
&lt;p&gt;因为我是使用的ALL-in-ONE安装，遇到了不少问题，可详见FAQ如何解决。安装成功后会提示如下指引：
&lt;img src="https://xiaoping378.github.io/images/TKEStack-allinone-2022-01-25-09-10-56.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;默认初始安装后，很多pod是双副本的，我这里仅是验证功能使用，全部改成了单副本。&lt;/p&gt;
&lt;h2 id="多租户管理"&gt;多租户管理&lt;a class="td-heading-self-link" href="#%e5%a4%9a%e7%a7%9f%e6%88%b7%e7%ae%a1%e7%90%86" aria-label="Heading self-link"&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;tkestack采用&lt;a href="https://xiaoping378.github.io/docs/3-devops/casbin"&gt;Casbin框架&lt;/a&gt;实现的权限管理功能，默认集成的Model，查看&lt;a href="https://github.com/tkestack/tke/blob/a024c064880d9180dc8b6d615ffc58b64bb7f903/api/auth/types.go#L633"&gt;源码&lt;/a&gt;得知：&lt;/p&gt;</description></item></channel></rss>